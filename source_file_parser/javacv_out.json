[
    {
        "project_name": "javacv",
        "file_name": "AndroidFrameConverter.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/AndroidFrameConverter.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**\n     * Convert YUV 4:2:0 SP (NV21) data to BGR, as received, for example,\n     * via {@link Camera.PreviewCallback#onPreviewFrame(byte[],Camera)}.\n     */",
        "source_code": "\npublic Frame convert(byte[] data, int width, int height) {\n    if (frame == null || frame.imageWidth != width\n            || frame.imageHeight != height || frame.imageChannels != 3) {\n        if (frame != null) {\n            frame.close();\n        }\n        frame = new Frame(width, height, Frame.DEPTH_UBYTE, 3);\n    }\n    ByteBuffer out = (ByteBuffer)frame.image[0];\n    int stride = frame.imageStride;\n\n    // ported from https://android.googlesource.com/platform/development/+/master/tools/yuv420sp2rgb/yuv420sp2rgb.c\n    int offset = height * width;\n    for (int i = 0; i < height; i++) {\n        for (int j = 0; j < width; j++) {\n            int Y = data[i * width + j] & 0xFF;\n            int V = data[offset + (i/2) * width + 2 * (j/2)    ] & 0xFF;\n            int U = data[offset + (i/2) * width + 2 * (j/2) + 1] & 0xFF;\n\n            // Yuv Convert\n            Y -= 16;\n            U -= 128;\n            V -= 128;\n\n            if (Y < 0)\n                Y = 0;\n\n            // R = (int)(1.164 * Y + 2.018 * U);\n            // G = (int)(1.164 * Y - 0.813 * V - 0.391 * U);\n            // B = (int)(1.164 * Y + 1.596 * V);\n\n            int B = (int)(1192 * Y + 2066 * U);\n            int G = (int)(1192 * Y - 833 * V - 400 * U);\n            int R = (int)(1192 * Y + 1634 * V);\n\n            R = Math.min(262143, Math.max(0, R));\n            G = Math.min(262143, Math.max(0, G));\n            B = Math.min(262143, Math.max(0, B));\n\n            R >>= 10; R &= 0xff;\n            G >>= 10; G &= 0xff;\n            B >>= 10; B &= 0xff;\n\n            out.put(i * stride + 3 * j,     (byte)B);\n            out.put(i * stride + 3 * j + 1, (byte)G);\n            out.put(i * stride + 3 * j + 2, (byte)R);\n        }\n    }\n    return frame;\n}\n",
        "class_name": "AndroidFrameConverter",
        "method_name": "convert",
        "argument_name": [
            "byte[] data",
            "int width",
            "int height"
        ],
        "full_context": "/*\n * Copyright (C) 2015-2016 Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport android.graphics.Bitmap;\nimport android.hardware.Camera;\nimport java.nio.ByteBuffer;\nimport java.nio.ByteOrder;\n\n/**\n * A utility class to copy data between {@link Frame} and {@link Bitmap}.\n * Since {@link Bitmap} does not expose its internal buffer, we cannot share\n * allocated memory with {@link Frame}.\n * <p>\n * This class is not optimized for speed. For best performance, convert first\n * your data to and from RGBA with optimized functions from FFmpeg or OpenCV.\n * Further, pixel formats other than grayscale, BGR, and RGBA are not well\n * supported. Their conversions might fail in undefined ways.\n *\n * @author Samuel Audet\n */\npublic class AndroidFrameConverter extends FrameConverter<Bitmap> {\n    Bitmap bitmap;\n    ByteBuffer buffer;\n    byte[] row;\n\n    /**\n     * Convert YUV 4:2:0 SP (NV21) data to BGR, as received, for example,\n     * via {@link Camera.PreviewCallback#onPreviewFrame(byte[],Camera)}.\n     */\n    public Frame convert(byte[] data, int width, int height) {\n        if (frame == null || frame.imageWidth != width\n                || frame.imageHeight != height || frame.imageChannels != 3) {\n            if (frame != null) {\n                frame.close();\n            }\n            frame = new Frame(width, height, Frame.DEPTH_UBYTE, 3);\n        }\n        ByteBuffer out = (ByteBuffer)frame.image[0];\n        int stride = frame.imageStride;\n\n        // ported from https://android.googlesource.com/platform/development/+/master/tools/yuv420sp2rgb/yuv420sp2rgb.c\n        int offset = height * width;\n        for (int i = 0; i < height; i++) {\n            for (int j = 0; j < width; j++) {\n                int Y = data[i * width + j] & 0xFF;\n                int V = data[offset + (i/2) * width + 2 * (j/2)    ] & 0xFF;\n                int U = data[offset + (i/2) * width + 2 * (j/2) + 1] & 0xFF;\n\n                // Yuv Convert\n                Y -= 16;\n                U -= 128;\n                V -= 128;\n\n                if (Y < 0)\n                    Y = 0;\n\n                // R = (int)(1.164 * Y + 2.018 * U);\n                // G = (int)(1.164 * Y - 0.813 * V - 0.391 * U);\n                // B = (int)(1.164 * Y + 1.596 * V);\n\n                int B = (int)(1192 * Y + 2066 * U);\n                int G = (int)(1192 * Y - 833 * V - 400 * U);\n                int R = (int)(1192 * Y + 1634 * V);\n\n                R = Math.min(262143, Math.max(0, R));\n                G = Math.min(262143, Math.max(0, G));\n                B = Math.min(262143, Math.max(0, B));\n\n                R >>= 10; R &= 0xff;\n                G >>= 10; G &= 0xff;\n                B >>= 10; B &= 0xff;\n\n                out.put(i * stride + 3 * j,     (byte)B);\n                out.put(i * stride + 3 * j + 1, (byte)G);\n                out.put(i * stride + 3 * j + 2, (byte)R);\n            }\n        }\n        return frame;\n    }\n\n    @Override public Frame convert(Bitmap bitmap) {\n        if (bitmap == null) {\n            return null;\n        }\n\n        int channels = 0;\n        switch (bitmap.getConfig()) {\n            case ALPHA_8:   channels = 1; break;\n            case RGB_565:\n            case ARGB_4444: channels = 2; break;\n            case ARGB_8888: channels = 4; break;\n            default: assert false;\n        }\n\n        if (frame == null || frame.imageWidth != bitmap.getWidth() || frame.imageStride != bitmap.getRowBytes()\n                || frame.imageHeight != bitmap.getHeight() || frame.imageChannels != channels) {\n            if (frame != null) {\n                frame.close();\n            }\n            frame = new Frame(bitmap.getWidth(), bitmap.getHeight(), Frame.DEPTH_UBYTE, channels, bitmap.getRowBytes());\n        }\n\n        bitmap.copyPixelsToBuffer(frame.image[0].position(0));\n\n        return frame;\n    }\n\n    ByteBuffer gray2rgba(ByteBuffer in, int width, int height, int stride, int rowBytes) {\n        if (buffer == null || buffer.capacity() < height * rowBytes) {\n            buffer = ByteBuffer.allocate(height * rowBytes);\n        }\n        if (row == null || row.length != stride)\n            row = new byte[stride];\n        for (int y = 0; y < height; y++) {\n            in.position(y * stride);\n            in.get(row);\n            for (int x = 0; x < width; x++) {\n                // GRAY -> RGBA\n                byte B = row[x];\n                int rgba = (B & 0xff) << 24 |\n                           (B & 0xff) << 16 |\n                           (B & 0xff) <<  8 | 0xff;\n                buffer.putInt(y * rowBytes + 4 * x, rgba);\n            }\n        }\n        return buffer;\n    }\n\n    ByteBuffer bgr2rgba(ByteBuffer in, int width, int height, int stride, int rowBytes) {\n        if (!in.order().equals(ByteOrder.LITTLE_ENDIAN)) {\n            in = in.order(ByteOrder.LITTLE_ENDIAN);\n        }\n        if (buffer == null || buffer.capacity() < height * rowBytes) {\n            buffer = ByteBuffer.allocate(height * rowBytes);\n        }\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                // BGR -> RGBA\n                int rgb;\n                if (x < width - 1 || y < height - 1) {\n                    rgb = in.getInt(y * stride + 3 * x);\n                } else {\n                    int b = in.get(y * stride + 3 * x    ) & 0xff;\n                    int g = in.get(y * stride + 3 * x + 1) & 0xff;\n                    int r = in.get(y * stride + 3 * x + 2) & 0xff;\n                    rgb = (r << 16) | (g << 8) | b;\n                }\n                buffer.putInt(y * rowBytes + 4 * x, (rgb << 8) | 0xff);\n            }\n        }\n        return buffer;\n    }\n\n    @Override public Bitmap convert(Frame frame) {\n        if (frame == null || frame.image == null) {\n            return null;\n        }\n\n        Bitmap.Config config = null;\n        switch (frame.imageChannels) {\n            case 2: config = Bitmap.Config.RGB_565; break;\n            case 1:\n            case 3:\n            case 4: config = Bitmap.Config.ARGB_8888; break;\n            default: assert false;\n        }\n\n        if (bitmap == null || bitmap.getWidth() != frame.imageWidth\n                || bitmap.getHeight() != frame.imageHeight || bitmap.getConfig() != config) {\n            bitmap = Bitmap.createBitmap(frame.imageWidth, frame.imageHeight, config);\n        }\n\n        // assume frame.imageDepth == Frame.DEPTH_UBYTE\n        ByteBuffer in = (ByteBuffer)frame.image[0];\n        int width = frame.imageWidth;\n        int height = frame.imageHeight;\n        int stride = frame.imageStride;\n        int rowBytes = bitmap.getRowBytes();\n        if (frame.imageChannels == 1) {\n            gray2rgba(in, width, height, stride, rowBytes);\n            bitmap.copyPixelsFromBuffer(buffer.position(0));\n        } else if (frame.imageChannels == 3) {\n            bgr2rgba(in, width, height, stride, rowBytes);\n            bitmap.copyPixelsFromBuffer(buffer.position(0));\n        } else {\n            // assume matching strides\n            bitmap.copyPixelsFromBuffer(in.position(0));\n        }\n        return bitmap;\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport android.graphics.Bitmap;\n\nimport android.hardware.Camera;\n\nimport java.nio.ByteBuffer;\n\nimport java.nio.ByteOrder;\n\npublic class AndroidFrameConverter extends FrameConverter<Bitmap> {\n    Bitmap bitmap;\n    ByteBuffer buffer;\n    byte[] row;\n    public Frame convert(byte[] data, int width, int height);\n    public Frame convert(Bitmap bitmap);\n    ByteBuffer gray2rgba(ByteBuffer in, int width, int height, int stride, int rowBytes);\n    ByteBuffer bgr2rgba(ByteBuffer in, int width, int height, int stride, int rowBytes);\n    public Bitmap convert(Frame frame);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "Blobs.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/Blobs.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": null,
        "source_code": "// Determine the next (higher number) region that meets the desired conditions\npublic static int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n{\n    double DParent = (double) Parent; \n    double DColor = (double) Color; if(DColor > 0) DColor = 1;\n    \n    int i;\n    for(i = Label; i <= MaxLabel; i++)\n    {\n        double [] Region = RegionData[i];\n        double ThisParent = Region[BLOBPARENT];\n        double ThisColor = Region[BLOBCOLOR];\n        if(DParent >= 0 && DParent != ThisParent) continue;\n        if(DColor >= 0 && DColor != ThisColor) continue;\n        if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n        break;      // We have a match!\n    }\n    if(i > MaxLabel) i = -1;    // Use -1 to flag that there was no match\n    return i;\n}\n",
        "class_name": "Blobs",
        "method_name": "NextRegion",
        "argument_name": [
            "int Parent",
            "int Color",
            "double MinArea",
            "double MaxArea",
            "int Label"
        ],
        "full_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\n\n//***************************************************************//\n//* Blob analysis package  Version3.0 3 Oct 2012                *//\n//* - Version 1.0:  8 Aug 2003                                  *//\n//* - Version 1.2:  3 Jan 2008                                  *//\n//* - Version 1.3:  5 Jan 2008 Add BLOBCOLOR                    *//\n//* - Version 1.4: 13 January 2008 Add ROI function             *//\n//* - Version 1.5: 13 April 2008 Fix perimeter on Region 0      *//\n//* - Version 1.6:  1 May 2008 Reduce size of working storage   *//\n//* - Version 1.7:  2 May 2008 Speed up run code initialization *//\n//* - Version 1.8:  4 May 2008 Fix bugs in perimeter & Reg 0    *//\n//* - Version 2.0:  3 Jan 2009 Add labeling functionality       *//\n//* - Version 3.0:  3 Oct 2012 Convert to Java                  *//\n//* -   Eliminate labeling functionality (but it's still there) *//\n//* -   Simplify (at slight expense of performance)             *//\n//* -   Reduce to 4 connectivity                                *//\n//*                                                             *//\n//* Input: IplImage binary image                                *//\n//* Output: attributes of each connected region                 *//\n//* Internal data: labeled array (could easily be externalized) *//\n//* Author: Dave Grossman                                       *//\n//* Email: dgrossman2@gmail.com                                 *//\n//* Acknowledgement: my code is based on an algorithm that was  *//\n//* to the best of my knowledge originally developed by Gerry   *//\n//* Agin of SRI around the year 1973. I have not been able to   *//\n//* find any published references to his earlier work. I posted *//\n//* early versions of my program to OpenCV, where they morphed  *//\n//* eventually into cvBlobsLib.                                 *//\n//*                                                             *//\n//* As the author of this code, I place all of this code into   *//\n//* the public domain. Users can use it for any legal purpose.  *//\n//*                                                             *//\n//*             - Dave Grossman                                 *//\n//*                                                             *//\n//* Typical calling sequence:                                   *//\n//*     Blobs Blob = new Blobs();                               *//\n//*     Blob.BlobAnalysis(                                      *//\n//*         image3, // image                                    *//\n//*         -1, -1, // ROI start col, row (-1 means full image) *//\n//*         -1, -1, // ROI cols, rows                           *//\n//*         0,      // border (0 = black; 1 = white)            *//\n//*         20);    // minarea                                  *//\n//*     Blob.PrintRegionData();                                 *//\n//*     int BlobLabel = Blob.NextRegion(                        *//\n//*         -1,     // parentcolor (-1 = ignore)                *//\n//*         0,      // color (0 = black; 1 = white; -1 = ignore *//\n//*         100,    // minarea                                  *//\n//*         500,    // maxarea                                  *//\n//*         15);    // starting label (default 0)               *//\n//*                                                             *//\n//* Ellipse properties can be derived from moments:             *//\n//*     h = (XX + YY) / 2                                       *//\n//*     Major axis = h + sqrt ( h^2 - XX * YY + XY^2)           *//\n//*     Minor axis = h - sqrt ( h^2 - XX * YY2 + XY^2)          *//\n//*     Eccentricity = (sqrt(abs(XX - YY)) + 4 * XY)/AREA       *//\n//***************************************************************//\n\npublic class Blobs\n{\n    // The following parameters should be configured by the user:\n    // On ScanSnap Manager, \"Best\" setting = 300dpi gray level\n    // jpg compression is set to minimum so that quality is highest\n    // Each page jpg image is then a little under 1 MB \n    static int BLOBROWCOUNT = 3500; // 11 inches * 8.5 inches standard page\n    static int BLOBCOLCOUNT = 2700; // with some added cushion to be safe\n    \n    // Allow for vast number of blobs so there is no memory overrun\n    static int BLOBTOTALCOUNT = (BLOBROWCOUNT + BLOBCOLCOUNT) * 5;\n\n    //--------------------------------------------------------------\n    // Do not change anything below this line\n    public static int BLOBLABEL = 0;\n    public static int BLOBPARENT = 1;\n    public static int BLOBCOLOR = 2;\n    public static int BLOBAREA = 3;\n    public static int BLOBPERIMETER = 4;\n    public static int BLOBSUMX = 5;\n    public static int BLOBSUMY = 6;\n    public static int BLOBSUMXX = 7;\n    public static int BLOBSUMYY = 8;\n    public static int BLOBSUMXY = 9;\n    public static int BLOBMINX = 10;\n    public static int BLOBMAXX = 11;\n    public static int BLOBMINY = 12;\n    public static int BLOBMAXY = 13;\n    public static int BLOBDATACOUNT = 14; \n\n    public static int [][] LabelMat = new int [BLOBROWCOUNT][BLOBCOLCOUNT];\n    public static double [][] RegionData = new double [BLOBTOTALCOUNT][BLOBDATACOUNT];\n    public static int MaxLabel; \n    \n    public int LabelA, LabelB, LabelC, LabelD;\n    public int ColorA, ColorB, ColorC, ColorD;\n    public int jrow, jcol;  // index within ROI\n    public static int [] SubsumedLabel = new int [BLOBTOTALCOUNT];\n    public static int [] CondensationMap = new int [BLOBTOTALCOUNT];\n    \n    // Print out all the data for all the regions (blobs)\n    public void PrintRegionData() { PrintRegionData(0, MaxLabel); }\n    public void PrintRegionData(int Label0, int Label1)\n    {\n        if(Label0 < 0) Label0 = 0;\n        if(Label1 > MaxLabel) Label1 = MaxLabel;\n        if(Label1 < Label0) return;\n        for(int Label = Label0; Label <= Label1; Label++)\n        {\n            double [] Property = RegionData[Label];\n            \n            int ThisLabel = (int)Property[BLOBLABEL];\n            int ThisParent = (int)Property[BLOBPARENT];\n            int ThisColor = (int)Property[BLOBCOLOR];\n            double ThisArea = Property[BLOBAREA];\n            double ThisPerimeter = Property[BLOBPERIMETER];\n            double ThisSumX = Property[BLOBSUMX];\n            double ThisSumY = Property[BLOBSUMY];\n            double ThisSumXX = Property[BLOBSUMXX];\n            double ThisSumYY = Property[BLOBSUMYY];\n            double ThisSumXY = Property[BLOBSUMXY];\n            int ThisMinX = (int)Property[BLOBMINX];\n            int ThisMaxX = (int)Property[BLOBMAXX];\n            int ThisMinY = (int)Property[BLOBMINY];\n            int ThisMaxY = (int)Property[BLOBMAXY];\n            \n            String Str1 = \" \" + Label + \": L[\" + ThisLabel + \"] P[\" + ThisParent + \"] C[\" + ThisColor + \"]\";\n            String Str2 = \" AP[\" + ThisArea + \", \" + ThisPerimeter + \"]\";\n            String Str3 = \" M1[\" + ThisSumX + \", \" + ThisSumY + \"] M2[\" + ThisSumXX + \", \" + ThisSumYY + \", \" + ThisSumXY + \"]\";\n            String Str4 = \" MINMAX[\" + ThisMinX + \", \" + ThisMaxX + \", \" + ThisMinY + \", \" + ThisMaxY + \"]\";\n            \n            String Str = Str1 + Str2 + Str3 + Str4;\n            System.out.println(Str);\n        }\n        System.out.println();\n    }\n\n    // Determine the next (higher number) region that meets the desired conditions\n    public static int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n    {\n        double DParent = (double) Parent; \n        double DColor = (double) Color; if(DColor > 0) DColor = 1;\n        \n        int i;\n        for(i = Label; i <= MaxLabel; i++)\n        {\n            double [] Region = RegionData[i];\n            double ThisParent = Region[BLOBPARENT];\n            double ThisColor = Region[BLOBCOLOR];\n            if(DParent >= 0 && DParent != ThisParent) continue;\n            if(DColor >= 0 && DColor != ThisColor) continue;\n            if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n            break;      // We have a match!\n        }\n        if(i > MaxLabel) i = -1;    // Use -1 to flag that there was no match\n        return i;\n    }\n\n    // Determine the prior (lower number) region that meets the desired conditions\n    public static int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n    {\n        double DParent = (double) Parent; \n        double DColor = (double) Color; if(DColor > 0) DColor = 1;\n        \n        int i;\n        for(i = Label; i >= 0; i--)\n        {\n            double [] Region = RegionData[i];\n            double ThisParent = Region[BLOBPARENT];\n            double ThisColor = Region[BLOBCOLOR];\n            if(DParent >= 0 && DParent != ThisParent) continue;\n            if(DColor >= 0 && DColor != ThisColor) continue;\n            if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n            break;      // We have a match!\n        }\n        if(i < 0) i = -1;   // Use -1 to flag that there was no match\n        return i;\n    }\n    \n    public void ResetRegion(int Label)\n    {\n        double [] RegionD = RegionData[Label];\n        RegionD[BLOBLABEL] = \n        RegionD[BLOBPARENT] = \n        RegionD[BLOBCOLOR] =\n        RegionD[BLOBAREA] =\n        RegionD[BLOBPERIMETER] =\n        RegionD[BLOBSUMX] =\n        RegionD[BLOBSUMY] = \n        RegionD[BLOBSUMXX] = \n        RegionD[BLOBSUMYY] = \n        RegionD[BLOBSUMXY] = \n        RegionD[BLOBMINX] = \n        RegionD[BLOBMAXX] = \n        RegionD[BLOBMINY] = \n        RegionD[BLOBMAXY] = 0.0;\n        System.arraycopy(RegionD,0,RegionData[Label],0,BLOBDATACOUNT);  // RegionData[Label] <- RegionD;\n    }\n    \n    public void OldRegion(\n            int NewLabelD,  // 3rd update this (may be the same as Label1 or Label2)\n            int Label1,     // 1st increment this by 1 \n            int Label2)     // 2nd increment this by 1\n    {\n        int DeltaPerimeter = 0;\n        \n        if(Label1 >= 0 && Label1 != NewLabelD)\n        {\n            DeltaPerimeter++;\n            double [] Region1 = RegionData[Label1];\n            Region1[BLOBPERIMETER]++;\n            System.arraycopy(Region1,0,RegionData[Label1],0,BLOBDATACOUNT); // RegionData[Label1] <- Region1;\n        }\n        \n        if(Label2 >= 0 && Label2 != NewLabelD)\n        {\n            DeltaPerimeter++;\n            double [] Region2 = RegionData[Label2];\n            Region2[BLOBPERIMETER]++;\n            System.arraycopy(Region2,0,RegionData[Label2],0,BLOBDATACOUNT); // RegionData[Label2] <- Region2;\n        }\n        \n        LabelD = NewLabelD;\n        double [] RegionD = RegionData[LabelD];\n        RegionD[BLOBLABEL] = LabelD;\n        RegionD[BLOBPARENT] += 0.0;     // no change\n        RegionD[BLOBCOLOR] += 0.0;      // no change\n        RegionD[BLOBAREA] += 1.0;\n        RegionD[BLOBPERIMETER] += DeltaPerimeter;\n        RegionD[BLOBSUMX] += jcol;\n        RegionD[BLOBSUMY] += jrow;\n        RegionD[BLOBSUMXX] += jcol*jcol;\n        RegionD[BLOBSUMYY] += jrow*jrow;\n        RegionD[BLOBSUMXY] += jcol*jrow;\n        RegionD[BLOBMINX] = Math.min(RegionD[BLOBMINX], jcol);\n        RegionD[BLOBMAXX] = Math.max(RegionD[BLOBMAXX], jcol);\n        RegionD[BLOBMINY] = Math.min(RegionD[BLOBMINY], jrow);\n        RegionD[BLOBMAXY] = Math.max(RegionD[BLOBMAXY], jrow);\n        System.arraycopy(RegionD,0,RegionData[LabelD],0,BLOBDATACOUNT); // RegionData[LabelD] <- RegionD;\n   }\n    \n    public void NewRegion(int ParentLabel)\n    {\n        LabelD = ++MaxLabel;\n        double [] RegionD = RegionData[LabelD];\n        RegionD[BLOBLABEL] = LabelD;\n        RegionD[BLOBPARENT] = (double) ParentLabel;\n        RegionD[BLOBCOLOR] = ColorD;\n        RegionD[BLOBAREA] = 1.0;\n        RegionD[BLOBPERIMETER] = 2.0;\n        RegionD[BLOBSUMX] = jcol;\n        RegionD[BLOBSUMY] = jrow;\n        RegionD[BLOBSUMXX] = jcol*jcol;\n        RegionD[BLOBSUMYY] = jrow*jrow;\n        RegionD[BLOBSUMXY] = jcol*jrow;\n        RegionD[BLOBMINX] = jcol;\n        RegionD[BLOBMAXX] = jcol;\n        RegionD[BLOBMINY] = jrow;\n        RegionD[BLOBMAXY] = jrow;\n\n        System.arraycopy(RegionD,0,RegionData[LabelD],0,BLOBDATACOUNT); // RegionData[LabelD] <- RegionD;\n        SubsumedLabel[LabelD] = -1;     // Flag label as not subsumed\n\n        double [] RegionB = RegionData[LabelB];\n        RegionB[BLOBPERIMETER]++;\n        System.arraycopy(RegionB,0,RegionData[LabelB],0,BLOBDATACOUNT); // RegionData[LabelB] <- RegionB;\n        \n        double [] RegionC = RegionData[LabelC];\n        RegionC[BLOBPERIMETER]++;\n\n        System.arraycopy(RegionC,0,RegionData[LabelC],0,BLOBDATACOUNT); // RegionData[LabelC] <- RegionC;\n    }\n    \n    public void Subsume(int GoodLabel, int BadLabel, int PSign) // Combine data with parent\n    {\n        LabelD = GoodLabel;\n        double [] GoodRegion = RegionData[GoodLabel];   \n        double [] BadRegion = RegionData[BadLabel];\n    \n        GoodRegion[BLOBLABEL] = GoodRegion[BLOBLABEL];      // no change\n        GoodRegion[BLOBPARENT] = GoodRegion[BLOBPARENT];    // no change\n        GoodRegion[BLOBCOLOR] = GoodRegion[BLOBCOLOR];      // no change\n        GoodRegion[BLOBAREA] += BadRegion[BLOBAREA];\n        GoodRegion[BLOBPERIMETER] += BadRegion[BLOBPERIMETER] * PSign;  // + external or - internal perimeter\n        GoodRegion[BLOBSUMX] += BadRegion[BLOBSUMX];\n        GoodRegion[BLOBSUMY] += BadRegion[BLOBSUMY];\n        GoodRegion[BLOBSUMXX] += BadRegion[BLOBSUMXX];\n        GoodRegion[BLOBSUMYY] += BadRegion[BLOBSUMYY];\n        GoodRegion[BLOBSUMXY] += BadRegion[BLOBSUMXY];\n        GoodRegion[BLOBMINX] = Math.min(GoodRegion[BLOBMINX], BadRegion[BLOBMINX]);\n        GoodRegion[BLOBMAXX] = Math.max(GoodRegion[BLOBMAXX], BadRegion[BLOBMAXX]);\n        GoodRegion[BLOBMINY] = Math.min(GoodRegion[BLOBMINY], BadRegion[BLOBMINY]);\n        GoodRegion[BLOBMAXY] = Math.max(GoodRegion[BLOBMAXY], BadRegion[BLOBMAXY]);\n        \n        System.arraycopy(GoodRegion,0,RegionData[GoodLabel],0,BLOBDATACOUNT);   // RegionData[GoodLabel] <- GoodRegion;\n    }\n\n    public static int SubsumptionChain(int x) { return SubsumptionChain(x, 0); }\n    public static int SubsumptionChain(int x, int Print)\n    {\n        String Str = \"\";\n        if(Print > 0) Str = \"Subsumption chain for \" + x + \": \";\n        int Lastx = x;\n        while(x > -1)\n        {\n            Lastx = x;\n            if(Print > 0) Str += \" \" + x;\n            if(x == 0) break;\n            x = SubsumedLabel[x];\n        }\n        if(Print > 0) System.out.println(Str);\n        return Lastx;\n    }\n\n    //---------------------------------------------------------------------------------------\n    // Main blob analysis routine\n    //---------------------------------------------------------------------------------------\n    // RegionData[0] is the border. It has Property[BLOBPARENT] = 0. \n\n    public int BlobAnalysis(IplImage Src,           // input image\n                int Col0, int Row0,                 // start of ROI\n                int Cols, int Rows,                 // size of ROI\n                int Border,                         // border color (0 = black; 1 = white)\n                int MinArea)                        // minimum region area\n    {\n        CvMat SrcMat = Src.asCvMat();\n        int SrcCols = SrcMat.cols();\n        int SrcRows = SrcMat.rows();\n        \n        if(Col0 < 0) Col0 = 0;\n        if(Row0 < 0) Row0 = 0;\n        if(Cols < 0) Cols = SrcCols;\n        if(Rows < 0) Rows = SrcRows;\n        if(Col0 + Cols > SrcCols) Cols = SrcCols - Col0;\n        if(Row0 + Rows > SrcRows) Rows = SrcRows - Row0;\n\n        if(Cols > BLOBCOLCOUNT || Rows > BLOBROWCOUNT )\n        {\n            System.out.println(\"Error in Class Blobs: Image too large: Edit Blobs.java\");\n            System.exit(666);\n            return 0;\n        }\n        \n        // Initialization\n        int FillLabel = 0;\n        int FillColor = 0; if(Border > 0) { FillColor = 1; }\n        LabelA = LabelB = LabelC = LabelD = 0;\n        ColorA = ColorB = ColorC = ColorD = FillColor;\n        for(int k = 0; k < BLOBTOTALCOUNT; k++) SubsumedLabel[k] = -1;\n        \n        // Initialize border region\n        MaxLabel = 0;\n        double [] BorderRegion = RegionData[0];\n        BorderRegion[BLOBLABEL] = 0.0;\n        BorderRegion[BLOBPARENT] = -1.0;\n        BorderRegion[BLOBAREA] = Rows + Cols + 4;   // Top, left, and 4 corners\n        BorderRegion[BLOBCOLOR] = FillColor;\n        BorderRegion[BLOBSUMX] = 0.5 * ( (2.0 + Cols) * (Cols - 1.0) ) - Rows - 1 ;\n        BorderRegion[BLOBSUMY] = 0.5 * ( (2.0 + Rows) * (Rows - 1.0) ) - Cols - 1 ;\n        BorderRegion[BLOBMINX] = -1;\n        BorderRegion[BLOBMINY] = -1;\n        BorderRegion[BLOBMAXX] = Cols + 1.0;\n        BorderRegion[BLOBMAXY] = Rows + 1.0;\n        System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n        \n        //  The cells are identified this way\n        //          Last |AB|\n        //          This |CD|\n        //\n        // With 4 connectivity, there are 8 possibilities for the cells:\n        //                      No color transition     Color transition\n        //          Case              1  2  3  4          5  6  7  8 \n        //          Last Row        |pp|pp|pq|pq|       |pp|pp|pq|pq|   \n        //          This Row        |pP|qQ|pP|qQ|       |pQ|qP|pQ|qP|\n        //\n        // Region numbers are p, q, r, x; where p<>q\n        // Upper case letter is the current element at column=x row=y\n        // Color is 0 or 1      (1 stands for 255 in the actual image)\n        // Note that Case 4 is complicated because it joins two regions\n        //--------------------------\n        // Case 1: Colors A=B; C=D; A=C     \n        // Case 2: Colors A=B; C=D; A<>C    \n        // Case 3: Colors A<>B;C=D; A=C     \n        // Case 4: Colors A<>B;C=D; A<>C    \n        // Case 5: Colors A=B; C<>D; A=C    \n        // Case 6: Colors A=B; C<>D; A<>C   \n        // Case 7: Colors A<>B;C<>D; A=C    \n        // Case 8: Colors A<>B;C<>D; A<>C   \n        //--------------------------\n                    \n        // Loop over rows of ROI. irow = Row0 is 1st row of image; irow = Row0+Row is last row of image.\n        for(int irow = Row0; irow < Row0+Rows; irow++)  // index within Src\n        {\n            jrow = irow - Row0; // index within ROI. 0 is first row. Rows is last row.\n            \n            // Loop over columns of ROI.\n            for(int icol = Col0; icol < Col0+Cols; icol++)  // index within Src\n            {\n                jcol = icol - Col0; // index within ROI\n \n                // initialize\n                ColorA = ColorB = ColorC = FillColor;\n                LabelA = LabelB = LabelC = LabelD = 0;\n                ColorD = (int) SrcMat.get(jrow,jcol);       // fetch color of cell\n            \n                if(jrow == 0 || jcol == 0)  // first column or row\n                {\n                    if(jcol > 0)\n                    {\n                        ColorC = (int) SrcMat.get(jrow,jcol-1);\n                        LabelC = LabelMat[jrow][jcol-1];\n                    }\n                    if(jrow > 0)\n                    {\n                        ColorB = (int) SrcMat.get(jrow-1,jcol);\n                        LabelB = LabelMat[jrow-1][jcol];\n                    }\n                }\n                else\n                {\n                    ColorA = (int) SrcMat.get(jrow-1,jcol-1); if(ColorA > 0) ColorA = 1;\n                    ColorB = (int) SrcMat.get(jrow-1,jcol); if(ColorB > 0) ColorB = 1;\n                    ColorC = (int) SrcMat.get(jrow,jcol-1); if(ColorC > 0) ColorC = 1;\n                    LabelA = LabelMat[jrow-1][jcol-1];\n                    LabelB = LabelMat[jrow-1][jcol];\n                    LabelC = LabelMat[jrow][jcol-1];\n                }   \n                if(ColorA > 0) ColorA = 1;\n                if(ColorB > 0) ColorB = 1;\n                if(ColorC > 0) ColorC = 1;\n                if(ColorD > 0) ColorD = 1;\n                    \n                // Determine Case\n                int Case = 0;\n                if(ColorA == ColorB)\n                {\n                    if(ColorC == ColorD) { if(ColorA == ColorC) Case = 1; else Case = 2; }\n                    else { if(ColorA == ColorC) Case = 5; else Case = 6; }\n                }\n                else\n                {\n                    if(ColorC == ColorD) { if(ColorA == ColorC) Case = 3; else Case = 4; }\n                    else { if(ColorA == ColorC) Case = 7; else Case = 8; }\n                }\n\n                // Take appropriate action\n                if(Case == 1) { OldRegion(LabelC, -1, -1); }\n                else if(Case == 2 || Case == 3) { OldRegion(LabelC, LabelB, LabelC); }\n                else if(Case == 5 || Case == 8) // Isolated\n                {\n                    if((jrow == Rows || jcol == Cols) && ColorD == FillColor) { OldRegion(0, -1, -1); } // attached to border region 0\n                    else NewRegion(LabelB);\n                }\n                else if(Case == 6 || Case == 7) { OldRegion(LabelB, LabelB, LabelC); }\n                else            // Case 4 - The complicated situation\n                {\n                    int LabelBRoot = SubsumptionChain(LabelB); \n                    int LabelCRoot = SubsumptionChain(LabelC);\n                    int LabelRoot = Math.min(LabelBRoot, LabelCRoot);\n                    int LabelX;\n                    if(LabelBRoot < LabelCRoot) { OldRegion(LabelB, -1, -1); LabelX = LabelC; }\n                    else { OldRegion(LabelC, -1, -1); LabelX = LabelB; }\n                    int NextLabelX = LabelX;\n                    while(LabelRoot < LabelX)\n                    {\n                        NextLabelX = SubsumedLabel[LabelX];\n                        SubsumedLabel[LabelX] = LabelRoot;\n                        LabelX = NextLabelX;\n                    }\n                }\n                    \n                // Last column or row. Final corner was handled earlier in Cases 5 and 8.\n                if((jrow == Rows || jcol == Cols) && ColorD == FillColor)\n                {\n                    if(jcol < Cols)         // bottom row   \n                    {\n                        if(ColorC != FillColor)     // Subsume B chain to border region 0\n                        {\n                            int LabelRoot = SubsumptionChain(LabelB);\n                            SubsumedLabel[LabelRoot] = 0;\n                        }\n                    }\n                    else if(jrow < Rows)    // right column\n                    {\n                        if(ColorB != FillColor)     // Subsume C chain to border region 0\n                        {\n                            int LabelRoot = SubsumptionChain(LabelC);\n                            SubsumedLabel[LabelRoot] = 0;\n                        }\n                    }\n                    OldRegion(0, -1, -1);   // attached to border region 0\n                }\n\n                LabelMat[jrow][jcol] = LabelD;\n                    \n            }\n        }\n\n        // Compute Condensation map\n        int Offset = 0;\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            if(SubsumedLabel[Label] > -1) Offset++;\n            CondensationMap[Label] = Label - Offset;\n        }\n\n        // Subsume regions that were flagged as connected; Perimeters add\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            int BetterLabel = SubsumptionChain(Label);\n            if(BetterLabel != Label) Subsume(BetterLabel, Label, 1);\n        }   \n\n        // Condense subsumed regions\n        int NewMaxLabel = 0;\n        for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n        {\n            if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n            {\n                double [] OldRegion = RegionData[OldLabel];\n                int OldParent = (int) OldRegion[BLOBPARENT];\n                int NewLabel = CondensationMap[OldLabel];\n                int NewParent = SubsumptionChain(OldParent);\n                NewParent = CondensationMap[NewParent];\n                OldRegion[BLOBLABEL] = (double) NewLabel;\n                OldRegion[BLOBPARENT] = (double) NewParent;\n                System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n                NewMaxLabel = NewLabel;\n            }\n        }\n    \n        // Zero out unneeded high labels\n        for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n        MaxLabel = NewMaxLabel;\n        \n        // Flag for subsumption regions that have too small area\n        for(int Label = MaxLabel; Label > 0; Label--)\n        {\n            double [] ThisRegion = RegionData[Label];\n            int ThisArea = (int) ThisRegion[BLOBAREA];\n            if(ThisArea < MinArea)\n            {\n                int ThisParent = (int) ThisRegion[BLOBPARENT];\n                SubsumedLabel[Label] =  ThisParent;             // Flag this label as having been subsumed\n            }\n            else SubsumedLabel[Label] =  -1;\n        }\n        \n        // Compute Condensation map\n        Offset = 0;\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            if(SubsumedLabel[Label] > -1) Offset++;\n            CondensationMap[Label] = Label - Offset;      \n        }\n\n        // Subsume regions that were flagged as enclosed; Perimeters subtract\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            int BetterLabel = SubsumptionChain(Label);\n            if(BetterLabel != Label) Subsume(BetterLabel, Label, -1);\n        }   \n    \n        // Condense subsumed regions\n        for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n        {\n            if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n            {\n                double [] OldRegion = RegionData[OldLabel];\n                int OldParent = (int) OldRegion[BLOBPARENT];\n                int NewLabel = CondensationMap[OldLabel];\n                int NewParent = SubsumptionChain(OldParent);\n                NewParent = CondensationMap[NewParent];\n                OldRegion[BLOBLABEL] = (double) NewLabel;\n                OldRegion[BLOBPARENT] = (double) NewParent;\n                System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n                NewMaxLabel = NewLabel;\n            }\n        }\n        \n        // Zero out unneeded high labels\n        for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n        MaxLabel = NewMaxLabel;\n\n        // Normalize summation fields into moments \n        for(int Label = 0; Label <= MaxLabel; Label++)\n        {\n            double [] ThisRegion = RegionData[Label];\n            \n            // Extract fields\n            double Area = ThisRegion[BLOBAREA];\n            double SumX = ThisRegion[BLOBSUMX];\n            double SumY = ThisRegion[BLOBSUMY];\n            double SumXX = ThisRegion[BLOBSUMXX];\n            double SumYY = ThisRegion[BLOBSUMYY];\n            double SumXY = ThisRegion[BLOBSUMXY];\n            \n            // Get averages\n            SumX /= Area;\n            SumY /= Area;\n            SumXX /= Area;\n            SumYY /= Area;\n            SumXY /= Area;\n            \n            // Create moments\n            SumXX -= SumX * SumX;\n            SumYY -= SumY * SumY;\n            SumXY -= SumX * SumY;\n            if(SumXY > -1.0E-14 && SumXY < 1.0E-14) SumXY = (float) 0.0; // Eliminate roundoff error\n\n            ThisRegion[BLOBSUMX] = SumX;\n            ThisRegion[BLOBSUMY] = SumY;\n            ThisRegion[BLOBSUMXX] = SumXX;\n            ThisRegion[BLOBSUMYY] = SumYY;\n            ThisRegion[BLOBSUMXY] = SumXY;\n\n            System.arraycopy(ThisRegion,0,RegionData[Label],0,BLOBDATACOUNT);   // RegionData[Label] <- ThisRegion;\n        }\n    \n        // Adjust border region\n        BorderRegion = RegionData[0];\n        BorderRegion[BLOBSUMXX] = BorderRegion[BLOBSUMYY] = BorderRegion[BLOBSUMXY] = 0;    // Mark invalid fields\n        System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n        \n        return MaxLabel;\n    }\n    \n    // Sort RegionData array on any column. (I couldn't figure out how to use the built-in java sort.)\n    static double iField, jField;\n    static double [] iProperty, jProperty;\n    public static void SortRegions(int Col)\n    {\n        for(int i = 0; i < MaxLabel; i++)\n        {\n            for(int j = i+1; j <= Blobs.MaxLabel; j++)\n            {\n                iProperty = RegionData[i];\n                jProperty = RegionData[j];\n                iField = iProperty[Col];\n                jField = jProperty[Col];\n                if(iField > jField)\n                {\n                    RegionData[i] = jProperty;\n                    RegionData[j] = iProperty;\n                }\n            }\n        }\n    }\n}\n\n\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\npublic class Blobs {\n    static int BLOBROWCOUNT;\n    static int BLOBCOLCOUNT;\n    static int BLOBTOTALCOUNT;\n    static public int BLOBLABEL;\n    static public int BLOBPARENT;\n    static public int BLOBCOLOR;\n    static public int BLOBAREA;\n    static public int BLOBPERIMETER;\n    static public int BLOBSUMX;\n    static public int BLOBSUMY;\n    static public int BLOBSUMXX;\n    static public int BLOBSUMYY;\n    static public int BLOBSUMXY;\n    static public int BLOBMINX;\n    static public int BLOBMAXX;\n    static public int BLOBMINY;\n    static public int BLOBMAXY;\n    static public int BLOBDATACOUNT;\n    static public int[][] LabelMat;\n    static public double[][] RegionData;\n    static public int MaxLabel;\n    public int LabelA, LabelB, LabelC, LabelD;\n    public int ColorA, ColorB, ColorC, ColorD;\n    public int jrow, jcol;\n    static public int[] SubsumedLabel;\n    static public int[] CondensationMap;\n    public  PrintRegionData();\n    public  PrintRegionData(int Label0, int Label1);\n    static public int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label);\n    static public int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label);\n    public  ResetRegion(int Label);\n    public  OldRegion(int NewLabelD, int Label1, int Label2);\n    public  NewRegion(int ParentLabel);\n    public  Subsume(int GoodLabel, int BadLabel, int PSign);\n    static public int SubsumptionChain(int x);\n    static public int SubsumptionChain(int x, int Print);\n    public int BlobAnalysis(IplImage Src, int Col0, int Row0, int Cols, int Rows, int Border, int MinArea);\n    static double iField, jField;\n    static double[] iProperty, jProperty;\n    static public  SortRegions(int Col);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "Blobs.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/Blobs.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": null,
        "source_code": "// Determine the prior (lower number) region that meets the desired conditions\npublic static int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n{\n    double DParent = (double) Parent; \n    double DColor = (double) Color; if(DColor > 0) DColor = 1;\n    \n    int i;\n    for(i = Label; i >= 0; i--)\n    {\n        double [] Region = RegionData[i];\n        double ThisParent = Region[BLOBPARENT];\n        double ThisColor = Region[BLOBCOLOR];\n        if(DParent >= 0 && DParent != ThisParent) continue;\n        if(DColor >= 0 && DColor != ThisColor) continue;\n        if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n        break;      // We have a match!\n    }\n    if(i < 0) i = -1;   // Use -1 to flag that there was no match\n    return i;\n}\n",
        "class_name": "Blobs",
        "method_name": "PriorRegion",
        "argument_name": [
            "int Parent",
            "int Color",
            "double MinArea",
            "double MaxArea",
            "int Label"
        ],
        "full_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\n\n//***************************************************************//\n//* Blob analysis package  Version3.0 3 Oct 2012                *//\n//* - Version 1.0:  8 Aug 2003                                  *//\n//* - Version 1.2:  3 Jan 2008                                  *//\n//* - Version 1.3:  5 Jan 2008 Add BLOBCOLOR                    *//\n//* - Version 1.4: 13 January 2008 Add ROI function             *//\n//* - Version 1.5: 13 April 2008 Fix perimeter on Region 0      *//\n//* - Version 1.6:  1 May 2008 Reduce size of working storage   *//\n//* - Version 1.7:  2 May 2008 Speed up run code initialization *//\n//* - Version 1.8:  4 May 2008 Fix bugs in perimeter & Reg 0    *//\n//* - Version 2.0:  3 Jan 2009 Add labeling functionality       *//\n//* - Version 3.0:  3 Oct 2012 Convert to Java                  *//\n//* -   Eliminate labeling functionality (but it's still there) *//\n//* -   Simplify (at slight expense of performance)             *//\n//* -   Reduce to 4 connectivity                                *//\n//*                                                             *//\n//* Input: IplImage binary image                                *//\n//* Output: attributes of each connected region                 *//\n//* Internal data: labeled array (could easily be externalized) *//\n//* Author: Dave Grossman                                       *//\n//* Email: dgrossman2@gmail.com                                 *//\n//* Acknowledgement: my code is based on an algorithm that was  *//\n//* to the best of my knowledge originally developed by Gerry   *//\n//* Agin of SRI around the year 1973. I have not been able to   *//\n//* find any published references to his earlier work. I posted *//\n//* early versions of my program to OpenCV, where they morphed  *//\n//* eventually into cvBlobsLib.                                 *//\n//*                                                             *//\n//* As the author of this code, I place all of this code into   *//\n//* the public domain. Users can use it for any legal purpose.  *//\n//*                                                             *//\n//*             - Dave Grossman                                 *//\n//*                                                             *//\n//* Typical calling sequence:                                   *//\n//*     Blobs Blob = new Blobs();                               *//\n//*     Blob.BlobAnalysis(                                      *//\n//*         image3, // image                                    *//\n//*         -1, -1, // ROI start col, row (-1 means full image) *//\n//*         -1, -1, // ROI cols, rows                           *//\n//*         0,      // border (0 = black; 1 = white)            *//\n//*         20);    // minarea                                  *//\n//*     Blob.PrintRegionData();                                 *//\n//*     int BlobLabel = Blob.NextRegion(                        *//\n//*         -1,     // parentcolor (-1 = ignore)                *//\n//*         0,      // color (0 = black; 1 = white; -1 = ignore *//\n//*         100,    // minarea                                  *//\n//*         500,    // maxarea                                  *//\n//*         15);    // starting label (default 0)               *//\n//*                                                             *//\n//* Ellipse properties can be derived from moments:             *//\n//*     h = (XX + YY) / 2                                       *//\n//*     Major axis = h + sqrt ( h^2 - XX * YY + XY^2)           *//\n//*     Minor axis = h - sqrt ( h^2 - XX * YY2 + XY^2)          *//\n//*     Eccentricity = (sqrt(abs(XX - YY)) + 4 * XY)/AREA       *//\n//***************************************************************//\n\npublic class Blobs\n{\n    // The following parameters should be configured by the user:\n    // On ScanSnap Manager, \"Best\" setting = 300dpi gray level\n    // jpg compression is set to minimum so that quality is highest\n    // Each page jpg image is then a little under 1 MB \n    static int BLOBROWCOUNT = 3500; // 11 inches * 8.5 inches standard page\n    static int BLOBCOLCOUNT = 2700; // with some added cushion to be safe\n    \n    // Allow for vast number of blobs so there is no memory overrun\n    static int BLOBTOTALCOUNT = (BLOBROWCOUNT + BLOBCOLCOUNT) * 5;\n\n    //--------------------------------------------------------------\n    // Do not change anything below this line\n    public static int BLOBLABEL = 0;\n    public static int BLOBPARENT = 1;\n    public static int BLOBCOLOR = 2;\n    public static int BLOBAREA = 3;\n    public static int BLOBPERIMETER = 4;\n    public static int BLOBSUMX = 5;\n    public static int BLOBSUMY = 6;\n    public static int BLOBSUMXX = 7;\n    public static int BLOBSUMYY = 8;\n    public static int BLOBSUMXY = 9;\n    public static int BLOBMINX = 10;\n    public static int BLOBMAXX = 11;\n    public static int BLOBMINY = 12;\n    public static int BLOBMAXY = 13;\n    public static int BLOBDATACOUNT = 14; \n\n    public static int [][] LabelMat = new int [BLOBROWCOUNT][BLOBCOLCOUNT];\n    public static double [][] RegionData = new double [BLOBTOTALCOUNT][BLOBDATACOUNT];\n    public static int MaxLabel; \n    \n    public int LabelA, LabelB, LabelC, LabelD;\n    public int ColorA, ColorB, ColorC, ColorD;\n    public int jrow, jcol;  // index within ROI\n    public static int [] SubsumedLabel = new int [BLOBTOTALCOUNT];\n    public static int [] CondensationMap = new int [BLOBTOTALCOUNT];\n    \n    // Print out all the data for all the regions (blobs)\n    public void PrintRegionData() { PrintRegionData(0, MaxLabel); }\n    public void PrintRegionData(int Label0, int Label1)\n    {\n        if(Label0 < 0) Label0 = 0;\n        if(Label1 > MaxLabel) Label1 = MaxLabel;\n        if(Label1 < Label0) return;\n        for(int Label = Label0; Label <= Label1; Label++)\n        {\n            double [] Property = RegionData[Label];\n            \n            int ThisLabel = (int)Property[BLOBLABEL];\n            int ThisParent = (int)Property[BLOBPARENT];\n            int ThisColor = (int)Property[BLOBCOLOR];\n            double ThisArea = Property[BLOBAREA];\n            double ThisPerimeter = Property[BLOBPERIMETER];\n            double ThisSumX = Property[BLOBSUMX];\n            double ThisSumY = Property[BLOBSUMY];\n            double ThisSumXX = Property[BLOBSUMXX];\n            double ThisSumYY = Property[BLOBSUMYY];\n            double ThisSumXY = Property[BLOBSUMXY];\n            int ThisMinX = (int)Property[BLOBMINX];\n            int ThisMaxX = (int)Property[BLOBMAXX];\n            int ThisMinY = (int)Property[BLOBMINY];\n            int ThisMaxY = (int)Property[BLOBMAXY];\n            \n            String Str1 = \" \" + Label + \": L[\" + ThisLabel + \"] P[\" + ThisParent + \"] C[\" + ThisColor + \"]\";\n            String Str2 = \" AP[\" + ThisArea + \", \" + ThisPerimeter + \"]\";\n            String Str3 = \" M1[\" + ThisSumX + \", \" + ThisSumY + \"] M2[\" + ThisSumXX + \", \" + ThisSumYY + \", \" + ThisSumXY + \"]\";\n            String Str4 = \" MINMAX[\" + ThisMinX + \", \" + ThisMaxX + \", \" + ThisMinY + \", \" + ThisMaxY + \"]\";\n            \n            String Str = Str1 + Str2 + Str3 + Str4;\n            System.out.println(Str);\n        }\n        System.out.println();\n    }\n\n    // Determine the next (higher number) region that meets the desired conditions\n    public static int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n    {\n        double DParent = (double) Parent; \n        double DColor = (double) Color; if(DColor > 0) DColor = 1;\n        \n        int i;\n        for(i = Label; i <= MaxLabel; i++)\n        {\n            double [] Region = RegionData[i];\n            double ThisParent = Region[BLOBPARENT];\n            double ThisColor = Region[BLOBCOLOR];\n            if(DParent >= 0 && DParent != ThisParent) continue;\n            if(DColor >= 0 && DColor != ThisColor) continue;\n            if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n            break;      // We have a match!\n        }\n        if(i > MaxLabel) i = -1;    // Use -1 to flag that there was no match\n        return i;\n    }\n\n    // Determine the prior (lower number) region that meets the desired conditions\n    public static int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n    {\n        double DParent = (double) Parent; \n        double DColor = (double) Color; if(DColor > 0) DColor = 1;\n        \n        int i;\n        for(i = Label; i >= 0; i--)\n        {\n            double [] Region = RegionData[i];\n            double ThisParent = Region[BLOBPARENT];\n            double ThisColor = Region[BLOBCOLOR];\n            if(DParent >= 0 && DParent != ThisParent) continue;\n            if(DColor >= 0 && DColor != ThisColor) continue;\n            if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n            break;      // We have a match!\n        }\n        if(i < 0) i = -1;   // Use -1 to flag that there was no match\n        return i;\n    }\n    \n    public void ResetRegion(int Label)\n    {\n        double [] RegionD = RegionData[Label];\n        RegionD[BLOBLABEL] = \n        RegionD[BLOBPARENT] = \n        RegionD[BLOBCOLOR] =\n        RegionD[BLOBAREA] =\n        RegionD[BLOBPERIMETER] =\n        RegionD[BLOBSUMX] =\n        RegionD[BLOBSUMY] = \n        RegionD[BLOBSUMXX] = \n        RegionD[BLOBSUMYY] = \n        RegionD[BLOBSUMXY] = \n        RegionD[BLOBMINX] = \n        RegionD[BLOBMAXX] = \n        RegionD[BLOBMINY] = \n        RegionD[BLOBMAXY] = 0.0;\n        System.arraycopy(RegionD,0,RegionData[Label],0,BLOBDATACOUNT);  // RegionData[Label] <- RegionD;\n    }\n    \n    public void OldRegion(\n            int NewLabelD,  // 3rd update this (may be the same as Label1 or Label2)\n            int Label1,     // 1st increment this by 1 \n            int Label2)     // 2nd increment this by 1\n    {\n        int DeltaPerimeter = 0;\n        \n        if(Label1 >= 0 && Label1 != NewLabelD)\n        {\n            DeltaPerimeter++;\n            double [] Region1 = RegionData[Label1];\n            Region1[BLOBPERIMETER]++;\n            System.arraycopy(Region1,0,RegionData[Label1],0,BLOBDATACOUNT); // RegionData[Label1] <- Region1;\n        }\n        \n        if(Label2 >= 0 && Label2 != NewLabelD)\n        {\n            DeltaPerimeter++;\n            double [] Region2 = RegionData[Label2];\n            Region2[BLOBPERIMETER]++;\n            System.arraycopy(Region2,0,RegionData[Label2],0,BLOBDATACOUNT); // RegionData[Label2] <- Region2;\n        }\n        \n        LabelD = NewLabelD;\n        double [] RegionD = RegionData[LabelD];\n        RegionD[BLOBLABEL] = LabelD;\n        RegionD[BLOBPARENT] += 0.0;     // no change\n        RegionD[BLOBCOLOR] += 0.0;      // no change\n        RegionD[BLOBAREA] += 1.0;\n        RegionD[BLOBPERIMETER] += DeltaPerimeter;\n        RegionD[BLOBSUMX] += jcol;\n        RegionD[BLOBSUMY] += jrow;\n        RegionD[BLOBSUMXX] += jcol*jcol;\n        RegionD[BLOBSUMYY] += jrow*jrow;\n        RegionD[BLOBSUMXY] += jcol*jrow;\n        RegionD[BLOBMINX] = Math.min(RegionD[BLOBMINX], jcol);\n        RegionD[BLOBMAXX] = Math.max(RegionD[BLOBMAXX], jcol);\n        RegionD[BLOBMINY] = Math.min(RegionD[BLOBMINY], jrow);\n        RegionD[BLOBMAXY] = Math.max(RegionD[BLOBMAXY], jrow);\n        System.arraycopy(RegionD,0,RegionData[LabelD],0,BLOBDATACOUNT); // RegionData[LabelD] <- RegionD;\n   }\n    \n    public void NewRegion(int ParentLabel)\n    {\n        LabelD = ++MaxLabel;\n        double [] RegionD = RegionData[LabelD];\n        RegionD[BLOBLABEL] = LabelD;\n        RegionD[BLOBPARENT] = (double) ParentLabel;\n        RegionD[BLOBCOLOR] = ColorD;\n        RegionD[BLOBAREA] = 1.0;\n        RegionD[BLOBPERIMETER] = 2.0;\n        RegionD[BLOBSUMX] = jcol;\n        RegionD[BLOBSUMY] = jrow;\n        RegionD[BLOBSUMXX] = jcol*jcol;\n        RegionD[BLOBSUMYY] = jrow*jrow;\n        RegionD[BLOBSUMXY] = jcol*jrow;\n        RegionD[BLOBMINX] = jcol;\n        RegionD[BLOBMAXX] = jcol;\n        RegionD[BLOBMINY] = jrow;\n        RegionD[BLOBMAXY] = jrow;\n\n        System.arraycopy(RegionD,0,RegionData[LabelD],0,BLOBDATACOUNT); // RegionData[LabelD] <- RegionD;\n        SubsumedLabel[LabelD] = -1;     // Flag label as not subsumed\n\n        double [] RegionB = RegionData[LabelB];\n        RegionB[BLOBPERIMETER]++;\n        System.arraycopy(RegionB,0,RegionData[LabelB],0,BLOBDATACOUNT); // RegionData[LabelB] <- RegionB;\n        \n        double [] RegionC = RegionData[LabelC];\n        RegionC[BLOBPERIMETER]++;\n\n        System.arraycopy(RegionC,0,RegionData[LabelC],0,BLOBDATACOUNT); // RegionData[LabelC] <- RegionC;\n    }\n    \n    public void Subsume(int GoodLabel, int BadLabel, int PSign) // Combine data with parent\n    {\n        LabelD = GoodLabel;\n        double [] GoodRegion = RegionData[GoodLabel];   \n        double [] BadRegion = RegionData[BadLabel];\n    \n        GoodRegion[BLOBLABEL] = GoodRegion[BLOBLABEL];      // no change\n        GoodRegion[BLOBPARENT] = GoodRegion[BLOBPARENT];    // no change\n        GoodRegion[BLOBCOLOR] = GoodRegion[BLOBCOLOR];      // no change\n        GoodRegion[BLOBAREA] += BadRegion[BLOBAREA];\n        GoodRegion[BLOBPERIMETER] += BadRegion[BLOBPERIMETER] * PSign;  // + external or - internal perimeter\n        GoodRegion[BLOBSUMX] += BadRegion[BLOBSUMX];\n        GoodRegion[BLOBSUMY] += BadRegion[BLOBSUMY];\n        GoodRegion[BLOBSUMXX] += BadRegion[BLOBSUMXX];\n        GoodRegion[BLOBSUMYY] += BadRegion[BLOBSUMYY];\n        GoodRegion[BLOBSUMXY] += BadRegion[BLOBSUMXY];\n        GoodRegion[BLOBMINX] = Math.min(GoodRegion[BLOBMINX], BadRegion[BLOBMINX]);\n        GoodRegion[BLOBMAXX] = Math.max(GoodRegion[BLOBMAXX], BadRegion[BLOBMAXX]);\n        GoodRegion[BLOBMINY] = Math.min(GoodRegion[BLOBMINY], BadRegion[BLOBMINY]);\n        GoodRegion[BLOBMAXY] = Math.max(GoodRegion[BLOBMAXY], BadRegion[BLOBMAXY]);\n        \n        System.arraycopy(GoodRegion,0,RegionData[GoodLabel],0,BLOBDATACOUNT);   // RegionData[GoodLabel] <- GoodRegion;\n    }\n\n    public static int SubsumptionChain(int x) { return SubsumptionChain(x, 0); }\n    public static int SubsumptionChain(int x, int Print)\n    {\n        String Str = \"\";\n        if(Print > 0) Str = \"Subsumption chain for \" + x + \": \";\n        int Lastx = x;\n        while(x > -1)\n        {\n            Lastx = x;\n            if(Print > 0) Str += \" \" + x;\n            if(x == 0) break;\n            x = SubsumedLabel[x];\n        }\n        if(Print > 0) System.out.println(Str);\n        return Lastx;\n    }\n\n    //---------------------------------------------------------------------------------------\n    // Main blob analysis routine\n    //---------------------------------------------------------------------------------------\n    // RegionData[0] is the border. It has Property[BLOBPARENT] = 0. \n\n    public int BlobAnalysis(IplImage Src,           // input image\n                int Col0, int Row0,                 // start of ROI\n                int Cols, int Rows,                 // size of ROI\n                int Border,                         // border color (0 = black; 1 = white)\n                int MinArea)                        // minimum region area\n    {\n        CvMat SrcMat = Src.asCvMat();\n        int SrcCols = SrcMat.cols();\n        int SrcRows = SrcMat.rows();\n        \n        if(Col0 < 0) Col0 = 0;\n        if(Row0 < 0) Row0 = 0;\n        if(Cols < 0) Cols = SrcCols;\n        if(Rows < 0) Rows = SrcRows;\n        if(Col0 + Cols > SrcCols) Cols = SrcCols - Col0;\n        if(Row0 + Rows > SrcRows) Rows = SrcRows - Row0;\n\n        if(Cols > BLOBCOLCOUNT || Rows > BLOBROWCOUNT )\n        {\n            System.out.println(\"Error in Class Blobs: Image too large: Edit Blobs.java\");\n            System.exit(666);\n            return 0;\n        }\n        \n        // Initialization\n        int FillLabel = 0;\n        int FillColor = 0; if(Border > 0) { FillColor = 1; }\n        LabelA = LabelB = LabelC = LabelD = 0;\n        ColorA = ColorB = ColorC = ColorD = FillColor;\n        for(int k = 0; k < BLOBTOTALCOUNT; k++) SubsumedLabel[k] = -1;\n        \n        // Initialize border region\n        MaxLabel = 0;\n        double [] BorderRegion = RegionData[0];\n        BorderRegion[BLOBLABEL] = 0.0;\n        BorderRegion[BLOBPARENT] = -1.0;\n        BorderRegion[BLOBAREA] = Rows + Cols + 4;   // Top, left, and 4 corners\n        BorderRegion[BLOBCOLOR] = FillColor;\n        BorderRegion[BLOBSUMX] = 0.5 * ( (2.0 + Cols) * (Cols - 1.0) ) - Rows - 1 ;\n        BorderRegion[BLOBSUMY] = 0.5 * ( (2.0 + Rows) * (Rows - 1.0) ) - Cols - 1 ;\n        BorderRegion[BLOBMINX] = -1;\n        BorderRegion[BLOBMINY] = -1;\n        BorderRegion[BLOBMAXX] = Cols + 1.0;\n        BorderRegion[BLOBMAXY] = Rows + 1.0;\n        System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n        \n        //  The cells are identified this way\n        //          Last |AB|\n        //          This |CD|\n        //\n        // With 4 connectivity, there are 8 possibilities for the cells:\n        //                      No color transition     Color transition\n        //          Case              1  2  3  4          5  6  7  8 \n        //          Last Row        |pp|pp|pq|pq|       |pp|pp|pq|pq|   \n        //          This Row        |pP|qQ|pP|qQ|       |pQ|qP|pQ|qP|\n        //\n        // Region numbers are p, q, r, x; where p<>q\n        // Upper case letter is the current element at column=x row=y\n        // Color is 0 or 1      (1 stands for 255 in the actual image)\n        // Note that Case 4 is complicated because it joins two regions\n        //--------------------------\n        // Case 1: Colors A=B; C=D; A=C     \n        // Case 2: Colors A=B; C=D; A<>C    \n        // Case 3: Colors A<>B;C=D; A=C     \n        // Case 4: Colors A<>B;C=D; A<>C    \n        // Case 5: Colors A=B; C<>D; A=C    \n        // Case 6: Colors A=B; C<>D; A<>C   \n        // Case 7: Colors A<>B;C<>D; A=C    \n        // Case 8: Colors A<>B;C<>D; A<>C   \n        //--------------------------\n                    \n        // Loop over rows of ROI. irow = Row0 is 1st row of image; irow = Row0+Row is last row of image.\n        for(int irow = Row0; irow < Row0+Rows; irow++)  // index within Src\n        {\n            jrow = irow - Row0; // index within ROI. 0 is first row. Rows is last row.\n            \n            // Loop over columns of ROI.\n            for(int icol = Col0; icol < Col0+Cols; icol++)  // index within Src\n            {\n                jcol = icol - Col0; // index within ROI\n \n                // initialize\n                ColorA = ColorB = ColorC = FillColor;\n                LabelA = LabelB = LabelC = LabelD = 0;\n                ColorD = (int) SrcMat.get(jrow,jcol);       // fetch color of cell\n            \n                if(jrow == 0 || jcol == 0)  // first column or row\n                {\n                    if(jcol > 0)\n                    {\n                        ColorC = (int) SrcMat.get(jrow,jcol-1);\n                        LabelC = LabelMat[jrow][jcol-1];\n                    }\n                    if(jrow > 0)\n                    {\n                        ColorB = (int) SrcMat.get(jrow-1,jcol);\n                        LabelB = LabelMat[jrow-1][jcol];\n                    }\n                }\n                else\n                {\n                    ColorA = (int) SrcMat.get(jrow-1,jcol-1); if(ColorA > 0) ColorA = 1;\n                    ColorB = (int) SrcMat.get(jrow-1,jcol); if(ColorB > 0) ColorB = 1;\n                    ColorC = (int) SrcMat.get(jrow,jcol-1); if(ColorC > 0) ColorC = 1;\n                    LabelA = LabelMat[jrow-1][jcol-1];\n                    LabelB = LabelMat[jrow-1][jcol];\n                    LabelC = LabelMat[jrow][jcol-1];\n                }   \n                if(ColorA > 0) ColorA = 1;\n                if(ColorB > 0) ColorB = 1;\n                if(ColorC > 0) ColorC = 1;\n                if(ColorD > 0) ColorD = 1;\n                    \n                // Determine Case\n                int Case = 0;\n                if(ColorA == ColorB)\n                {\n                    if(ColorC == ColorD) { if(ColorA == ColorC) Case = 1; else Case = 2; }\n                    else { if(ColorA == ColorC) Case = 5; else Case = 6; }\n                }\n                else\n                {\n                    if(ColorC == ColorD) { if(ColorA == ColorC) Case = 3; else Case = 4; }\n                    else { if(ColorA == ColorC) Case = 7; else Case = 8; }\n                }\n\n                // Take appropriate action\n                if(Case == 1) { OldRegion(LabelC, -1, -1); }\n                else if(Case == 2 || Case == 3) { OldRegion(LabelC, LabelB, LabelC); }\n                else if(Case == 5 || Case == 8) // Isolated\n                {\n                    if((jrow == Rows || jcol == Cols) && ColorD == FillColor) { OldRegion(0, -1, -1); } // attached to border region 0\n                    else NewRegion(LabelB);\n                }\n                else if(Case == 6 || Case == 7) { OldRegion(LabelB, LabelB, LabelC); }\n                else            // Case 4 - The complicated situation\n                {\n                    int LabelBRoot = SubsumptionChain(LabelB); \n                    int LabelCRoot = SubsumptionChain(LabelC);\n                    int LabelRoot = Math.min(LabelBRoot, LabelCRoot);\n                    int LabelX;\n                    if(LabelBRoot < LabelCRoot) { OldRegion(LabelB, -1, -1); LabelX = LabelC; }\n                    else { OldRegion(LabelC, -1, -1); LabelX = LabelB; }\n                    int NextLabelX = LabelX;\n                    while(LabelRoot < LabelX)\n                    {\n                        NextLabelX = SubsumedLabel[LabelX];\n                        SubsumedLabel[LabelX] = LabelRoot;\n                        LabelX = NextLabelX;\n                    }\n                }\n                    \n                // Last column or row. Final corner was handled earlier in Cases 5 and 8.\n                if((jrow == Rows || jcol == Cols) && ColorD == FillColor)\n                {\n                    if(jcol < Cols)         // bottom row   \n                    {\n                        if(ColorC != FillColor)     // Subsume B chain to border region 0\n                        {\n                            int LabelRoot = SubsumptionChain(LabelB);\n                            SubsumedLabel[LabelRoot] = 0;\n                        }\n                    }\n                    else if(jrow < Rows)    // right column\n                    {\n                        if(ColorB != FillColor)     // Subsume C chain to border region 0\n                        {\n                            int LabelRoot = SubsumptionChain(LabelC);\n                            SubsumedLabel[LabelRoot] = 0;\n                        }\n                    }\n                    OldRegion(0, -1, -1);   // attached to border region 0\n                }\n\n                LabelMat[jrow][jcol] = LabelD;\n                    \n            }\n        }\n\n        // Compute Condensation map\n        int Offset = 0;\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            if(SubsumedLabel[Label] > -1) Offset++;\n            CondensationMap[Label] = Label - Offset;\n        }\n\n        // Subsume regions that were flagged as connected; Perimeters add\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            int BetterLabel = SubsumptionChain(Label);\n            if(BetterLabel != Label) Subsume(BetterLabel, Label, 1);\n        }   \n\n        // Condense subsumed regions\n        int NewMaxLabel = 0;\n        for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n        {\n            if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n            {\n                double [] OldRegion = RegionData[OldLabel];\n                int OldParent = (int) OldRegion[BLOBPARENT];\n                int NewLabel = CondensationMap[OldLabel];\n                int NewParent = SubsumptionChain(OldParent);\n                NewParent = CondensationMap[NewParent];\n                OldRegion[BLOBLABEL] = (double) NewLabel;\n                OldRegion[BLOBPARENT] = (double) NewParent;\n                System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n                NewMaxLabel = NewLabel;\n            }\n        }\n    \n        // Zero out unneeded high labels\n        for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n        MaxLabel = NewMaxLabel;\n        \n        // Flag for subsumption regions that have too small area\n        for(int Label = MaxLabel; Label > 0; Label--)\n        {\n            double [] ThisRegion = RegionData[Label];\n            int ThisArea = (int) ThisRegion[BLOBAREA];\n            if(ThisArea < MinArea)\n            {\n                int ThisParent = (int) ThisRegion[BLOBPARENT];\n                SubsumedLabel[Label] =  ThisParent;             // Flag this label as having been subsumed\n            }\n            else SubsumedLabel[Label] =  -1;\n        }\n        \n        // Compute Condensation map\n        Offset = 0;\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            if(SubsumedLabel[Label] > -1) Offset++;\n            CondensationMap[Label] = Label - Offset;      \n        }\n\n        // Subsume regions that were flagged as enclosed; Perimeters subtract\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            int BetterLabel = SubsumptionChain(Label);\n            if(BetterLabel != Label) Subsume(BetterLabel, Label, -1);\n        }   \n    \n        // Condense subsumed regions\n        for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n        {\n            if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n            {\n                double [] OldRegion = RegionData[OldLabel];\n                int OldParent = (int) OldRegion[BLOBPARENT];\n                int NewLabel = CondensationMap[OldLabel];\n                int NewParent = SubsumptionChain(OldParent);\n                NewParent = CondensationMap[NewParent];\n                OldRegion[BLOBLABEL] = (double) NewLabel;\n                OldRegion[BLOBPARENT] = (double) NewParent;\n                System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n                NewMaxLabel = NewLabel;\n            }\n        }\n        \n        // Zero out unneeded high labels\n        for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n        MaxLabel = NewMaxLabel;\n\n        // Normalize summation fields into moments \n        for(int Label = 0; Label <= MaxLabel; Label++)\n        {\n            double [] ThisRegion = RegionData[Label];\n            \n            // Extract fields\n            double Area = ThisRegion[BLOBAREA];\n            double SumX = ThisRegion[BLOBSUMX];\n            double SumY = ThisRegion[BLOBSUMY];\n            double SumXX = ThisRegion[BLOBSUMXX];\n            double SumYY = ThisRegion[BLOBSUMYY];\n            double SumXY = ThisRegion[BLOBSUMXY];\n            \n            // Get averages\n            SumX /= Area;\n            SumY /= Area;\n            SumXX /= Area;\n            SumYY /= Area;\n            SumXY /= Area;\n            \n            // Create moments\n            SumXX -= SumX * SumX;\n            SumYY -= SumY * SumY;\n            SumXY -= SumX * SumY;\n            if(SumXY > -1.0E-14 && SumXY < 1.0E-14) SumXY = (float) 0.0; // Eliminate roundoff error\n\n            ThisRegion[BLOBSUMX] = SumX;\n            ThisRegion[BLOBSUMY] = SumY;\n            ThisRegion[BLOBSUMXX] = SumXX;\n            ThisRegion[BLOBSUMYY] = SumYY;\n            ThisRegion[BLOBSUMXY] = SumXY;\n\n            System.arraycopy(ThisRegion,0,RegionData[Label],0,BLOBDATACOUNT);   // RegionData[Label] <- ThisRegion;\n        }\n    \n        // Adjust border region\n        BorderRegion = RegionData[0];\n        BorderRegion[BLOBSUMXX] = BorderRegion[BLOBSUMYY] = BorderRegion[BLOBSUMXY] = 0;    // Mark invalid fields\n        System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n        \n        return MaxLabel;\n    }\n    \n    // Sort RegionData array on any column. (I couldn't figure out how to use the built-in java sort.)\n    static double iField, jField;\n    static double [] iProperty, jProperty;\n    public static void SortRegions(int Col)\n    {\n        for(int i = 0; i < MaxLabel; i++)\n        {\n            for(int j = i+1; j <= Blobs.MaxLabel; j++)\n            {\n                iProperty = RegionData[i];\n                jProperty = RegionData[j];\n                iField = iProperty[Col];\n                jField = jProperty[Col];\n                if(iField > jField)\n                {\n                    RegionData[i] = jProperty;\n                    RegionData[j] = iProperty;\n                }\n            }\n        }\n    }\n}\n\n\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\npublic class Blobs {\n    static int BLOBROWCOUNT;\n    static int BLOBCOLCOUNT;\n    static int BLOBTOTALCOUNT;\n    static public int BLOBLABEL;\n    static public int BLOBPARENT;\n    static public int BLOBCOLOR;\n    static public int BLOBAREA;\n    static public int BLOBPERIMETER;\n    static public int BLOBSUMX;\n    static public int BLOBSUMY;\n    static public int BLOBSUMXX;\n    static public int BLOBSUMYY;\n    static public int BLOBSUMXY;\n    static public int BLOBMINX;\n    static public int BLOBMAXX;\n    static public int BLOBMINY;\n    static public int BLOBMAXY;\n    static public int BLOBDATACOUNT;\n    static public int[][] LabelMat;\n    static public double[][] RegionData;\n    static public int MaxLabel;\n    public int LabelA, LabelB, LabelC, LabelD;\n    public int ColorA, ColorB, ColorC, ColorD;\n    public int jrow, jcol;\n    static public int[] SubsumedLabel;\n    static public int[] CondensationMap;\n    public  PrintRegionData();\n    public  PrintRegionData(int Label0, int Label1);\n    static public int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label);\n    static public int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label);\n    public  ResetRegion(int Label);\n    public  OldRegion(int NewLabelD, int Label1, int Label2);\n    public  NewRegion(int ParentLabel);\n    public  Subsume(int GoodLabel, int BadLabel, int PSign);\n    static public int SubsumptionChain(int x);\n    static public int SubsumptionChain(int x, int Print);\n    public int BlobAnalysis(IplImage Src, int Col0, int Row0, int Cols, int Rows, int Border, int MinArea);\n    static double iField, jField;\n    static double[] iProperty, jProperty;\n    static public  SortRegions(int Col);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "Blobs.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/Blobs.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": null,
        "source_code": "//---------------------------------------------------------------------------------------\n// Main blob analysis routine\n//---------------------------------------------------------------------------------------\n// RegionData[0] is the border. It has Property[BLOBPARENT] = 0. \n\npublic int BlobAnalysis(IplImage Src,           // input image\n            int Col0, int Row0,                 // start of ROI\n            int Cols, int Rows,                 // size of ROI\n            int Border,                         // border color (0 = black; 1 = white)\n            int MinArea)                        // minimum region area\n{\n    CvMat SrcMat = Src.asCvMat();\n    int SrcCols = SrcMat.cols();\n    int SrcRows = SrcMat.rows();\n    \n    if(Col0 < 0) Col0 = 0;\n    if(Row0 < 0) Row0 = 0;\n    if(Cols < 0) Cols = SrcCols;\n    if(Rows < 0) Rows = SrcRows;\n    if(Col0 + Cols > SrcCols) Cols = SrcCols - Col0;\n    if(Row0 + Rows > SrcRows) Rows = SrcRows - Row0;\n\n    if(Cols > BLOBCOLCOUNT || Rows > BLOBROWCOUNT )\n    {\n        System.out.println(\"Error in Class Blobs: Image too large: Edit Blobs.java\");\n        System.exit(666);\n        return 0;\n    }\n    \n    // Initialization\n    int FillLabel = 0;\n    int FillColor = 0; if(Border > 0) { FillColor = 1; }\n    LabelA = LabelB = LabelC = LabelD = 0;\n    ColorA = ColorB = ColorC = ColorD = FillColor;\n    for(int k = 0; k < BLOBTOTALCOUNT; k++) SubsumedLabel[k] = -1;\n    \n    // Initialize border region\n    MaxLabel = 0;\n    double [] BorderRegion = RegionData[0];\n    BorderRegion[BLOBLABEL] = 0.0;\n    BorderRegion[BLOBPARENT] = -1.0;\n    BorderRegion[BLOBAREA] = Rows + Cols + 4;   // Top, left, and 4 corners\n    BorderRegion[BLOBCOLOR] = FillColor;\n    BorderRegion[BLOBSUMX] = 0.5 * ( (2.0 + Cols) * (Cols - 1.0) ) - Rows - 1 ;\n    BorderRegion[BLOBSUMY] = 0.5 * ( (2.0 + Rows) * (Rows - 1.0) ) - Cols - 1 ;\n    BorderRegion[BLOBMINX] = -1;\n    BorderRegion[BLOBMINY] = -1;\n    BorderRegion[BLOBMAXX] = Cols + 1.0;\n    BorderRegion[BLOBMAXY] = Rows + 1.0;\n    System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n    \n    //  The cells are identified this way\n    //          Last |AB|\n    //          This |CD|\n    //\n    // With 4 connectivity, there are 8 possibilities for the cells:\n    //                      No color transition     Color transition\n    //          Case              1  2  3  4          5  6  7  8 \n    //          Last Row        |pp|pp|pq|pq|       |pp|pp|pq|pq|   \n    //          This Row        |pP|qQ|pP|qQ|       |pQ|qP|pQ|qP|\n    //\n    // Region numbers are p, q, r, x; where p<>q\n    // Upper case letter is the current element at column=x row=y\n    // Color is 0 or 1      (1 stands for 255 in the actual image)\n    // Note that Case 4 is complicated because it joins two regions\n    //--------------------------\n    // Case 1: Colors A=B; C=D; A=C     \n    // Case 2: Colors A=B; C=D; A<>C    \n    // Case 3: Colors A<>B;C=D; A=C     \n    // Case 4: Colors A<>B;C=D; A<>C    \n    // Case 5: Colors A=B; C<>D; A=C    \n    // Case 6: Colors A=B; C<>D; A<>C   \n    // Case 7: Colors A<>B;C<>D; A=C    \n    // Case 8: Colors A<>B;C<>D; A<>C   \n    //--------------------------\n                \n    // Loop over rows of ROI. irow = Row0 is 1st row of image; irow = Row0+Row is last row of image.\n    for(int irow = Row0; irow < Row0+Rows; irow++)  // index within Src\n    {\n        jrow = irow - Row0; // index within ROI. 0 is first row. Rows is last row.\n        \n        // Loop over columns of ROI.\n        for(int icol = Col0; icol < Col0+Cols; icol++)  // index within Src\n        {\n            jcol = icol - Col0; // index within ROI\n\n            // initialize\n            ColorA = ColorB = ColorC = FillColor;\n            LabelA = LabelB = LabelC = LabelD = 0;\n            ColorD = (int) SrcMat.get(jrow,jcol);       // fetch color of cell\n        \n            if(jrow == 0 || jcol == 0)  // first column or row\n            {\n                if(jcol > 0)\n                {\n                    ColorC = (int) SrcMat.get(jrow,jcol-1);\n                    LabelC = LabelMat[jrow][jcol-1];\n                }\n                if(jrow > 0)\n                {\n                    ColorB = (int) SrcMat.get(jrow-1,jcol);\n                    LabelB = LabelMat[jrow-1][jcol];\n                }\n            }\n            else\n            {\n                ColorA = (int) SrcMat.get(jrow-1,jcol-1); if(ColorA > 0) ColorA = 1;\n                ColorB = (int) SrcMat.get(jrow-1,jcol); if(ColorB > 0) ColorB = 1;\n                ColorC = (int) SrcMat.get(jrow,jcol-1); if(ColorC > 0) ColorC = 1;\n                LabelA = LabelMat[jrow-1][jcol-1];\n                LabelB = LabelMat[jrow-1][jcol];\n                LabelC = LabelMat[jrow][jcol-1];\n            }   \n            if(ColorA > 0) ColorA = 1;\n            if(ColorB > 0) ColorB = 1;\n            if(ColorC > 0) ColorC = 1;\n            if(ColorD > 0) ColorD = 1;\n                \n            // Determine Case\n            int Case = 0;\n            if(ColorA == ColorB)\n            {\n                if(ColorC == ColorD) { if(ColorA == ColorC) Case = 1; else Case = 2; }\n                else { if(ColorA == ColorC) Case = 5; else Case = 6; }\n            }\n            else\n            {\n                if(ColorC == ColorD) { if(ColorA == ColorC) Case = 3; else Case = 4; }\n                else { if(ColorA == ColorC) Case = 7; else Case = 8; }\n            }\n\n            // Take appropriate action\n            if(Case == 1) { OldRegion(LabelC, -1, -1); }\n            else if(Case == 2 || Case == 3) { OldRegion(LabelC, LabelB, LabelC); }\n            else if(Case == 5 || Case == 8) // Isolated\n            {\n                if((jrow == Rows || jcol == Cols) && ColorD == FillColor) { OldRegion(0, -1, -1); } // attached to border region 0\n                else NewRegion(LabelB);\n            }\n            else if(Case == 6 || Case == 7) { OldRegion(LabelB, LabelB, LabelC); }\n            else            // Case 4 - The complicated situation\n            {\n                int LabelBRoot = SubsumptionChain(LabelB); \n                int LabelCRoot = SubsumptionChain(LabelC);\n                int LabelRoot = Math.min(LabelBRoot, LabelCRoot);\n                int LabelX;\n                if(LabelBRoot < LabelCRoot) { OldRegion(LabelB, -1, -1); LabelX = LabelC; }\n                else { OldRegion(LabelC, -1, -1); LabelX = LabelB; }\n                int NextLabelX = LabelX;\n                while(LabelRoot < LabelX)\n                {\n                    NextLabelX = SubsumedLabel[LabelX];\n                    SubsumedLabel[LabelX] = LabelRoot;\n                    LabelX = NextLabelX;\n                }\n            }\n                \n            // Last column or row. Final corner was handled earlier in Cases 5 and 8.\n            if((jrow == Rows || jcol == Cols) && ColorD == FillColor)\n            {\n                if(jcol < Cols)         // bottom row   \n                {\n                    if(ColorC != FillColor)     // Subsume B chain to border region 0\n                    {\n                        int LabelRoot = SubsumptionChain(LabelB);\n                        SubsumedLabel[LabelRoot] = 0;\n                    }\n                }\n                else if(jrow < Rows)    // right column\n                {\n                    if(ColorB != FillColor)     // Subsume C chain to border region 0\n                    {\n                        int LabelRoot = SubsumptionChain(LabelC);\n                        SubsumedLabel[LabelRoot] = 0;\n                    }\n                }\n                OldRegion(0, -1, -1);   // attached to border region 0\n            }\n\n            LabelMat[jrow][jcol] = LabelD;\n                \n        }\n    }\n\n    // Compute Condensation map\n    int Offset = 0;\n    for(int Label = 1; Label <= MaxLabel; Label++)\n    {\n        if(SubsumedLabel[Label] > -1) Offset++;\n        CondensationMap[Label] = Label - Offset;\n    }\n\n    // Subsume regions that were flagged as connected; Perimeters add\n    for(int Label = 1; Label <= MaxLabel; Label++)\n    {\n        int BetterLabel = SubsumptionChain(Label);\n        if(BetterLabel != Label) Subsume(BetterLabel, Label, 1);\n    }   \n\n    // Condense subsumed regions\n    int NewMaxLabel = 0;\n    for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n    {\n        if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n        {\n            double [] OldRegion = RegionData[OldLabel];\n            int OldParent = (int) OldRegion[BLOBPARENT];\n            int NewLabel = CondensationMap[OldLabel];\n            int NewParent = SubsumptionChain(OldParent);\n            NewParent = CondensationMap[NewParent];\n            OldRegion[BLOBLABEL] = (double) NewLabel;\n            OldRegion[BLOBPARENT] = (double) NewParent;\n            System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n            NewMaxLabel = NewLabel;\n        }\n    }\n\n    // Zero out unneeded high labels\n    for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n    MaxLabel = NewMaxLabel;\n    \n    // Flag for subsumption regions that have too small area\n    for(int Label = MaxLabel; Label > 0; Label--)\n    {\n        double [] ThisRegion = RegionData[Label];\n        int ThisArea = (int) ThisRegion[BLOBAREA];\n        if(ThisArea < MinArea)\n        {\n            int ThisParent = (int) ThisRegion[BLOBPARENT];\n            SubsumedLabel[Label] =  ThisParent;             // Flag this label as having been subsumed\n        }\n        else SubsumedLabel[Label] =  -1;\n    }\n    \n    // Compute Condensation map\n    Offset = 0;\n    for(int Label = 1; Label <= MaxLabel; Label++)\n    {\n        if(SubsumedLabel[Label] > -1) Offset++;\n        CondensationMap[Label] = Label - Offset;      \n    }\n\n    // Subsume regions that were flagged as enclosed; Perimeters subtract\n    for(int Label = 1; Label <= MaxLabel; Label++)\n    {\n        int BetterLabel = SubsumptionChain(Label);\n        if(BetterLabel != Label) Subsume(BetterLabel, Label, -1);\n    }   \n\n    // Condense subsumed regions\n    for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n    {\n        if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n        {\n            double [] OldRegion = RegionData[OldLabel];\n            int OldParent = (int) OldRegion[BLOBPARENT];\n            int NewLabel = CondensationMap[OldLabel];\n            int NewParent = SubsumptionChain(OldParent);\n            NewParent = CondensationMap[NewParent];\n            OldRegion[BLOBLABEL] = (double) NewLabel;\n            OldRegion[BLOBPARENT] = (double) NewParent;\n            System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n            NewMaxLabel = NewLabel;\n        }\n    }\n    \n    // Zero out unneeded high labels\n    for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n    MaxLabel = NewMaxLabel;\n\n    // Normalize summation fields into moments \n    for(int Label = 0; Label <= MaxLabel; Label++)\n    {\n        double [] ThisRegion = RegionData[Label];\n        \n        // Extract fields\n        double Area = ThisRegion[BLOBAREA];\n        double SumX = ThisRegion[BLOBSUMX];\n        double SumY = ThisRegion[BLOBSUMY];\n        double SumXX = ThisRegion[BLOBSUMXX];\n        double SumYY = ThisRegion[BLOBSUMYY];\n        double SumXY = ThisRegion[BLOBSUMXY];\n        \n        // Get averages\n        SumX /= Area;\n        SumY /= Area;\n        SumXX /= Area;\n        SumYY /= Area;\n        SumXY /= Area;\n        \n        // Create moments\n        SumXX -= SumX * SumX;\n        SumYY -= SumY * SumY;\n        SumXY -= SumX * SumY;\n        if(SumXY > -1.0E-14 && SumXY < 1.0E-14) SumXY = (float) 0.0; // Eliminate roundoff error\n\n        ThisRegion[BLOBSUMX] = SumX;\n        ThisRegion[BLOBSUMY] = SumY;\n        ThisRegion[BLOBSUMXX] = SumXX;\n        ThisRegion[BLOBSUMYY] = SumYY;\n        ThisRegion[BLOBSUMXY] = SumXY;\n\n        System.arraycopy(ThisRegion,0,RegionData[Label],0,BLOBDATACOUNT);   // RegionData[Label] <- ThisRegion;\n    }\n\n    // Adjust border region\n    BorderRegion = RegionData[0];\n    BorderRegion[BLOBSUMXX] = BorderRegion[BLOBSUMYY] = BorderRegion[BLOBSUMXY] = 0;    // Mark invalid fields\n    System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n    \n    return MaxLabel;\n}\n",
        "class_name": "Blobs",
        "method_name": "BlobAnalysis",
        "argument_name": [
            "IplImage Src",
            "int Col0",
            "int Row0",
            "int Cols",
            "int Rows",
            "int Border",
            "int MinArea"
        ],
        "full_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\n\n//***************************************************************//\n//* Blob analysis package  Version3.0 3 Oct 2012                *//\n//* - Version 1.0:  8 Aug 2003                                  *//\n//* - Version 1.2:  3 Jan 2008                                  *//\n//* - Version 1.3:  5 Jan 2008 Add BLOBCOLOR                    *//\n//* - Version 1.4: 13 January 2008 Add ROI function             *//\n//* - Version 1.5: 13 April 2008 Fix perimeter on Region 0      *//\n//* - Version 1.6:  1 May 2008 Reduce size of working storage   *//\n//* - Version 1.7:  2 May 2008 Speed up run code initialization *//\n//* - Version 1.8:  4 May 2008 Fix bugs in perimeter & Reg 0    *//\n//* - Version 2.0:  3 Jan 2009 Add labeling functionality       *//\n//* - Version 3.0:  3 Oct 2012 Convert to Java                  *//\n//* -   Eliminate labeling functionality (but it's still there) *//\n//* -   Simplify (at slight expense of performance)             *//\n//* -   Reduce to 4 connectivity                                *//\n//*                                                             *//\n//* Input: IplImage binary image                                *//\n//* Output: attributes of each connected region                 *//\n//* Internal data: labeled array (could easily be externalized) *//\n//* Author: Dave Grossman                                       *//\n//* Email: dgrossman2@gmail.com                                 *//\n//* Acknowledgement: my code is based on an algorithm that was  *//\n//* to the best of my knowledge originally developed by Gerry   *//\n//* Agin of SRI around the year 1973. I have not been able to   *//\n//* find any published references to his earlier work. I posted *//\n//* early versions of my program to OpenCV, where they morphed  *//\n//* eventually into cvBlobsLib.                                 *//\n//*                                                             *//\n//* As the author of this code, I place all of this code into   *//\n//* the public domain. Users can use it for any legal purpose.  *//\n//*                                                             *//\n//*             - Dave Grossman                                 *//\n//*                                                             *//\n//* Typical calling sequence:                                   *//\n//*     Blobs Blob = new Blobs();                               *//\n//*     Blob.BlobAnalysis(                                      *//\n//*         image3, // image                                    *//\n//*         -1, -1, // ROI start col, row (-1 means full image) *//\n//*         -1, -1, // ROI cols, rows                           *//\n//*         0,      // border (0 = black; 1 = white)            *//\n//*         20);    // minarea                                  *//\n//*     Blob.PrintRegionData();                                 *//\n//*     int BlobLabel = Blob.NextRegion(                        *//\n//*         -1,     // parentcolor (-1 = ignore)                *//\n//*         0,      // color (0 = black; 1 = white; -1 = ignore *//\n//*         100,    // minarea                                  *//\n//*         500,    // maxarea                                  *//\n//*         15);    // starting label (default 0)               *//\n//*                                                             *//\n//* Ellipse properties can be derived from moments:             *//\n//*     h = (XX + YY) / 2                                       *//\n//*     Major axis = h + sqrt ( h^2 - XX * YY + XY^2)           *//\n//*     Minor axis = h - sqrt ( h^2 - XX * YY2 + XY^2)          *//\n//*     Eccentricity = (sqrt(abs(XX - YY)) + 4 * XY)/AREA       *//\n//***************************************************************//\n\npublic class Blobs\n{\n    // The following parameters should be configured by the user:\n    // On ScanSnap Manager, \"Best\" setting = 300dpi gray level\n    // jpg compression is set to minimum so that quality is highest\n    // Each page jpg image is then a little under 1 MB \n    static int BLOBROWCOUNT = 3500; // 11 inches * 8.5 inches standard page\n    static int BLOBCOLCOUNT = 2700; // with some added cushion to be safe\n    \n    // Allow for vast number of blobs so there is no memory overrun\n    static int BLOBTOTALCOUNT = (BLOBROWCOUNT + BLOBCOLCOUNT) * 5;\n\n    //--------------------------------------------------------------\n    // Do not change anything below this line\n    public static int BLOBLABEL = 0;\n    public static int BLOBPARENT = 1;\n    public static int BLOBCOLOR = 2;\n    public static int BLOBAREA = 3;\n    public static int BLOBPERIMETER = 4;\n    public static int BLOBSUMX = 5;\n    public static int BLOBSUMY = 6;\n    public static int BLOBSUMXX = 7;\n    public static int BLOBSUMYY = 8;\n    public static int BLOBSUMXY = 9;\n    public static int BLOBMINX = 10;\n    public static int BLOBMAXX = 11;\n    public static int BLOBMINY = 12;\n    public static int BLOBMAXY = 13;\n    public static int BLOBDATACOUNT = 14; \n\n    public static int [][] LabelMat = new int [BLOBROWCOUNT][BLOBCOLCOUNT];\n    public static double [][] RegionData = new double [BLOBTOTALCOUNT][BLOBDATACOUNT];\n    public static int MaxLabel; \n    \n    public int LabelA, LabelB, LabelC, LabelD;\n    public int ColorA, ColorB, ColorC, ColorD;\n    public int jrow, jcol;  // index within ROI\n    public static int [] SubsumedLabel = new int [BLOBTOTALCOUNT];\n    public static int [] CondensationMap = new int [BLOBTOTALCOUNT];\n    \n    // Print out all the data for all the regions (blobs)\n    public void PrintRegionData() { PrintRegionData(0, MaxLabel); }\n    public void PrintRegionData(int Label0, int Label1)\n    {\n        if(Label0 < 0) Label0 = 0;\n        if(Label1 > MaxLabel) Label1 = MaxLabel;\n        if(Label1 < Label0) return;\n        for(int Label = Label0; Label <= Label1; Label++)\n        {\n            double [] Property = RegionData[Label];\n            \n            int ThisLabel = (int)Property[BLOBLABEL];\n            int ThisParent = (int)Property[BLOBPARENT];\n            int ThisColor = (int)Property[BLOBCOLOR];\n            double ThisArea = Property[BLOBAREA];\n            double ThisPerimeter = Property[BLOBPERIMETER];\n            double ThisSumX = Property[BLOBSUMX];\n            double ThisSumY = Property[BLOBSUMY];\n            double ThisSumXX = Property[BLOBSUMXX];\n            double ThisSumYY = Property[BLOBSUMYY];\n            double ThisSumXY = Property[BLOBSUMXY];\n            int ThisMinX = (int)Property[BLOBMINX];\n            int ThisMaxX = (int)Property[BLOBMAXX];\n            int ThisMinY = (int)Property[BLOBMINY];\n            int ThisMaxY = (int)Property[BLOBMAXY];\n            \n            String Str1 = \" \" + Label + \": L[\" + ThisLabel + \"] P[\" + ThisParent + \"] C[\" + ThisColor + \"]\";\n            String Str2 = \" AP[\" + ThisArea + \", \" + ThisPerimeter + \"]\";\n            String Str3 = \" M1[\" + ThisSumX + \", \" + ThisSumY + \"] M2[\" + ThisSumXX + \", \" + ThisSumYY + \", \" + ThisSumXY + \"]\";\n            String Str4 = \" MINMAX[\" + ThisMinX + \", \" + ThisMaxX + \", \" + ThisMinY + \", \" + ThisMaxY + \"]\";\n            \n            String Str = Str1 + Str2 + Str3 + Str4;\n            System.out.println(Str);\n        }\n        System.out.println();\n    }\n\n    // Determine the next (higher number) region that meets the desired conditions\n    public static int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n    {\n        double DParent = (double) Parent; \n        double DColor = (double) Color; if(DColor > 0) DColor = 1;\n        \n        int i;\n        for(i = Label; i <= MaxLabel; i++)\n        {\n            double [] Region = RegionData[i];\n            double ThisParent = Region[BLOBPARENT];\n            double ThisColor = Region[BLOBCOLOR];\n            if(DParent >= 0 && DParent != ThisParent) continue;\n            if(DColor >= 0 && DColor != ThisColor) continue;\n            if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n            break;      // We have a match!\n        }\n        if(i > MaxLabel) i = -1;    // Use -1 to flag that there was no match\n        return i;\n    }\n\n    // Determine the prior (lower number) region that meets the desired conditions\n    public static int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label)\n    {\n        double DParent = (double) Parent; \n        double DColor = (double) Color; if(DColor > 0) DColor = 1;\n        \n        int i;\n        for(i = Label; i >= 0; i--)\n        {\n            double [] Region = RegionData[i];\n            double ThisParent = Region[BLOBPARENT];\n            double ThisColor = Region[BLOBCOLOR];\n            if(DParent >= 0 && DParent != ThisParent) continue;\n            if(DColor >= 0 && DColor != ThisColor) continue;\n            if(Region[BLOBAREA] < MinArea || Region[BLOBAREA] > MaxArea) continue;  \n            break;      // We have a match!\n        }\n        if(i < 0) i = -1;   // Use -1 to flag that there was no match\n        return i;\n    }\n    \n    public void ResetRegion(int Label)\n    {\n        double [] RegionD = RegionData[Label];\n        RegionD[BLOBLABEL] = \n        RegionD[BLOBPARENT] = \n        RegionD[BLOBCOLOR] =\n        RegionD[BLOBAREA] =\n        RegionD[BLOBPERIMETER] =\n        RegionD[BLOBSUMX] =\n        RegionD[BLOBSUMY] = \n        RegionD[BLOBSUMXX] = \n        RegionD[BLOBSUMYY] = \n        RegionD[BLOBSUMXY] = \n        RegionD[BLOBMINX] = \n        RegionD[BLOBMAXX] = \n        RegionD[BLOBMINY] = \n        RegionD[BLOBMAXY] = 0.0;\n        System.arraycopy(RegionD,0,RegionData[Label],0,BLOBDATACOUNT);  // RegionData[Label] <- RegionD;\n    }\n    \n    public void OldRegion(\n            int NewLabelD,  // 3rd update this (may be the same as Label1 or Label2)\n            int Label1,     // 1st increment this by 1 \n            int Label2)     // 2nd increment this by 1\n    {\n        int DeltaPerimeter = 0;\n        \n        if(Label1 >= 0 && Label1 != NewLabelD)\n        {\n            DeltaPerimeter++;\n            double [] Region1 = RegionData[Label1];\n            Region1[BLOBPERIMETER]++;\n            System.arraycopy(Region1,0,RegionData[Label1],0,BLOBDATACOUNT); // RegionData[Label1] <- Region1;\n        }\n        \n        if(Label2 >= 0 && Label2 != NewLabelD)\n        {\n            DeltaPerimeter++;\n            double [] Region2 = RegionData[Label2];\n            Region2[BLOBPERIMETER]++;\n            System.arraycopy(Region2,0,RegionData[Label2],0,BLOBDATACOUNT); // RegionData[Label2] <- Region2;\n        }\n        \n        LabelD = NewLabelD;\n        double [] RegionD = RegionData[LabelD];\n        RegionD[BLOBLABEL] = LabelD;\n        RegionD[BLOBPARENT] += 0.0;     // no change\n        RegionD[BLOBCOLOR] += 0.0;      // no change\n        RegionD[BLOBAREA] += 1.0;\n        RegionD[BLOBPERIMETER] += DeltaPerimeter;\n        RegionD[BLOBSUMX] += jcol;\n        RegionD[BLOBSUMY] += jrow;\n        RegionD[BLOBSUMXX] += jcol*jcol;\n        RegionD[BLOBSUMYY] += jrow*jrow;\n        RegionD[BLOBSUMXY] += jcol*jrow;\n        RegionD[BLOBMINX] = Math.min(RegionD[BLOBMINX], jcol);\n        RegionD[BLOBMAXX] = Math.max(RegionD[BLOBMAXX], jcol);\n        RegionD[BLOBMINY] = Math.min(RegionD[BLOBMINY], jrow);\n        RegionD[BLOBMAXY] = Math.max(RegionD[BLOBMAXY], jrow);\n        System.arraycopy(RegionD,0,RegionData[LabelD],0,BLOBDATACOUNT); // RegionData[LabelD] <- RegionD;\n   }\n    \n    public void NewRegion(int ParentLabel)\n    {\n        LabelD = ++MaxLabel;\n        double [] RegionD = RegionData[LabelD];\n        RegionD[BLOBLABEL] = LabelD;\n        RegionD[BLOBPARENT] = (double) ParentLabel;\n        RegionD[BLOBCOLOR] = ColorD;\n        RegionD[BLOBAREA] = 1.0;\n        RegionD[BLOBPERIMETER] = 2.0;\n        RegionD[BLOBSUMX] = jcol;\n        RegionD[BLOBSUMY] = jrow;\n        RegionD[BLOBSUMXX] = jcol*jcol;\n        RegionD[BLOBSUMYY] = jrow*jrow;\n        RegionD[BLOBSUMXY] = jcol*jrow;\n        RegionD[BLOBMINX] = jcol;\n        RegionD[BLOBMAXX] = jcol;\n        RegionD[BLOBMINY] = jrow;\n        RegionD[BLOBMAXY] = jrow;\n\n        System.arraycopy(RegionD,0,RegionData[LabelD],0,BLOBDATACOUNT); // RegionData[LabelD] <- RegionD;\n        SubsumedLabel[LabelD] = -1;     // Flag label as not subsumed\n\n        double [] RegionB = RegionData[LabelB];\n        RegionB[BLOBPERIMETER]++;\n        System.arraycopy(RegionB,0,RegionData[LabelB],0,BLOBDATACOUNT); // RegionData[LabelB] <- RegionB;\n        \n        double [] RegionC = RegionData[LabelC];\n        RegionC[BLOBPERIMETER]++;\n\n        System.arraycopy(RegionC,0,RegionData[LabelC],0,BLOBDATACOUNT); // RegionData[LabelC] <- RegionC;\n    }\n    \n    public void Subsume(int GoodLabel, int BadLabel, int PSign) // Combine data with parent\n    {\n        LabelD = GoodLabel;\n        double [] GoodRegion = RegionData[GoodLabel];   \n        double [] BadRegion = RegionData[BadLabel];\n    \n        GoodRegion[BLOBLABEL] = GoodRegion[BLOBLABEL];      // no change\n        GoodRegion[BLOBPARENT] = GoodRegion[BLOBPARENT];    // no change\n        GoodRegion[BLOBCOLOR] = GoodRegion[BLOBCOLOR];      // no change\n        GoodRegion[BLOBAREA] += BadRegion[BLOBAREA];\n        GoodRegion[BLOBPERIMETER] += BadRegion[BLOBPERIMETER] * PSign;  // + external or - internal perimeter\n        GoodRegion[BLOBSUMX] += BadRegion[BLOBSUMX];\n        GoodRegion[BLOBSUMY] += BadRegion[BLOBSUMY];\n        GoodRegion[BLOBSUMXX] += BadRegion[BLOBSUMXX];\n        GoodRegion[BLOBSUMYY] += BadRegion[BLOBSUMYY];\n        GoodRegion[BLOBSUMXY] += BadRegion[BLOBSUMXY];\n        GoodRegion[BLOBMINX] = Math.min(GoodRegion[BLOBMINX], BadRegion[BLOBMINX]);\n        GoodRegion[BLOBMAXX] = Math.max(GoodRegion[BLOBMAXX], BadRegion[BLOBMAXX]);\n        GoodRegion[BLOBMINY] = Math.min(GoodRegion[BLOBMINY], BadRegion[BLOBMINY]);\n        GoodRegion[BLOBMAXY] = Math.max(GoodRegion[BLOBMAXY], BadRegion[BLOBMAXY]);\n        \n        System.arraycopy(GoodRegion,0,RegionData[GoodLabel],0,BLOBDATACOUNT);   // RegionData[GoodLabel] <- GoodRegion;\n    }\n\n    public static int SubsumptionChain(int x) { return SubsumptionChain(x, 0); }\n    public static int SubsumptionChain(int x, int Print)\n    {\n        String Str = \"\";\n        if(Print > 0) Str = \"Subsumption chain for \" + x + \": \";\n        int Lastx = x;\n        while(x > -1)\n        {\n            Lastx = x;\n            if(Print > 0) Str += \" \" + x;\n            if(x == 0) break;\n            x = SubsumedLabel[x];\n        }\n        if(Print > 0) System.out.println(Str);\n        return Lastx;\n    }\n\n    //---------------------------------------------------------------------------------------\n    // Main blob analysis routine\n    //---------------------------------------------------------------------------------------\n    // RegionData[0] is the border. It has Property[BLOBPARENT] = 0. \n\n    public int BlobAnalysis(IplImage Src,           // input image\n                int Col0, int Row0,                 // start of ROI\n                int Cols, int Rows,                 // size of ROI\n                int Border,                         // border color (0 = black; 1 = white)\n                int MinArea)                        // minimum region area\n    {\n        CvMat SrcMat = Src.asCvMat();\n        int SrcCols = SrcMat.cols();\n        int SrcRows = SrcMat.rows();\n        \n        if(Col0 < 0) Col0 = 0;\n        if(Row0 < 0) Row0 = 0;\n        if(Cols < 0) Cols = SrcCols;\n        if(Rows < 0) Rows = SrcRows;\n        if(Col0 + Cols > SrcCols) Cols = SrcCols - Col0;\n        if(Row0 + Rows > SrcRows) Rows = SrcRows - Row0;\n\n        if(Cols > BLOBCOLCOUNT || Rows > BLOBROWCOUNT )\n        {\n            System.out.println(\"Error in Class Blobs: Image too large: Edit Blobs.java\");\n            System.exit(666);\n            return 0;\n        }\n        \n        // Initialization\n        int FillLabel = 0;\n        int FillColor = 0; if(Border > 0) { FillColor = 1; }\n        LabelA = LabelB = LabelC = LabelD = 0;\n        ColorA = ColorB = ColorC = ColorD = FillColor;\n        for(int k = 0; k < BLOBTOTALCOUNT; k++) SubsumedLabel[k] = -1;\n        \n        // Initialize border region\n        MaxLabel = 0;\n        double [] BorderRegion = RegionData[0];\n        BorderRegion[BLOBLABEL] = 0.0;\n        BorderRegion[BLOBPARENT] = -1.0;\n        BorderRegion[BLOBAREA] = Rows + Cols + 4;   // Top, left, and 4 corners\n        BorderRegion[BLOBCOLOR] = FillColor;\n        BorderRegion[BLOBSUMX] = 0.5 * ( (2.0 + Cols) * (Cols - 1.0) ) - Rows - 1 ;\n        BorderRegion[BLOBSUMY] = 0.5 * ( (2.0 + Rows) * (Rows - 1.0) ) - Cols - 1 ;\n        BorderRegion[BLOBMINX] = -1;\n        BorderRegion[BLOBMINY] = -1;\n        BorderRegion[BLOBMAXX] = Cols + 1.0;\n        BorderRegion[BLOBMAXY] = Rows + 1.0;\n        System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n        \n        //  The cells are identified this way\n        //          Last |AB|\n        //          This |CD|\n        //\n        // With 4 connectivity, there are 8 possibilities for the cells:\n        //                      No color transition     Color transition\n        //          Case              1  2  3  4          5  6  7  8 \n        //          Last Row        |pp|pp|pq|pq|       |pp|pp|pq|pq|   \n        //          This Row        |pP|qQ|pP|qQ|       |pQ|qP|pQ|qP|\n        //\n        // Region numbers are p, q, r, x; where p<>q\n        // Upper case letter is the current element at column=x row=y\n        // Color is 0 or 1      (1 stands for 255 in the actual image)\n        // Note that Case 4 is complicated because it joins two regions\n        //--------------------------\n        // Case 1: Colors A=B; C=D; A=C     \n        // Case 2: Colors A=B; C=D; A<>C    \n        // Case 3: Colors A<>B;C=D; A=C     \n        // Case 4: Colors A<>B;C=D; A<>C    \n        // Case 5: Colors A=B; C<>D; A=C    \n        // Case 6: Colors A=B; C<>D; A<>C   \n        // Case 7: Colors A<>B;C<>D; A=C    \n        // Case 8: Colors A<>B;C<>D; A<>C   \n        //--------------------------\n                    \n        // Loop over rows of ROI. irow = Row0 is 1st row of image; irow = Row0+Row is last row of image.\n        for(int irow = Row0; irow < Row0+Rows; irow++)  // index within Src\n        {\n            jrow = irow - Row0; // index within ROI. 0 is first row. Rows is last row.\n            \n            // Loop over columns of ROI.\n            for(int icol = Col0; icol < Col0+Cols; icol++)  // index within Src\n            {\n                jcol = icol - Col0; // index within ROI\n \n                // initialize\n                ColorA = ColorB = ColorC = FillColor;\n                LabelA = LabelB = LabelC = LabelD = 0;\n                ColorD = (int) SrcMat.get(jrow,jcol);       // fetch color of cell\n            \n                if(jrow == 0 || jcol == 0)  // first column or row\n                {\n                    if(jcol > 0)\n                    {\n                        ColorC = (int) SrcMat.get(jrow,jcol-1);\n                        LabelC = LabelMat[jrow][jcol-1];\n                    }\n                    if(jrow > 0)\n                    {\n                        ColorB = (int) SrcMat.get(jrow-1,jcol);\n                        LabelB = LabelMat[jrow-1][jcol];\n                    }\n                }\n                else\n                {\n                    ColorA = (int) SrcMat.get(jrow-1,jcol-1); if(ColorA > 0) ColorA = 1;\n                    ColorB = (int) SrcMat.get(jrow-1,jcol); if(ColorB > 0) ColorB = 1;\n                    ColorC = (int) SrcMat.get(jrow,jcol-1); if(ColorC > 0) ColorC = 1;\n                    LabelA = LabelMat[jrow-1][jcol-1];\n                    LabelB = LabelMat[jrow-1][jcol];\n                    LabelC = LabelMat[jrow][jcol-1];\n                }   \n                if(ColorA > 0) ColorA = 1;\n                if(ColorB > 0) ColorB = 1;\n                if(ColorC > 0) ColorC = 1;\n                if(ColorD > 0) ColorD = 1;\n                    \n                // Determine Case\n                int Case = 0;\n                if(ColorA == ColorB)\n                {\n                    if(ColorC == ColorD) { if(ColorA == ColorC) Case = 1; else Case = 2; }\n                    else { if(ColorA == ColorC) Case = 5; else Case = 6; }\n                }\n                else\n                {\n                    if(ColorC == ColorD) { if(ColorA == ColorC) Case = 3; else Case = 4; }\n                    else { if(ColorA == ColorC) Case = 7; else Case = 8; }\n                }\n\n                // Take appropriate action\n                if(Case == 1) { OldRegion(LabelC, -1, -1); }\n                else if(Case == 2 || Case == 3) { OldRegion(LabelC, LabelB, LabelC); }\n                else if(Case == 5 || Case == 8) // Isolated\n                {\n                    if((jrow == Rows || jcol == Cols) && ColorD == FillColor) { OldRegion(0, -1, -1); } // attached to border region 0\n                    else NewRegion(LabelB);\n                }\n                else if(Case == 6 || Case == 7) { OldRegion(LabelB, LabelB, LabelC); }\n                else            // Case 4 - The complicated situation\n                {\n                    int LabelBRoot = SubsumptionChain(LabelB); \n                    int LabelCRoot = SubsumptionChain(LabelC);\n                    int LabelRoot = Math.min(LabelBRoot, LabelCRoot);\n                    int LabelX;\n                    if(LabelBRoot < LabelCRoot) { OldRegion(LabelB, -1, -1); LabelX = LabelC; }\n                    else { OldRegion(LabelC, -1, -1); LabelX = LabelB; }\n                    int NextLabelX = LabelX;\n                    while(LabelRoot < LabelX)\n                    {\n                        NextLabelX = SubsumedLabel[LabelX];\n                        SubsumedLabel[LabelX] = LabelRoot;\n                        LabelX = NextLabelX;\n                    }\n                }\n                    \n                // Last column or row. Final corner was handled earlier in Cases 5 and 8.\n                if((jrow == Rows || jcol == Cols) && ColorD == FillColor)\n                {\n                    if(jcol < Cols)         // bottom row   \n                    {\n                        if(ColorC != FillColor)     // Subsume B chain to border region 0\n                        {\n                            int LabelRoot = SubsumptionChain(LabelB);\n                            SubsumedLabel[LabelRoot] = 0;\n                        }\n                    }\n                    else if(jrow < Rows)    // right column\n                    {\n                        if(ColorB != FillColor)     // Subsume C chain to border region 0\n                        {\n                            int LabelRoot = SubsumptionChain(LabelC);\n                            SubsumedLabel[LabelRoot] = 0;\n                        }\n                    }\n                    OldRegion(0, -1, -1);   // attached to border region 0\n                }\n\n                LabelMat[jrow][jcol] = LabelD;\n                    \n            }\n        }\n\n        // Compute Condensation map\n        int Offset = 0;\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            if(SubsumedLabel[Label] > -1) Offset++;\n            CondensationMap[Label] = Label - Offset;\n        }\n\n        // Subsume regions that were flagged as connected; Perimeters add\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            int BetterLabel = SubsumptionChain(Label);\n            if(BetterLabel != Label) Subsume(BetterLabel, Label, 1);\n        }   \n\n        // Condense subsumed regions\n        int NewMaxLabel = 0;\n        for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n        {\n            if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n            {\n                double [] OldRegion = RegionData[OldLabel];\n                int OldParent = (int) OldRegion[BLOBPARENT];\n                int NewLabel = CondensationMap[OldLabel];\n                int NewParent = SubsumptionChain(OldParent);\n                NewParent = CondensationMap[NewParent];\n                OldRegion[BLOBLABEL] = (double) NewLabel;\n                OldRegion[BLOBPARENT] = (double) NewParent;\n                System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n                NewMaxLabel = NewLabel;\n            }\n        }\n    \n        // Zero out unneeded high labels\n        for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n        MaxLabel = NewMaxLabel;\n        \n        // Flag for subsumption regions that have too small area\n        for(int Label = MaxLabel; Label > 0; Label--)\n        {\n            double [] ThisRegion = RegionData[Label];\n            int ThisArea = (int) ThisRegion[BLOBAREA];\n            if(ThisArea < MinArea)\n            {\n                int ThisParent = (int) ThisRegion[BLOBPARENT];\n                SubsumedLabel[Label] =  ThisParent;             // Flag this label as having been subsumed\n            }\n            else SubsumedLabel[Label] =  -1;\n        }\n        \n        // Compute Condensation map\n        Offset = 0;\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            if(SubsumedLabel[Label] > -1) Offset++;\n            CondensationMap[Label] = Label - Offset;      \n        }\n\n        // Subsume regions that were flagged as enclosed; Perimeters subtract\n        for(int Label = 1; Label <= MaxLabel; Label++)\n        {\n            int BetterLabel = SubsumptionChain(Label);\n            if(BetterLabel != Label) Subsume(BetterLabel, Label, -1);\n        }   \n    \n        // Condense subsumed regions\n        for(int OldLabel = 1; OldLabel <= MaxLabel; OldLabel++)\n        {\n            if(SubsumedLabel[OldLabel] < 0) // Renumber valid regions only\n            {\n                double [] OldRegion = RegionData[OldLabel];\n                int OldParent = (int) OldRegion[BLOBPARENT];\n                int NewLabel = CondensationMap[OldLabel];\n                int NewParent = SubsumptionChain(OldParent);\n                NewParent = CondensationMap[NewParent];\n                OldRegion[BLOBLABEL] = (double) NewLabel;\n                OldRegion[BLOBPARENT] = (double) NewParent;\n                System.arraycopy(OldRegion,0,RegionData[NewLabel],0,BLOBDATACOUNT); //RegionData[NewLabel] <- ThisRegion;\n                NewMaxLabel = NewLabel;\n            }\n        }\n        \n        // Zero out unneeded high labels\n        for(int Label = NewMaxLabel+1; Label <= MaxLabel; Label++) ResetRegion(Label);\n        MaxLabel = NewMaxLabel;\n\n        // Normalize summation fields into moments \n        for(int Label = 0; Label <= MaxLabel; Label++)\n        {\n            double [] ThisRegion = RegionData[Label];\n            \n            // Extract fields\n            double Area = ThisRegion[BLOBAREA];\n            double SumX = ThisRegion[BLOBSUMX];\n            double SumY = ThisRegion[BLOBSUMY];\n            double SumXX = ThisRegion[BLOBSUMXX];\n            double SumYY = ThisRegion[BLOBSUMYY];\n            double SumXY = ThisRegion[BLOBSUMXY];\n            \n            // Get averages\n            SumX /= Area;\n            SumY /= Area;\n            SumXX /= Area;\n            SumYY /= Area;\n            SumXY /= Area;\n            \n            // Create moments\n            SumXX -= SumX * SumX;\n            SumYY -= SumY * SumY;\n            SumXY -= SumX * SumY;\n            if(SumXY > -1.0E-14 && SumXY < 1.0E-14) SumXY = (float) 0.0; // Eliminate roundoff error\n\n            ThisRegion[BLOBSUMX] = SumX;\n            ThisRegion[BLOBSUMY] = SumY;\n            ThisRegion[BLOBSUMXX] = SumXX;\n            ThisRegion[BLOBSUMYY] = SumYY;\n            ThisRegion[BLOBSUMXY] = SumXY;\n\n            System.arraycopy(ThisRegion,0,RegionData[Label],0,BLOBDATACOUNT);   // RegionData[Label] <- ThisRegion;\n        }\n    \n        // Adjust border region\n        BorderRegion = RegionData[0];\n        BorderRegion[BLOBSUMXX] = BorderRegion[BLOBSUMYY] = BorderRegion[BLOBSUMXY] = 0;    // Mark invalid fields\n        System.arraycopy(BorderRegion,0,RegionData[0],0,BLOBDATACOUNT); // RegionData[0] <- BorderRegion;\n        \n        return MaxLabel;\n    }\n    \n    // Sort RegionData array on any column. (I couldn't figure out how to use the built-in java sort.)\n    static double iField, jField;\n    static double [] iProperty, jProperty;\n    public static void SortRegions(int Col)\n    {\n        for(int i = 0; i < MaxLabel; i++)\n        {\n            for(int j = i+1; j <= Blobs.MaxLabel; j++)\n            {\n                iProperty = RegionData[i];\n                jProperty = RegionData[j];\n                iField = iProperty[Col];\n                jField = jProperty[Col];\n                if(iField > jField)\n                {\n                    RegionData[i] = jProperty;\n                    RegionData[j] = iProperty;\n                }\n            }\n        }\n    }\n}\n\n\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\npublic class Blobs {\n    static int BLOBROWCOUNT;\n    static int BLOBCOLCOUNT;\n    static int BLOBTOTALCOUNT;\n    static public int BLOBLABEL;\n    static public int BLOBPARENT;\n    static public int BLOBCOLOR;\n    static public int BLOBAREA;\n    static public int BLOBPERIMETER;\n    static public int BLOBSUMX;\n    static public int BLOBSUMY;\n    static public int BLOBSUMXX;\n    static public int BLOBSUMYY;\n    static public int BLOBSUMXY;\n    static public int BLOBMINX;\n    static public int BLOBMAXX;\n    static public int BLOBMINY;\n    static public int BLOBMAXY;\n    static public int BLOBDATACOUNT;\n    static public int[][] LabelMat;\n    static public double[][] RegionData;\n    static public int MaxLabel;\n    public int LabelA, LabelB, LabelC, LabelD;\n    public int ColorA, ColorB, ColorC, ColorD;\n    public int jrow, jcol;\n    static public int[] SubsumedLabel;\n    static public int[] CondensationMap;\n    public  PrintRegionData();\n    public  PrintRegionData(int Label0, int Label1);\n    static public int NextRegion(int Parent, int Color, double MinArea, double MaxArea, int Label);\n    static public int PriorRegion(int Parent, int Color, double MinArea, double MaxArea, int Label);\n    public  ResetRegion(int Label);\n    public  OldRegion(int NewLabelD, int Label1, int Label2);\n    public  NewRegion(int ParentLabel);\n    public  Subsume(int GoodLabel, int BadLabel, int PSign);\n    static public int SubsumptionChain(int x);\n    static public int SubsumptionChain(int x, int Print);\n    public int BlobAnalysis(IplImage Src, int Col0, int Row0, int Cols, int Rows, int Border, int MinArea);\n    static double iField, jField;\n    static double[] iProperty, jProperty;\n    static public  SortRegions(int Col);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "FFmpegFrameGrabber.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/FFmpegFrameGrabber.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**Estimation of audio frames per second.\n     *\n     * Care must be taken as this method may require unnecessary call of\n     * grabFrame(true, false, false, false, false) with frameGrabbed set to true.\n     *\n     * @return (double) getSampleRate()) / samples_frame.nb_samples()\n     * if samples_frame.nb_samples() is not zero, otherwise return 0\n     */",
        "source_code": "\npublic double getAudioFrameRate() {\n    if (audio_st == null) {\n        return 0.0;\n    } else {\n        if (samples_frame == null || samples_frame.nb_samples() == 0) {\n            try {\n                grabFrame(true, false, false, false, false);\n                frameGrabbed = true;\n            } catch (Exception e) {\n                return 0.0;\n            }\n        }\n        if (samples_frame != null && samples_frame.nb_samples() != 0)\n            return ((double) getSampleRate()) / samples_frame.nb_samples();\n        else return 0.0;\n\n    }\n}\n",
        "class_name": "FFmpegFrameGrabber",
        "method_name": "getAudioFrameRate",
        "argument_name": [],
        "full_context": "/*\n * Copyright (C) 2009-2022 Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n *\n * Based on the avcodec_sample.0.5.0.c file available at\n * http://web.me.com/dhoerl/Home/Tech_Blog/Entries/2009/1/22_Revised_avcodec_sample.c_files/avcodec_sample.0.5.0.c\n * by Martin B\u00f6hme, Stephen Dranger, and David Hoerl\n * as well as on the decoding_encoding.c file included in FFmpeg 0.11.1,\n * and on the decode_video.c file included in FFmpeg 4.4,\n * which is covered by the following copyright notice:\n *\n * Copyright (c) 2001 Fabrice Bellard\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\n\npackage org.bytedeco.javacv;\n\nimport java.io.BufferedInputStream;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.net.URL;\nimport java.nio.Buffer;\nimport java.nio.ByteBuffer;\nimport java.util.Collections;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport org.bytedeco.javacpp.BytePointer;\nimport org.bytedeco.javacpp.DoublePointer;\nimport org.bytedeco.javacpp.IntPointer;\nimport org.bytedeco.javacpp.Loader;\nimport org.bytedeco.javacpp.Pointer;\nimport org.bytedeco.javacpp.PointerScope;\nimport org.bytedeco.javacpp.PointerPointer;\n\nimport org.bytedeco.ffmpeg.avcodec.*;\nimport org.bytedeco.ffmpeg.avformat.*;\nimport org.bytedeco.ffmpeg.avutil.*;\nimport org.bytedeco.ffmpeg.swresample.*;\nimport org.bytedeco.ffmpeg.swscale.*;\nimport static org.bytedeco.ffmpeg.global.avcodec.*;\nimport static org.bytedeco.ffmpeg.global.avdevice.*;\nimport static org.bytedeco.ffmpeg.global.avformat.*;\nimport static org.bytedeco.ffmpeg.global.avutil.*;\nimport static org.bytedeco.ffmpeg.global.swresample.*;\nimport static org.bytedeco.ffmpeg.global.swscale.*;\n\n/**\n *\n * @author Samuel Audet\n */\npublic class FFmpegFrameGrabber extends FrameGrabber {\n\n    public static class Exception extends FrameGrabber.Exception {\n        public Exception(String message) { super(message + \" (For more details, make sure FFmpegLogCallback.set() has been called.)\"); }\n        public Exception(String message, Throwable cause) { super(message, cause); }\n    }\n\n    public static String[] getDeviceDescriptions() throws Exception {\n        tryLoad();\n        throw new UnsupportedOperationException(\"Device enumeration not support by FFmpeg.\");\n    }\n\n    public static FFmpegFrameGrabber createDefault(File deviceFile)   throws Exception { return new FFmpegFrameGrabber(deviceFile); }\n    public static FFmpegFrameGrabber createDefault(String devicePath) throws Exception { return new FFmpegFrameGrabber(devicePath); }\n    public static FFmpegFrameGrabber createDefault(int deviceNumber)  throws Exception { throw new Exception(FFmpegFrameGrabber.class + \" does not support device numbers.\"); }\n\n    private static Exception loadingException = null;\n    public static void tryLoad() throws Exception {\n        if (loadingException != null) {\n            throw loadingException;\n        } else {\n            try {\n                Loader.load(org.bytedeco.ffmpeg.global.avutil.class);\n                Loader.load(org.bytedeco.ffmpeg.global.swresample.class);\n                Loader.load(org.bytedeco.ffmpeg.global.avcodec.class);\n                Loader.load(org.bytedeco.ffmpeg.global.avformat.class);\n                Loader.load(org.bytedeco.ffmpeg.global.swscale.class);\n\n                // Register all formats and codecs\n                av_jni_set_java_vm(Loader.getJavaVM(), null);\n//                avcodec_register_all();\n//                av_register_all();\n                avformat_network_init();\n\n                Loader.load(org.bytedeco.ffmpeg.global.avdevice.class);\n                avdevice_register_all();\n            } catch (Throwable t) {\n                if (t instanceof Exception) {\n                    throw loadingException = (Exception)t;\n                } else {\n                    throw loadingException = new Exception(\"Failed to load \" + FFmpegFrameGrabber.class, t);\n                }\n            }\n        }\n    }\n\n    static {\n        try {\n            tryLoad();\n//            FFmpegLockCallback.init();\n        } catch (Exception ex) { }\n    }\n\n    public FFmpegFrameGrabber(URL url) {\n        this(url.toString());\n    }\n    public FFmpegFrameGrabber(File file) {\n        this(file.getAbsolutePath());\n    }\n    public FFmpegFrameGrabber(String filename) {\n        this.filename = filename;\n        this.pixelFormat = AV_PIX_FMT_NONE;\n        this.sampleFormat = AV_SAMPLE_FMT_NONE;\n    }\n    /** Calls {@code FFmpegFrameGrabber(inputStream, Integer.MAX_VALUE - 8)}\n     *  so that the whole input stream is seekable. */\n    public FFmpegFrameGrabber(InputStream inputStream) {\n        this(inputStream, Integer.MAX_VALUE - 8);\n    }\n    /** Set maximumSize to 0 to disable seek and minimize startup time. */\n    public FFmpegFrameGrabber(InputStream inputStream, int maximumSize) {\n        this.inputStream = inputStream;\n        this.closeInputStream = true;\n        this.pixelFormat = AV_PIX_FMT_NONE;\n        this.sampleFormat = AV_SAMPLE_FMT_NONE;\n        this.maximumSize = maximumSize;\n    }\n    public void release() throws Exception {\n        synchronized (org.bytedeco.ffmpeg.global.avcodec.class) {\n            releaseUnsafe();\n        }\n    }\n    public synchronized void releaseUnsafe() throws Exception {\n        started = false;\n\n        if (plane_ptr != null && plane_ptr2 != null) {\n            plane_ptr.releaseReference();\n            plane_ptr2.releaseReference();\n            plane_ptr = plane_ptr2 = null;\n        }\n\n        if (pkt != null) {\n            if (pkt.stream_index() != -1) {\n                av_packet_unref(pkt);\n            }\n            pkt.releaseReference();\n            pkt = null;\n        }\n\n        // Free the RGB image\n        if (image_ptr != null) {\n            for (int i = 0; i < image_ptr.length; i++) {\n                if (imageMode != ImageMode.RAW) {\n                    av_free(image_ptr[i]);\n                }\n            }\n            image_ptr = null;\n        }\n        if (picture_rgb != null) {\n            av_frame_free(picture_rgb);\n            picture_rgb = null;\n        }\n\n        // Free the native format picture frame\n        if (picture != null) {\n            av_frame_free(picture);\n            picture = null;\n        }\n\n        // Close the video codec\n        if (video_c != null) {\n            avcodec_free_context(video_c);\n            video_c = null;\n        }\n\n        // Free the audio samples frame\n        if (samples_frame != null) {\n            av_frame_free(samples_frame);\n            samples_frame = null;\n        }\n\n        // Close the audio codec\n        if (audio_c != null) {\n            avcodec_free_context(audio_c);\n            audio_c = null;\n        }\n\n        // Close the video file\n        if (inputStream == null && oc != null && !oc.isNull()) {\n            avformat_close_input(oc);\n            oc = null;\n        }\n\n        if (img_convert_ctx != null) {\n            sws_freeContext(img_convert_ctx);\n            img_convert_ctx = null;\n        }\n\n        if (samples_ptr_out != null) {\n            for (int i = 0; i < samples_ptr_out.length; i++) {\n                av_free(samples_ptr_out[i].position(0));\n            }\n            samples_ptr_out = null;\n            samples_buf_out = null;\n        }\n\n        if (samples_convert_ctx != null) {\n            swr_free(samples_convert_ctx);\n            samples_convert_ctx = null;\n        }\n\n        frameGrabbed  = false;\n        frame         = null;\n        timestamp     = 0;\n        frameNumber   = 0;\n\n        if (inputStream != null) {\n            try {\n                if (oc == null) {\n                    // when called a second time\n                    if (closeInputStream) {\n                        inputStream.close();\n                    }\n                } else if (maximumSize > 0) {\n                    try {\n                        inputStream.reset();\n                    } catch (IOException ex) {\n                        // \"Resetting to invalid mark\", give up?\n                        System.err.println(\"Error on InputStream.reset(): \" + ex);\n                    }\n                }\n            } catch (IOException ex) {\n                throw new Exception(\"Error on InputStream.close(): \", ex);\n            } finally {\n                inputStreams.remove(oc);\n                if (avio != null) {\n                    if (avio.buffer() != null) {\n                        av_free(avio.buffer());\n                        avio.buffer(null);\n                    }\n                    av_free(avio);\n                    avio = null;\n                }\n                if (oc != null) {\n                    avformat_free_context(oc);\n                    oc = null;\n                }\n            }\n        }\n    }\n    @Override protected void finalize() throws Throwable {\n        super.finalize();\n        release();\n    }\n\n    static Map<Pointer,InputStream> inputStreams = Collections.synchronizedMap(new HashMap<Pointer,InputStream>());\n\n    static class ReadCallback extends Read_packet_Pointer_BytePointer_int {\n        @Override public int call(Pointer opaque, BytePointer buf, int buf_size) {\n            try {\n                byte[] b = new byte[buf_size];\n                InputStream is = inputStreams.get(opaque);\n                int size = is.read(b, 0, buf_size);\n                if (size < 0) {\n                    return AVERROR_EOF();\n                } else {\n                    buf.put(b, 0, size);\n                    return size;\n                }\n            }\n            catch (Throwable t) {\n                System.err.println(\"Error on InputStream.read(): \" + t);\n                return -1;\n            }\n        }\n    }\n\n    static class SeekCallback extends Seek_Pointer_long_int {\n        @Override public long call(Pointer opaque, long offset, int whence) {\n            try {\n                InputStream is = inputStreams.get(opaque);\n                long size = 0;\n                switch (whence) {\n                    case 0: is.reset(); break; // SEEK_SET\n                    case 1: break;             // SEEK_CUR\n                    case 2:                    // SEEK_END\n                        is.reset();\n                        while (true) {\n                            long n = is.skip(Long.MAX_VALUE);\n                            if (n == 0) break;\n                            size += n;\n                        }\n                        offset += size;\n                        is.reset();\n                        break;\n                    case AVSEEK_SIZE:\n                        long remaining = 0;\n                        while (true) {\n                            long n = is.skip(Long.MAX_VALUE);\n                            if (n == 0) break;\n                            remaining += n;\n                        }\n                        is.reset();\n                        while (true) {\n                            long n = is.skip(Long.MAX_VALUE);\n                            if (n == 0) break;\n                            size += n;\n                        }\n                        offset = size - remaining;\n                        is.reset();\n                        break;\n                    default: return -1;\n                }\n                long remaining = offset;\n                while (remaining > 0) {\n                    long skipped = is.skip(remaining);\n                    if (skipped == 0) break; // end of the stream\n                    remaining -= skipped;\n                }\n                return whence == AVSEEK_SIZE ? size : 0;\n            } catch (Throwable t) {\n                System.err.println(\"Error on InputStream.reset() or skip(): \" + t);\n                return -1;\n            }\n        }\n    }\n\n    static ReadCallback readCallback = new ReadCallback().retainReference();\n    static SeekCallback seekCallback = new SeekCallback().retainReference();\n\n    private InputStream     inputStream;\n    private boolean         closeInputStream;\n    private int             maximumSize;\n    private AVIOContext     avio;\n    private String          filename;\n    private AVFormatContext oc;\n    private AVStream        video_st, audio_st;\n    private AVCodecContext  video_c, audio_c;\n    private AVFrame         picture, picture_rgb;\n    private BytePointer[]   image_ptr;\n    private Buffer[]        image_buf;\n    private AVFrame         samples_frame;\n    private BytePointer[]   samples_ptr;\n    private Buffer[]        samples_buf;\n    private BytePointer[]   samples_ptr_out;\n    private Buffer[]        samples_buf_out;\n    private PointerPointer  plane_ptr, plane_ptr2;\n    private AVPacket        pkt;\n    private SwsContext      img_convert_ctx;\n    private SwrContext      samples_convert_ctx;\n    private int             samples_channels, samples_format, samples_rate;\n    private boolean         frameGrabbed;\n    private Frame           frame;\n    private int[]           streams;\n\n    private volatile boolean started = false;\n\n    public boolean isCloseInputStream() {\n        return closeInputStream;\n    }\n    public void setCloseInputStream(boolean closeInputStream) {\n        this.closeInputStream = closeInputStream;\n    }\n\n    /**\n     * Is there a video stream?\n     * @return  {@code video_st!=null;}\n     */\n    public boolean hasVideo() {\n        return video_st!=null;\n    }\n\n    /**\n     * Is there an audio stream?\n     * @return  {@code audio_st!=null;}\n     */\n    public boolean hasAudio() {\n        return audio_st!=null;\n    }\n\n    @Override public double getGamma() {\n        // default to a gamma of 2.2 for cheap Webcams, DV cameras, etc.\n        if (gamma == 0.0) {\n            return 2.2;\n        } else {\n            return gamma;\n        }\n    }\n\n    @Override public String getFormat() {\n        if (oc == null) {\n            return super.getFormat();\n        } else {\n            return oc.iformat().name().getString();\n        }\n    }\n\n    @Override public int getImageWidth() {\n        return imageWidth > 0 || video_c == null ? super.getImageWidth() : video_c.width();\n    }\n\n    @Override public int getImageHeight() {\n        return imageHeight > 0 || video_c == null ? super.getImageHeight() : video_c.height();\n    }\n\n    @Override public int getAudioChannels() {\n        return audioChannels > 0 || audio_c == null ? super.getAudioChannels() : audio_c.channels();\n    }\n\n    @Override public int getPixelFormat() {\n        if (imageMode == ImageMode.COLOR || imageMode == ImageMode.GRAY) {\n            if (pixelFormat == AV_PIX_FMT_NONE) {\n                return imageMode == ImageMode.COLOR ? AV_PIX_FMT_BGR24 : AV_PIX_FMT_GRAY8;\n            } else {\n                return pixelFormat;\n            }\n        } else if (video_c != null) { // RAW\n            return video_c.pix_fmt();\n        } else {\n            return super.getPixelFormat();\n        }\n    }\n\n    @Override public int getVideoCodec() {\n        return video_c == null ? super.getVideoCodec() : video_c.codec_id();\n    }\n\n    @Override\n    public String getVideoCodecName(){\n        return  video_c == null ? super.getVideoCodecName() : video_c.codec().name().getString();\n    }\n\n    @Override public int getVideoBitrate() {\n        return video_c == null ? super.getVideoBitrate() : (int)video_c.bit_rate();\n    }\n\n    @Override public double getAspectRatio() {\n        if (video_st == null) {\n            return super.getAspectRatio();\n        } else {\n            AVRational r = av_guess_sample_aspect_ratio(oc, video_st, picture);\n            double a = (double)r.num() / r.den();\n            return a == 0.0 ? 1.0 : a;\n        }\n    }\n\n    /** Returns {@link #getVideoFrameRate()} */\n    @Override public double getFrameRate() {\n        return getVideoFrameRate();\n    }\n\n    /**Estimation of audio frames per second.\n     *\n     * Care must be taken as this method may require unnecessary call of\n     * grabFrame(true, false, false, false, false) with frameGrabbed set to true.\n     *\n     * @return (double) getSampleRate()) / samples_frame.nb_samples()\n     * if samples_frame.nb_samples() is not zero, otherwise return 0\n     */\n    public double getAudioFrameRate() {\n        if (audio_st == null) {\n            return 0.0;\n        } else {\n            if (samples_frame == null || samples_frame.nb_samples() == 0) {\n                try {\n                    grabFrame(true, false, false, false, false);\n                    frameGrabbed = true;\n                } catch (Exception e) {\n                    return 0.0;\n                }\n            }\n            if (samples_frame != null && samples_frame.nb_samples() != 0)\n                return ((double) getSampleRate()) / samples_frame.nb_samples();\n            else return 0.0;\n\n        }\n    }\n\n    public double getVideoFrameRate() {\n        if (video_st == null) {\n            return super.getFrameRate();\n        } else {\n            AVRational r = video_st.avg_frame_rate();\n            if (r.num() == 0 && r.den() == 0) {\n                r = video_st.r_frame_rate();\n            }\n            return (double)r.num() / r.den();\n        }\n    }\n\n    @Override public int getAudioCodec() {\n        return audio_c == null ? super.getAudioCodec() : audio_c.codec_id();\n    }\n\n    @Override public String getAudioCodecName() {\n        return audio_c == null ? super.getAudioCodecName() : audio_c.codec().name().getString();\n    }\n\n    @Override public int getAudioBitrate() {\n        return audio_c == null ? super.getAudioBitrate() : (int)audio_c.bit_rate();\n    }\n\n    @Override public int getSampleFormat() {\n        if (sampleMode == SampleMode.SHORT || sampleMode == SampleMode.FLOAT) {\n            if (sampleFormat == AV_SAMPLE_FMT_NONE) {\n                return sampleMode == SampleMode.SHORT ? AV_SAMPLE_FMT_S16 : AV_SAMPLE_FMT_FLT;\n            } else {\n                return sampleFormat;\n            }\n        } else if (audio_c != null) { // RAW\n            return audio_c.sample_fmt();\n        } else {\n            return super.getSampleFormat();\n        }\n    }\n\n    @Override public int getSampleRate() {\n        return sampleRate > 0 || audio_c == null ? super.getSampleRate() : audio_c.sample_rate();\n    }\n\n    @Override public Map<String, String> getMetadata() {\n        if (oc == null) {\n            return super.getMetadata();\n        }\n        AVDictionaryEntry entry = null;\n        Map<String, String> metadata = new HashMap<String, String>();\n        while ((entry = av_dict_get(oc.metadata(), \"\", entry, AV_DICT_IGNORE_SUFFIX)) != null) {\n            metadata.put(entry.key().getString(charset), entry.value().getString(charset));\n        }\n        return metadata;\n    }\n\n    @Override public Map<String, String> getVideoMetadata() {\n        if (video_st == null) {\n            return super.getVideoMetadata();\n        }\n        AVDictionaryEntry entry = null;\n        Map<String, String> metadata = new HashMap<String, String>();\n        while ((entry = av_dict_get(video_st.metadata(), \"\", entry, AV_DICT_IGNORE_SUFFIX)) != null) {\n            metadata.put(entry.key().getString(charset), entry.value().getString(charset));\n        }\n        return metadata;\n    }\n\n    @Override public Map<String, String> getAudioMetadata() {\n        if (audio_st == null) {\n            return super.getAudioMetadata();\n        }\n        AVDictionaryEntry entry = null;\n        Map<String, String> metadata = new HashMap<String, String>();\n        while ((entry = av_dict_get(audio_st.metadata(), \"\", entry, AV_DICT_IGNORE_SUFFIX)) != null) {\n            metadata.put(entry.key().getString(charset), entry.value().getString(charset));\n        }\n        return metadata;\n    }\n\n    @Override public String getMetadata(String key) {\n        if (oc == null) {\n            return super.getMetadata(key);\n        }\n        AVDictionaryEntry entry = av_dict_get(oc.metadata(), key, null, 0);\n        return entry == null || entry.value() == null ? null : entry.value().getString(charset);\n    }\n\n    @Override public String getVideoMetadata(String key) {\n        if (video_st == null) {\n            return super.getVideoMetadata(key);\n        }\n        AVDictionaryEntry entry = av_dict_get(video_st.metadata(), key, null, 0);\n        return entry == null || entry.value() == null ? null : entry.value().getString(charset);\n    }\n\n    @Override public String getAudioMetadata(String key) {\n        if (audio_st == null) {\n            return super.getAudioMetadata(key);\n        }\n        AVDictionaryEntry entry = av_dict_get(audio_st.metadata(), key, null, 0);\n        return entry == null || entry.value() == null ? null : entry.value().getString(charset);\n    }\n\n    @Override public Map<String, Buffer> getVideoSideData() {\n        if (video_st == null) {\n            return super.getVideoSideData();\n        }\n        videoSideData = new HashMap<String, Buffer>();\n        for (int i = 0; i < video_st.nb_side_data(); i++) {\n            AVPacketSideData sd = video_st.side_data().position(i);\n            String key = av_packet_side_data_name(sd.type()).getString();\n            Buffer value = sd.data().capacity(sd.size()).asBuffer();\n            videoSideData.put(key, value);\n        }\n        return videoSideData;\n    }\n\n    @Override public Buffer getVideoSideData(String key) {\n        return getVideoSideData().get(key);\n    }\n\n    /** Returns the rotation in degrees from the side data of the video stream, or 0 if unknown. */\n    public double getDisplayRotation() {\n        ByteBuffer b = (ByteBuffer)getVideoSideData(\"Display Matrix\");\n        return b != null ? av_display_rotation_get(new IntPointer(new BytePointer(b))) : 0;\n    }\n\n    @Override public Map<String, Buffer> getAudioSideData() {\n        if (audio_st == null) {\n            return super.getAudioSideData();\n        }\n        audioSideData = new HashMap<String, Buffer>();\n        for (int i = 0; i < audio_st.nb_side_data(); i++) {\n            AVPacketSideData sd = audio_st.side_data().position(i);\n            String key = av_packet_side_data_name(sd.type()).getString();\n            Buffer value = sd.data().capacity(sd.size()).asBuffer();\n            audioSideData.put(key, value);\n        }\n        return audioSideData;\n    }\n\n    @Override public Buffer getAudioSideData(String key) {\n        return getAudioSideData().get(key);\n    }\n\n    /** default override of super.setFrameNumber implies setting\n     *  of a frame close to a video frame having that number */\n    @Override public void setFrameNumber(int frameNumber) throws Exception {\n        if (hasVideo()) setTimestamp(Math.round((1000000L * frameNumber + 500000L)/ getFrameRate()));\n        else super.frameNumber = frameNumber;\n    }\n\n    /** if there is video stream tries to seek to video frame with corresponding timestamp\n     *  otherwise sets super.frameNumber only because frameRate==0 if there is no video stream */\n    public void setVideoFrameNumber(int frameNumber) throws Exception {\n        // best guess, AVSEEK_FLAG_FRAME has not been implemented in FFmpeg...\n        if (hasVideo()) setVideoTimestamp(Math.round((1000000L * frameNumber + 500000L)/ getFrameRate()));\n        else super.frameNumber = frameNumber;\n    }\n\n    /** if there is audio stream tries to seek to audio frame with corresponding timestamp\n     *  ignoring otherwise */\n    public void setAudioFrameNumber(int frameNumber) throws Exception {\n        // best guess, AVSEEK_FLAG_FRAME has not been implemented in FFmpeg...\n        if (hasAudio()) setAudioTimestamp(Math.round((1000000L * frameNumber + 500000L)/ getAudioFrameRate()));\n    }\n\n    /** setTimestamp without checking frame content (using old code used in JavaCV versions prior to 1.4.1) */\n    @Override public void setTimestamp(long timestamp) throws Exception {\n        setTimestamp(timestamp, false);\n    }\n\n    /** setTimestamp with possibility to select between old quick seek code or new code\n     * doing check of frame content. The frame check can be useful with corrupted files, when seeking may\n     * end up with an empty frame not containing video nor audio */\n    public void setTimestamp(long timestamp, boolean checkFrame) throws Exception {\n        setTimestamp(timestamp, checkFrame ? EnumSet.of(Frame.Type.VIDEO, Frame.Type.AUDIO) : null);\n    }\n\n    /** setTimestamp with resulting video frame type if there is a video stream.\n     * This should provide precise seek to a video frame containing the requested timestamp\n     * in most cases.\n     * */\n    public void setVideoTimestamp(long timestamp) throws Exception {\n        setTimestamp(timestamp, EnumSet.of(Frame.Type.VIDEO));\n    }\n\n    /** setTimestamp with resulting audio frame type if there is an audio stream.\n     * This should provide precise seek to an audio frame containing the requested timestamp\n     * in most cases.\n     * */\n    public void setAudioTimestamp(long timestamp) throws Exception {\n        setTimestamp(timestamp, EnumSet.of(Frame.Type.AUDIO));\n    }\n\n    /** setTimestamp with a priority the resulting frame should be:\n     *  video (frameTypesToSeek contains only Frame.Type.VIDEO),\n     *  audio (frameTypesToSeek contains only Frame.Type.AUDIO),\n     *  or any (frameTypesToSeek contains both)\n     */\n    private synchronized void setTimestamp(long timestamp, EnumSet<Frame.Type> frameTypesToSeek) throws Exception {\n        int ret;\n        if (oc == null) {\n            super.timestamp = timestamp;\n        } else {\n            timestamp = timestamp * AV_TIME_BASE / 1000000L;\n\n            /* the stream start time */\n            long ts0 = 0;\n            if (oc.start_time() != AV_NOPTS_VALUE) ts0 = oc.start_time();\n\n            long early_ts = timestamp;\n            if (frameTypesToSeek!=null) {\n                /*  Sometimes the ffmpeg's avformat_seek_file(...) function brings us not to a position before\n                 *  the desired but few frames after. In case we need a frame-precision seek we may\n                 *  try to request an earlier timestamp.\n                 */\n                early_ts -= 500000L;\n                if (early_ts < 0L)early_ts = 0L;\n\n                /*  If frameTypesToSeek is set explicitly to VIDEO or AUDIO\n                 *  we need to use start time of the corresponding stream\n                 *  instead of the common start time\n                 */\n                if (frameTypesToSeek.size()==1) {\n                    if (frameTypesToSeek.contains(Frame.Type.VIDEO)) {\n                        if (video_st!=null && video_st.start_time() != AV_NOPTS_VALUE) {\n                            AVRational time_base = video_st.time_base();\n                            ts0 = 1000000L * video_st.start_time() * time_base.num() / time_base.den();\n                        }\n                    }\n                    else if (frameTypesToSeek.contains(Frame.Type.AUDIO)) {\n                        if (audio_st!=null && audio_st.start_time() != AV_NOPTS_VALUE) {\n                            AVRational time_base = audio_st.time_base();\n                            ts0 = 1000000L * audio_st.start_time() * time_base.num() / time_base.den();\n                        }\n                    }\n                }\n            }\n\n            /* add the stream start time */\n            timestamp += ts0;\n            early_ts += ts0;\n\n            if ((ret = avformat_seek_file(oc, -1, Long.MIN_VALUE, early_ts, Long.MAX_VALUE, AVSEEK_FLAG_BACKWARD)) < 0) {\n                throw new Exception(\"avformat_seek_file() error \" + ret + \": Could not seek file to timestamp \" + timestamp + \".\");\n            }\n            if (video_c != null) {\n                avcodec_flush_buffers(video_c);\n            }\n            if (audio_c != null) {\n                avcodec_flush_buffers(audio_c);\n            }\n            if (pkt.stream_index() != -1) {\n                av_packet_unref(pkt);\n                pkt.stream_index(-1);\n            }\n            /*     After the call of ffmpeg's avformat_seek_file(...) with the flag set to AVSEEK_FLAG_BACKWARD\n             * the decoding position should be located before the requested timestamp in a closest position\n             * from which all the active streams can be decoded successfully.\n             * The following seeking consists of two stages:\n             * 1. Grab frames till the frame corresponding to that \"closest\" position\n             * (the first frame containing decoded data).\n             *\n             * 2. Grab frames till the desired timestamp is reached. The number of steps is restricted\n             * by doubled estimation of frames between that \"closest\" position and the desired position.\n             *\n             * frameTypesToSeek parameter sets the preferred type of frames to seek.\n             * It can be chosen from three possible types: VIDEO, AUDIO or any of them.\n             * The setting means only a preference in the type. That is, if VIDEO or AUDIO is\n             * specified but the file does not have video or audio stream - any type will be used instead.\n             */\n\n            if (frameTypesToSeek != null //new code providing check of frame content while seeking to the timestamp\n                    && (frameTypesToSeek.contains(Frame.Type.VIDEO) || frameTypesToSeek.contains(Frame.Type.AUDIO))) {\n                boolean has_video = hasVideo();\n                boolean has_audio = hasAudio();\n\n                if (has_video || has_audio) {\n                    if ((frameTypesToSeek.contains(Frame.Type.VIDEO) && !has_video ) ||\n                            (frameTypesToSeek.contains(Frame.Type.AUDIO) && !has_audio ))\n                        frameTypesToSeek = EnumSet.of(Frame.Type.VIDEO, Frame.Type.AUDIO);\n\n                    long initialSeekPosition = Long.MIN_VALUE;\n                    long maxSeekSteps = 0;\n                    long count = 0;\n                    Frame seekFrame = null;\n                    seekFrame = grabFrame(frameTypesToSeek.contains(Frame.Type.AUDIO), frameTypesToSeek.contains(Frame.Type.VIDEO), false, false, false);\n                    if (seekFrame == null) return;\n\n                    initialSeekPosition = seekFrame.timestamp;\n                    double frameDuration = 0.0;\n                    if (seekFrame.image != null && this.getFrameRate() > 0)\n                        frameDuration =  AV_TIME_BASE / (double)getFrameRate();\n                    else if (seekFrame.samples != null && samples_frame != null && getSampleRate() > 0) {\n                        frameDuration =  AV_TIME_BASE * samples_frame.nb_samples() / (double)getSampleRate();\n                    }\n//                    if(frameDuration>0.0) {\n//                        maxSeekSteps = (long)(10*(timestamp - initialSeekPosition - frameDuration)/frameDuration);\n//                        if (maxSeekSteps<0) maxSeekSteps = 0;\n//                    }\n                    if(frameDuration>0.0) {\n                        maxSeekSteps = 0; //no more grab if the distance to the requested timestamp is smaller than frameDuration\n                        if (timestamp - initialSeekPosition + 1 > frameDuration)  //allow for a rounding error\n                                  maxSeekSteps = (long)(10*(timestamp - initialSeekPosition)/frameDuration);\n                    }\n                    else if (initialSeekPosition < timestamp) maxSeekSteps = 1000;\n\n                    double delta = 0.0; //for the timestamp correction\n                    count = 0;\n                    while(count < maxSeekSteps) {\n                        seekFrame = grabFrame(frameTypesToSeek.contains(Frame.Type.AUDIO), frameTypesToSeek.contains(Frame.Type.VIDEO), false, false, false);\n                        if (seekFrame == null) return; //is it better to throw NullPointerException?\n\n                        count++;\n                        double ts=seekFrame.timestamp;\n                        frameDuration = 0.0;\n                        if (seekFrame.image != null && this.getFrameRate() > 0)\n                            frameDuration =  AV_TIME_BASE / (double)getFrameRate();\n                        else if (seekFrame.samples != null && samples_frame != null && getSampleRate() > 0)\n                            frameDuration =  AV_TIME_BASE * samples_frame.nb_samples() / (double)getSampleRate();\n\n                        delta = 0.0;\n                        if (frameDuration>0.0) {\n                            delta = (ts-ts0)/frameDuration - Math.round((ts-ts0)/frameDuration);\n                            if (Math.abs(delta)>0.2) delta=0.0;\n                        }\n                        ts-=delta*frameDuration; // corrected timestamp\n                        if (ts + frameDuration > timestamp) break;\n                    }\n\n                    frameGrabbed = true;\n                }\n            } else { //old quick seeking code used in JavaCV versions prior to 1.4.1\n                /* comparing to timestamp +/- 1 avoids rouding issues for framerates\n                which are no proper divisors of 1000000, e.g. where\n                av_frame_get_best_effort_timestamp in grabFrame sets this.timestamp\n                to ...666 and the given timestamp has been rounded to ...667\n                (or vice versa)\n                 */\n                int count = 0; // prevent infinite loops with corrupted files\n                while (this.timestamp > timestamp + 1 && grabFrame(true, true, false, false) != null && count++ < 1000) {\n                    // flush frames if seeking backwards\n                }\n                count = 0;\n                while (this.timestamp < timestamp - 1 && grabFrame(true, true, false, false) != null && count++ < 1000) {\n                    // decode up to the desired frame\n                }\n                frameGrabbed = true;\n            }\n        }\n    }\n\n    /** Returns {@link #getLengthInVideoFrames()} */\n    @Override public int getLengthInFrames() {\n        // best guess...\n        return getLengthInVideoFrames();\n    }\n\n    @Override public long getLengthInTime() {\n        return oc.duration() * 1000000L / AV_TIME_BASE;\n    }\n\n    /** Returns {@code (int) Math.round(getLengthInTime() * getFrameRate() / 1000000L)}, which is an approximation in general. */\n    public int getLengthInVideoFrames() {\n        // best guess...\n        return (int) Math.round(getLengthInTime() * getFrameRate() / 1000000L);\n    }\n\n    public int getLengthInAudioFrames() {\n        // best guess...\n        double afr = getAudioFrameRate();\n        if (afr > 0) return (int) (getLengthInTime() * afr / 1000000L);\n        else return 0;\n    }\n\n    public AVFormatContext getFormatContext() {\n        return oc;\n    }\n\n    /** Calls {@code start(true)}. */\n    @Override public void start() throws Exception {\n        start(true);\n    }\n    /** Set findStreamInfo to false to minimize startup time, at the expense of robustness. */\n    public void start(boolean findStreamInfo) throws Exception {\n        synchronized (org.bytedeco.ffmpeg.global.avcodec.class) {\n            startUnsafe(findStreamInfo);\n        }\n    }\n    public void startUnsafe() throws Exception {\n        startUnsafe(true);\n    }\n    public synchronized void startUnsafe(boolean findStreamInfo) throws Exception {\n        try (PointerScope scope = new PointerScope()) {\n\n        if (oc != null && !oc.isNull()) {\n            throw new Exception(\"start() has already been called: Call stop() before calling start() again.\");\n        }\n\n        int ret;\n        img_convert_ctx = null;\n        oc              = new AVFormatContext(null);\n        video_c         = null;\n        audio_c         = null;\n        plane_ptr       = new PointerPointer(AVFrame.AV_NUM_DATA_POINTERS).retainReference();\n        plane_ptr2      = new PointerPointer(AVFrame.AV_NUM_DATA_POINTERS).retainReference();\n        pkt             = new AVPacket().retainReference();\n        frameGrabbed    = false;\n        frame           = new Frame();\n        timestamp       = 0;\n        frameNumber     = 0;\n\n        pkt.stream_index(-1);\n\n        // Open video file\n        AVInputFormat f = null;\n        if (format != null && format.length() > 0) {\n            if ((f = av_find_input_format(format)) == null) {\n                throw new Exception(\"av_find_input_format() error: Could not find input format \\\"\" + format + \"\\\".\");\n            }\n        }\n        AVDictionary options = new AVDictionary(null);\n        if (frameRate > 0) {\n            AVRational r = av_d2q(frameRate, 1001000);\n            av_dict_set(options, \"framerate\", r.num() + \"/\" + r.den(), 0);\n        }\n        if (pixelFormat >= 0) {\n            av_dict_set(options, \"pixel_format\", av_get_pix_fmt_name(pixelFormat).getString(), 0);\n        } else if (imageMode != ImageMode.RAW) {\n            av_dict_set(options, \"pixel_format\", imageMode == ImageMode.COLOR ? \"bgr24\" : \"gray8\", 0);\n        }\n        if (imageWidth > 0 && imageHeight > 0) {\n            av_dict_set(options, \"video_size\", imageWidth + \"x\" + imageHeight, 0);\n        }\n        if (sampleRate > 0) {\n            av_dict_set(options, \"sample_rate\", \"\" + sampleRate, 0);\n        }\n        if (audioChannels > 0) {\n            av_dict_set(options, \"channels\", \"\" + audioChannels, 0);\n        }\n        for (Entry<String, String> e : this.options.entrySet()) {\n            av_dict_set(options, e.getKey(), e.getValue(), 0);\n        }\n        if (inputStream != null) {\n            if (!inputStream.markSupported()) {\n                inputStream = new BufferedInputStream(inputStream);\n            }\n            inputStream.mark(maximumSize);\n            oc = avformat_alloc_context();\n            avio = avio_alloc_context(new BytePointer(av_malloc(4096)), 4096, 0, oc, readCallback, null, maximumSize > 0 ? seekCallback : null);\n            oc.pb(avio);\n\n            filename = inputStream.toString();\n            inputStreams.put(oc, inputStream);\n        }\n        if ((ret = avformat_open_input(oc, filename, f, options)) < 0) {\n            av_dict_set(options, \"pixel_format\", null, 0);\n            if ((ret = avformat_open_input(oc, filename, f, options)) < 0) {\n                throw new Exception(\"avformat_open_input() error \" + ret + \": Could not open input \\\"\" + filename + \"\\\". (Has setFormat() been called?)\");\n            }\n        }\n        av_dict_free(options);\n\n        oc.max_delay(maxDelay);\n\n        // Retrieve stream information, if desired\n        if (findStreamInfo && (ret = avformat_find_stream_info(oc, (PointerPointer)null)) < 0) {\n            throw new Exception(\"avformat_find_stream_info() error \" + ret + \": Could not find stream information.\");\n        }\n\n        if (av_log_get_level() >= AV_LOG_INFO) {\n            // Dump information about file onto standard error\n            av_dump_format(oc, 0, filename, 0);\n        }\n\n        // Find the first stream with the user-specified disposition property\n        int nb_streams = oc.nb_streams();\n        for (int i = 0; i < nb_streams; i++) {\n            AVStream st = oc.streams(i);\n            AVCodecParameters par = st.codecpar();\n            if (videoStream < 0 && par.codec_type() == AVMEDIA_TYPE_VIDEO && st.disposition() == videoDisposition) {\n                videoStream = i;\n            } else if (audioStream < 0 && par.codec_type() == AVMEDIA_TYPE_AUDIO && st.disposition() == audioDisposition) {\n                audioStream = i;\n            }\n        }\n\n        // Find the first video and audio stream, unless the user specified otherwise\n        video_st = audio_st = null;\n        AVCodecParameters video_par = null, audio_par = null;\n        streams = new int[nb_streams];\n        for (int i = 0; i < nb_streams; i++) {\n            AVStream st = oc.streams(i);\n            // Get a pointer to the codec context for the video or audio stream\n            AVCodecParameters par = st.codecpar();\n            streams[i] = par.codec_type();\n            if (video_st == null && par.codec_type() == AVMEDIA_TYPE_VIDEO && (videoStream < 0 || videoStream == i)) {\n                video_st = st;\n                video_par = par;\n                videoStream = i;\n            } else if (audio_st == null && par.codec_type() == AVMEDIA_TYPE_AUDIO && (audioStream < 0 || audioStream == i)) {\n                audio_st = st;\n                audio_par = par;\n                audioStream = i;\n            }\n        }\n        if (video_st == null && audio_st == null) {\n            throw new Exception(\"Did not find a video or audio stream inside \\\"\" + filename\n                    + \"\\\" for videoStream == \" + videoStream + \" and audioStream == \" + audioStream + \".\");\n        }\n\n        if (video_st != null) {\n            // Find the decoder for the video stream\n            AVCodec codec = avcodec_find_decoder_by_name(videoCodecName);\n            if (codec == null) {\n                codec = avcodec_find_decoder(video_par.codec_id());\n            }\n            if (codec == null) {\n                throw new Exception(\"avcodec_find_decoder() error: Unsupported video format or codec not found: \" + video_par.codec_id() + \".\");\n            }\n\n            /* Allocate a codec context for the decoder */\n            if ((video_c = avcodec_alloc_context3(codec)) == null) {\n                throw new Exception(\"avcodec_alloc_context3() error: Could not allocate video decoding context.\");\n            }\n\n            /* copy the stream parameters from the muxer */\n            if ((ret = avcodec_parameters_to_context(video_c, video_st.codecpar())) < 0) {\n                releaseUnsafe();\n                throw new Exception(\"avcodec_parameters_to_context() error \" + ret + \": Could not copy the video stream parameters.\");\n            }\n\n            options = new AVDictionary(null);\n            for (Entry<String, String> e : videoOptions.entrySet()) {\n                av_dict_set(options, e.getKey(), e.getValue(), 0);\n            }\n\n            // Enable multithreading when available\n            video_c.thread_count(0);\n\n            // Open video codec\n            if ((ret = avcodec_open2(video_c, codec, options)) < 0) {\n                throw new Exception(\"avcodec_open2() error \" + ret + \": Could not open video codec.\");\n            }\n            av_dict_free(options);\n\n            // Hack to correct wrong frame rates that seem to be generated by some codecs\n            if (video_c.time_base().num() > 1000 && video_c.time_base().den() == 1) {\n                video_c.time_base().den(1000);\n            }\n\n            // Allocate video frame and an AVFrame structure for the RGB image\n            if ((picture = av_frame_alloc()) == null) {\n                throw new Exception(\"av_frame_alloc() error: Could not allocate raw picture frame.\");\n            }\n            if ((picture_rgb = av_frame_alloc()) == null) {\n                throw new Exception(\"av_frame_alloc() error: Could not allocate RGB picture frame.\");\n            }\n\n            initPictureRGB();\n        }\n\n        if (audio_st != null) {\n            // Find the decoder for the audio stream\n            AVCodec codec = avcodec_find_decoder_by_name(audioCodecName);\n            if (codec == null) {\n                codec = avcodec_find_decoder(audio_par.codec_id());\n            }\n            if (codec == null) {\n                throw new Exception(\"avcodec_find_decoder() error: Unsupported audio format or codec not found: \" + audio_par.codec_id() + \".\");\n            }\n\n            /* Allocate a codec context for the decoder */\n            if ((audio_c = avcodec_alloc_context3(codec)) == null) {\n                throw new Exception(\"avcodec_alloc_context3() error: Could not allocate audio decoding context.\");\n            }\n\n            /* copy the stream parameters from the muxer */\n            if ((ret = avcodec_parameters_to_context(audio_c, audio_st.codecpar())) < 0) {\n                releaseUnsafe();\n                throw new Exception(\"avcodec_parameters_to_context() error \" + ret + \": Could not copy the audio stream parameters.\");\n            }\n\n            options = new AVDictionary(null);\n            for (Entry<String, String> e : audioOptions.entrySet()) {\n                av_dict_set(options, e.getKey(), e.getValue(), 0);\n            }\n\n            // Enable multithreading when available\n            audio_c.thread_count(0);\n\n            // Open audio codec\n            if ((ret = avcodec_open2(audio_c, codec, options)) < 0) {\n                throw new Exception(\"avcodec_open2() error \" + ret + \": Could not open audio codec.\");\n            }\n            av_dict_free(options);\n\n            // Allocate audio samples frame\n            if ((samples_frame = av_frame_alloc()) == null) {\n                throw new Exception(\"av_frame_alloc() error: Could not allocate audio frame.\");\n            }\n\n            samples_ptr = new BytePointer[] { null };\n            samples_buf = new Buffer[] { null };\n        }\n        started = true;\n\n        }\n    }\n\n    private void initPictureRGB() {\n        int width  = imageWidth  > 0 ? imageWidth  : video_c.width();\n        int height = imageHeight > 0 ? imageHeight : video_c.height();\n\n        switch (imageMode) {\n            case COLOR:\n            case GRAY:\n                // If size changes I new allocation is needed -> free the old one.\n                if (image_ptr != null) {\n                    // First kill all references, then free it.\n                    image_buf = null;\n                    BytePointer[] temp = image_ptr;\n                    image_ptr = null;\n                    av_free(temp[0]);\n                }\n                int fmt = getPixelFormat();\n\n                // work around bug in swscale: https://trac.ffmpeg.org/ticket/1031\n                int align = 32;\n                int stride = width;\n                for (int i = 1; i <= align; i += i) {\n                     stride = (width + (i - 1)) & ~(i - 1);\n                     av_image_fill_linesizes(picture_rgb.linesize(), fmt, stride);\n                     if ((picture_rgb.linesize(0) & (align - 1)) == 0) {\n                        break;\n                    }\n                }\n\n                // Determine required buffer size and allocate buffer\n                int size = av_image_get_buffer_size(fmt, stride, height, 1);\n                image_ptr = new BytePointer[] { new BytePointer(av_malloc(size)).capacity(size) };\n                image_buf = new Buffer[] { image_ptr[0].asBuffer() };\n\n                // Assign appropriate parts of buffer to image planes in picture_rgb\n                // Note that picture_rgb is an AVFrame, but AVFrame is a superset of AVPicture\n                av_image_fill_arrays(new PointerPointer(picture_rgb), picture_rgb.linesize(), image_ptr[0], fmt, stride, height, 1);\n                picture_rgb.format(fmt);\n                picture_rgb.width(width);\n                picture_rgb.height(height);\n                break;\n\n            case RAW:\n                image_ptr = new BytePointer[] { null };\n                image_buf = new Buffer[] { null };\n                break;\n\n            default:\n                assert false;\n        }\n    }\n\n    @Override public void stop() throws Exception {\n        release();\n    }\n\n    @Override public synchronized void trigger() throws Exception {\n        if (oc == null || oc.isNull()) {\n            throw new Exception(\"Could not trigger: No AVFormatContext. (Has start() been called?)\");\n        }\n        if (pkt.stream_index() != -1) {\n            av_packet_unref(pkt);\n            pkt.stream_index(-1);\n        }\n        for (int i = 0; i < numBuffers+1; i++) {\n            if (av_read_frame(oc, pkt) < 0) {\n                return;\n            }\n            av_packet_unref(pkt);\n        }\n    }\n\n    private void processImage() throws Exception {\n        frame.imageWidth  = imageWidth  > 0 ? imageWidth  : video_c.width();\n        frame.imageHeight = imageHeight > 0 ? imageHeight : video_c.height();\n        frame.imageDepth = Frame.DEPTH_UBYTE;\n        switch (imageMode) {\n            case COLOR:\n            case GRAY:\n                // Deinterlace Picture\n                if (deinterlace) {\n                    throw new Exception(\"Cannot deinterlace: Functionality moved to FFmpegFrameFilter.\");\n                }\n\n                // Has the size changed?\n                if (frame.imageWidth != picture_rgb.width() || frame.imageHeight != picture_rgb.height()) {\n                    initPictureRGB();\n                }\n\n                // Copy \"metadata\" fields\n                av_frame_copy_props(picture_rgb, picture);\n\n                // Convert the image into BGR or GRAY format that OpenCV uses\n                img_convert_ctx = sws_getCachedContext(img_convert_ctx,\n                        video_c.width(), video_c.height(), video_c.pix_fmt(),\n                        frame.imageWidth, frame.imageHeight, getPixelFormat(),\n                        imageScalingFlags != 0 ? imageScalingFlags : SWS_BILINEAR,\n                        null, null, (DoublePointer)null);\n                if (img_convert_ctx == null) {\n                    throw new Exception(\"sws_getCachedContext() error: Cannot initialize the conversion context.\");\n                }\n\n                // Convert the image from its native format to RGB or GRAY\n                sws_scale(img_convert_ctx, new PointerPointer(picture), picture.linesize(), 0,\n                        video_c.height(), new PointerPointer(picture_rgb), picture_rgb.linesize());\n                frame.imageStride = picture_rgb.linesize(0);\n                frame.image = image_buf;\n                frame.opaque = picture_rgb;\n                break;\n\n            case RAW:\n                frame.imageStride = picture.linesize(0);\n                BytePointer ptr = picture.data(0);\n                if (ptr != null && !ptr.equals(image_ptr[0])) {\n                    image_ptr[0] = ptr.capacity(frame.imageHeight * frame.imageStride);\n                    image_buf[0] = ptr.asBuffer();\n                }\n                frame.image = image_buf;\n                frame.opaque = picture;\n                break;\n\n            default:\n                assert false;\n        }\n        frame.image[0].limit(frame.imageHeight * frame.imageStride);\n        frame.imageChannels = frame.imageStride / frame.imageWidth;\n    }\n\n    private void processSamples() throws Exception {\n        int ret;\n\n        int sample_format = samples_frame.format();\n        int planes = av_sample_fmt_is_planar(sample_format) != 0 ? (int)samples_frame.channels() : 1;\n        int data_size = av_samples_get_buffer_size((IntPointer)null, audio_c.channels(),\n                samples_frame.nb_samples(), audio_c.sample_fmt(), 1) / planes;\n        if (samples_buf == null || samples_buf.length != planes) {\n            samples_ptr = new BytePointer[planes];\n            samples_buf = new Buffer[planes];\n        }\n        frame.sampleRate = audio_c.sample_rate();\n        frame.audioChannels = audio_c.channels();\n        frame.samples = samples_buf;\n        frame.opaque = samples_frame;\n        int sample_size = data_size / av_get_bytes_per_sample(sample_format);\n        for (int i = 0; i < planes; i++) {\n            BytePointer p = samples_frame.data(i);\n            if (!p.equals(samples_ptr[i]) || samples_ptr[i].capacity() < data_size) {\n                samples_ptr[i] = p.capacity(data_size);\n                ByteBuffer b   = p.asBuffer();\n                switch (sample_format) {\n                    case AV_SAMPLE_FMT_U8:\n                    case AV_SAMPLE_FMT_U8P:  samples_buf[i] = b; break;\n                    case AV_SAMPLE_FMT_S16:\n                    case AV_SAMPLE_FMT_S16P: samples_buf[i] = b.asShortBuffer();  break;\n                    case AV_SAMPLE_FMT_S32:\n                    case AV_SAMPLE_FMT_S32P: samples_buf[i] = b.asIntBuffer();    break;\n                    case AV_SAMPLE_FMT_FLT:\n                    case AV_SAMPLE_FMT_FLTP: samples_buf[i] = b.asFloatBuffer();  break;\n                    case AV_SAMPLE_FMT_DBL:\n                    case AV_SAMPLE_FMT_DBLP: samples_buf[i] = b.asDoubleBuffer(); break;\n                    default: assert false;\n                }\n            }\n            samples_buf[i].position(0).limit(sample_size);\n        }\n\n        if (audio_c.channels() != getAudioChannels() || audio_c.sample_fmt() != getSampleFormat() || audio_c.sample_rate() != getSampleRate()) {\n            if (samples_convert_ctx == null || samples_channels != getAudioChannels() || samples_format != getSampleFormat() || samples_rate != getSampleRate()) {\n                samples_convert_ctx = swr_alloc_set_opts(samples_convert_ctx, av_get_default_channel_layout(getAudioChannels()), getSampleFormat(), getSampleRate(),\n                        av_get_default_channel_layout(audio_c.channels()), audio_c.sample_fmt(), audio_c.sample_rate(), 0, null);\n                if (samples_convert_ctx == null) {\n                    throw new Exception(\"swr_alloc_set_opts() error: Cannot allocate the conversion context.\");\n                } else if ((ret = swr_init(samples_convert_ctx)) < 0) {\n                    throw new Exception(\"swr_init() error \" + ret + \": Cannot initialize the conversion context.\");\n                }\n                samples_channels = getAudioChannels();\n                samples_format = getSampleFormat();\n                samples_rate = getSampleRate();\n            }\n\n            int sample_size_in = samples_frame.nb_samples();\n            int planes_out = av_sample_fmt_is_planar(samples_format) != 0 ? (int)samples_frame.channels() : 1;\n            int sample_size_out = swr_get_out_samples(samples_convert_ctx, sample_size_in);\n            int sample_bytes_out = av_get_bytes_per_sample(samples_format);\n            int buffer_size_out = sample_size_out * sample_bytes_out * (planes_out > 1 ? 1 : samples_channels);\n            if (samples_buf_out == null || samples_buf.length != planes_out || samples_ptr_out[0].capacity() < buffer_size_out) {\n                for (int i = 0; samples_ptr_out != null && i < samples_ptr_out.length; i++) {\n                    av_free(samples_ptr_out[i].position(0));\n                }\n                samples_ptr_out = new BytePointer[planes_out];\n                samples_buf_out = new Buffer[planes_out];\n\n                for (int i = 0; i < planes_out; i++) {\n                    samples_ptr_out[i] = new BytePointer(av_malloc(buffer_size_out)).capacity(buffer_size_out);\n                    ByteBuffer b = samples_ptr_out[i].asBuffer();\n                    switch (samples_format) {\n                        case AV_SAMPLE_FMT_U8:\n                        case AV_SAMPLE_FMT_U8P:  samples_buf_out[i] = b; break;\n                        case AV_SAMPLE_FMT_S16:\n                        case AV_SAMPLE_FMT_S16P: samples_buf_out[i] = b.asShortBuffer();  break;\n                        case AV_SAMPLE_FMT_S32:\n                        case AV_SAMPLE_FMT_S32P: samples_buf_out[i] = b.asIntBuffer();    break;\n                        case AV_SAMPLE_FMT_FLT:\n                        case AV_SAMPLE_FMT_FLTP: samples_buf_out[i] = b.asFloatBuffer();  break;\n                        case AV_SAMPLE_FMT_DBL:\n                        case AV_SAMPLE_FMT_DBLP: samples_buf_out[i] = b.asDoubleBuffer(); break;\n                        default: assert false;\n                    }\n                }\n            }\n            frame.sampleRate = samples_rate;\n            frame.audioChannels = samples_channels;\n            frame.samples = samples_buf_out;\n\n            if ((ret = swr_convert(samples_convert_ctx, plane_ptr.put(samples_ptr_out), sample_size_out, plane_ptr2.put(samples_ptr), sample_size_in)) < 0) {\n                throw new Exception(\"swr_convert() error \" + ret + \": Cannot convert audio samples.\");\n            }\n            for (int i = 0; i < planes_out; i++) {\n                samples_ptr_out[i].position(0).limit(ret * (planes_out > 1 ? 1 : samples_channels));\n                samples_buf_out[i].position(0).limit(ret * (planes_out > 1 ? 1 : samples_channels));\n            }\n        }\n    }\n\n    public Frame grab() throws Exception {\n        return grabFrame(true, true, true, false, true);\n    }\n    public Frame grabImage() throws Exception {\n        return grabFrame(false, true, true, false, false);\n    }\n    public Frame grabSamples() throws Exception {\n        return grabFrame(true, false, true, false, false);\n    }\n    public Frame grabKeyFrame() throws Exception {\n        return grabFrame(false, true, true, true, false);\n    }\n    public Frame grabFrame(boolean doAudio, boolean doVideo, boolean doProcessing, boolean keyFrames) throws Exception {\n        return grabFrame(doAudio, doVideo, doProcessing, keyFrames, true);\n    }\n    public synchronized Frame grabFrame(boolean doAudio, boolean doVideo, boolean doProcessing, boolean keyFrames, boolean doData) throws Exception {\n        try (PointerScope scope = new PointerScope()) {\n\n        if (oc == null || oc.isNull()) {\n            throw new Exception(\"Could not grab: No AVFormatContext. (Has start() been called?)\");\n        } else if ((!doVideo || video_st == null) && (!doAudio || audio_st == null) && !doData) {\n            return null;\n        }\n        if (!started) {\n            throw new Exception(\"start() was not called successfully!\");\n        }\n\n        boolean videoFrameGrabbed = frameGrabbed && frame.image != null;\n        boolean audioFrameGrabbed = frameGrabbed && frame.samples != null;\n        boolean dataFrameGrabbed = frameGrabbed && frame.data != null;\n        frameGrabbed = false;\n        if (doVideo && videoFrameGrabbed) {\n            if (doProcessing) {\n                processImage();\n            }\n            frame.keyFrame = picture.key_frame() != 0;\n            return frame;\n        } else if (doAudio && audioFrameGrabbed) {\n            if (doProcessing) {\n                processSamples();\n            }\n            frame.keyFrame = samples_frame.key_frame() != 0;\n            return frame;\n        } else if (doData && dataFrameGrabbed) {\n            return frame;\n        }\n\n        frame.keyFrame = false;\n        frame.imageWidth = 0;\n        frame.imageHeight = 0;\n        frame.imageDepth = 0;\n        frame.imageChannels = 0;\n        frame.imageStride = 0;\n        frame.image = null;\n        frame.sampleRate = 0;\n        frame.audioChannels = 0;\n        frame.samples = null;\n        frame.data = null;\n        frame.opaque = null;\n        frame.type = null;\n\n        boolean done = false;\n        boolean readPacket = pkt.stream_index() == -1;\n        while (!done) {\n            int ret = 0;\n            if (readPacket) {\n                if (pkt.stream_index() != -1) {\n                    // Free the packet that was allocated by av_read_frame\n                    av_packet_unref(pkt);\n                    pkt.stream_index(-1);\n                }\n                if ((ret = av_read_frame(oc, pkt)) < 0) {\n                    if (ret == AVERROR_EAGAIN()) {\n                        try {\n                            Thread.sleep(10);\n                            continue;\n                        } catch (InterruptedException ex) {\n                            // reset interrupt to be nice\n                            Thread.currentThread().interrupt();\n                            return null;\n                        }\n                    }\n                    if ((doVideo && video_st != null) || (doAudio && audio_st != null)) {\n                        // The video or audio codec may have buffered some frames\n                        pkt.stream_index(doVideo && video_st != null ? video_st.index() : audio_st.index());\n                        pkt.flags(AV_PKT_FLAG_KEY);\n                        pkt.data(null);\n                        pkt.size(0);\n                    } else {\n                        pkt.stream_index(-1);\n                        return null;\n                    }\n                }\n            }\n\n            frame.streamIndex = pkt.stream_index();\n\n            // Is this a packet from the video stream?\n            if (doVideo && video_st != null && frame.streamIndex == video_st.index()\n                    && (!keyFrames || pkt.flags() == AV_PKT_FLAG_KEY)) {\n                // Decode video frame\n                if (readPacket) {\n                    ret = avcodec_send_packet(video_c, pkt);\n                    if (pkt.data() == null && pkt.size() == 0) {\n                        pkt.stream_index(-1);\n                    }\n                    if (ret == AVERROR_EAGAIN() || ret == AVERROR_EOF()) {\n                        // The video codec may have buffered some frames\n                    } else if (ret < 0) {\n                        // Ignore errors to emulate the behavior of the old API\n                        // throw new Exception(\"avcodec_send_packet() error \" + ret + \": Error sending a video packet for decoding.\");\n                    }\n                }\n\n                // Did we get a video frame?\n                while (!done) {\n                    ret = avcodec_receive_frame(video_c, picture);\n                    if (ret == AVERROR_EAGAIN() || ret == AVERROR_EOF()) {\n                        if (pkt.data() == null && pkt.size() == 0) {\n                            pkt.stream_index(-1);\n                            doVideo = false;\n                            if (doAudio) {\n                                readPacket = false;\n                                break;\n                            }\n                            return null;\n                        } else {\n                            readPacket = true;\n                            break;\n                        }\n                    } else if (ret < 0) {\n                        // Ignore errors to emulate the behavior of the old API\n                        // throw new Exception(\"avcodec_receive_frame() error \" + ret + \": Error during video decoding.\");\n                        readPacket = true;\n                        break;\n                    }\n\n                    if (!keyFrames || picture.pict_type() == AV_PICTURE_TYPE_I) {\n                        long pts = picture.best_effort_timestamp();\n                        AVRational time_base = video_st.time_base();\n                        timestamp = 1000000L * pts * time_base.num() / time_base.den();\n                        // best guess, AVCodecContext.frame_number = number of decoded frames...\n                        frameNumber = (int)Math.round(timestamp * getFrameRate() / 1000000L);\n                        frame.image = image_buf;\n                        if (doProcessing) {\n                            processImage();\n                        }\n                        /* the picture is allocated by the decoder. no need to\n                           free it */\n                        done = true;\n                        frame.timestamp = timestamp;\n                        frame.keyFrame = picture.key_frame() != 0;\n                        frame.pictType = (char)av_get_picture_type_char(picture.pict_type());\n                        frame.type = Frame.Type.VIDEO;\n                    }\n                }\n            } else if (doAudio && audio_st != null && frame.streamIndex == audio_st.index()) {\n                // Decode audio frame\n                if (readPacket) {\n                    ret = avcodec_send_packet(audio_c, pkt);\n                    if (ret < 0) {\n                        // Ignore errors to emulate the behavior of the old API\n                        // throw new Exception(\"avcodec_send_packet() error \" + ret + \": Error sending an audio packet for decoding.\");\n                    }\n                }\n\n                // Did we get an audio frame?\n                while (!done) {\n                    ret = avcodec_receive_frame(audio_c, samples_frame);\n                    if (ret == AVERROR_EAGAIN() || ret == AVERROR_EOF()) {\n                        if (pkt.data() == null && pkt.size() == 0) {\n                            pkt.stream_index(-1);\n                            doAudio = false;\n                            return null;\n                        } else {\n                            readPacket = true;\n                            break;\n                        }\n                    } else if (ret < 0) {\n                        // Ignore errors to emulate the behavior of the old API\n                        // throw new Exception(\"avcodec_receive_frame() error \" + ret + \": Error during audio decoding.\");\n                        readPacket = true;\n                        break;\n                    }\n\n                    long pts = samples_frame.best_effort_timestamp();\n                    AVRational time_base = audio_st.time_base();\n                    timestamp = 1000000L * pts * time_base.num() / time_base.den();\n                    frame.samples = samples_buf;\n                    /* if a frame has been decoded, output it */\n                    if (doProcessing) {\n                        processSamples();\n                    }\n                    done = true;\n                    frame.timestamp = timestamp;\n                    frame.keyFrame = samples_frame.key_frame() != 0;\n                    frame.type = Frame.Type.AUDIO;\n                }\n            } else if (readPacket && doData\n                    && frame.streamIndex > -1 && frame.streamIndex < streams.length\n                    && streams[frame.streamIndex] != AVMEDIA_TYPE_VIDEO && streams[frame.streamIndex] != AVMEDIA_TYPE_AUDIO) {\n                // Export the stream byte data for non audio / video frames\n                frame.data = pkt.data().position(0).capacity(pkt.size()).asByteBuffer();\n                frame.opaque = pkt;\n                done = true;\n                switch (streams[frame.streamIndex]) {\n                    case AVMEDIA_TYPE_DATA: frame.type = Frame.Type.DATA; break;\n                    case AVMEDIA_TYPE_SUBTITLE: frame.type = Frame.Type.SUBTITLE; break;\n                    case AVMEDIA_TYPE_ATTACHMENT: frame.type = Frame.Type.ATTACHMENT; break;\n                    default: frame.type = null;\n                }\n            } else {\n                // Current packet is not needed (different stream index required)\n                readPacket = true;\n            }\n        }\n        return frame;\n\n        }\n    }\n\n    public synchronized AVPacket grabPacket() throws Exception {\n        if (oc == null || oc.isNull()) {\n            throw new Exception(\"Could not grab: No AVFormatContext. (Has start() been called?)\");\n        }\n        if (!started) {\n            throw new Exception(\"start() was not called successfully!\");\n        }\n\n        // Return the next frame of a stream.\n        if (av_read_frame(oc, pkt) < 0) {\n            return null;\n        }\n\n        return pkt;\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport java.io.BufferedInputStream;\n\nimport java.io.File;\n\nimport java.io.IOException;\n\nimport java.io.InputStream;\n\nimport java.net.URL;\n\nimport java.nio.Buffer;\n\nimport java.nio.ByteBuffer;\n\nimport java.util.Collections;\n\nimport java.util.EnumSet;\n\nimport java.util.HashMap;\n\nimport java.util.Map;\n\nimport java.util.Map.Entry;\n\nimport org.bytedeco.javacpp.BytePointer;\n\nimport org.bytedeco.javacpp.DoublePointer;\n\nimport org.bytedeco.javacpp.IntPointer;\n\nimport org.bytedeco.javacpp.Loader;\n\nimport org.bytedeco.javacpp.Pointer;\n\nimport org.bytedeco.javacpp.PointerScope;\n\nimport org.bytedeco.javacpp.PointerPointer;\n\nimport org.bytedeco.ffmpeg.avcodec;\n\nimport org.bytedeco.ffmpeg.avformat;\n\nimport org.bytedeco.ffmpeg.avutil;\n\nimport org.bytedeco.ffmpeg.swresample;\n\nimport org.bytedeco.ffmpeg.swscale;\n\nimport static org.bytedeco.ffmpeg.global.avcodec;\n\nimport static org.bytedeco.ffmpeg.global.avdevice;\n\nimport static org.bytedeco.ffmpeg.global.avformat;\n\nimport static org.bytedeco.ffmpeg.global.avutil;\n\nimport static org.bytedeco.ffmpeg.global.swresample;\n\nimport static org.bytedeco.ffmpeg.global.swscale;\n\npublic class FFmpegFrameGrabber extends FrameGrabber {\n    static public class Exception extends FrameGrabber {\n        public Exception(String message);\n        public Exception(String message, Throwable cause);\n    }\n    static public String getDeviceDescriptions()throws Exception;\n    static public FFmpegFrameGrabber createDefault(File deviceFile)throws Exception;\n    static public FFmpegFrameGrabber createDefault(String devicePath)throws Exception;\n    static public FFmpegFrameGrabber createDefault(int deviceNumber)throws Exception;\n    static private Exception loadingException;\n    static public  tryLoad()throws Exception;\n    public FFmpegFrameGrabber(URL url);\n    public FFmpegFrameGrabber(File file);\n    public FFmpegFrameGrabber(String filename);\n    public FFmpegFrameGrabber(InputStream inputStream);\n    public FFmpegFrameGrabber(InputStream inputStream, int maximumSize);\n    public  release()throws Exception;\n    public synchronized  releaseUnsafe()throws Exception;\n    protected  finalize()throws Throwable;\n    static Map<Pointer, InputStream> inputStreams;\n    static class ReadCallback extends Read_packet_Pointer_BytePointer_int {\n        public int call(Pointer opaque, BytePointer buf, int buf_size);\n    }\n    static class SeekCallback extends Seek_Pointer_long_int {\n        public long call(Pointer opaque, long offset, int whence);\n    }\n    static ReadCallback readCallback;\n    static SeekCallback seekCallback;\n    private InputStream inputStream;\n    private boolean closeInputStream;\n    private int maximumSize;\n    private AVIOContext avio;\n    private String filename;\n    private AVFormatContext oc;\n    private AVStream video_st, audio_st;\n    private AVCodecContext video_c, audio_c;\n    private AVFrame picture, picture_rgb;\n    private BytePointer image_ptr;\n    private Buffer image_buf;\n    private AVFrame samples_frame;\n    private BytePointer samples_ptr;\n    private Buffer samples_buf;\n    private BytePointer samples_ptr_out;\n    private Buffer samples_buf_out;\n    private PointerPointer plane_ptr, plane_ptr2;\n    private AVPacket pkt;\n    private SwsContext img_convert_ctx;\n    private SwrContext samples_convert_ctx;\n    private int samples_channels, samples_format, samples_rate;\n    private boolean frameGrabbed;\n    private Frame frame;\n    private int[] streams;\n    volatile private boolean started;\n    public boolean isCloseInputStream();\n    public  setCloseInputStream(boolean closeInputStream);\n    public boolean hasVideo();\n    public boolean hasAudio();\n    public double getGamma();\n    public String getFormat();\n    public int getImageWidth();\n    public int getImageHeight();\n    public int getAudioChannels();\n    public int getPixelFormat();\n    public int getVideoCodec();\n    public String getVideoCodecName();\n    public int getVideoBitrate();\n    public double getAspectRatio();\n    public double getFrameRate();\n    public double getAudioFrameRate();\n    public double getVideoFrameRate();\n    public int getAudioCodec();\n    public String getAudioCodecName();\n    public int getAudioBitrate();\n    public int getSampleFormat();\n    public int getSampleRate();\n    public Map<String, String> getMetadata();\n    public Map<String, String> getVideoMetadata();\n    public Map<String, String> getAudioMetadata();\n    public String getMetadata(String key);\n    public String getVideoMetadata(String key);\n    public String getAudioMetadata(String key);\n    public Map<String, Buffer> getVideoSideData();\n    public Buffer getVideoSideData(String key);\n    public double getDisplayRotation();\n    public Map<String, Buffer> getAudioSideData();\n    public Buffer getAudioSideData(String key);\n    public  setFrameNumber(int frameNumber)throws Exception;\n    public  setVideoFrameNumber(int frameNumber)throws Exception;\n    public  setAudioFrameNumber(int frameNumber)throws Exception;\n    public  setTimestamp(long timestamp)throws Exception;\n    public  setTimestamp(long timestamp, boolean checkFrame)throws Exception;\n    public  setVideoTimestamp(long timestamp)throws Exception;\n    public  setAudioTimestamp(long timestamp)throws Exception;\n    synchronized private  setTimestamp(long timestamp, EnumSet<Frame> frameTypesToSeek)throws Exception;\n    public int getLengthInFrames();\n    public long getLengthInTime();\n    public int getLengthInVideoFrames();\n    public int getLengthInAudioFrames();\n    public AVFormatContext getFormatContext();\n    public  start()throws Exception;\n    public  start(boolean findStreamInfo)throws Exception;\n    public  startUnsafe()throws Exception;\n    public synchronized  startUnsafe(boolean findStreamInfo)throws Exception;\n    private  initPictureRGB();\n    public  stop()throws Exception;\n    public synchronized  trigger()throws Exception;\n    private  processImage()throws Exception;\n    private  processSamples()throws Exception;\n    public Frame grab()throws Exception;\n    public Frame grabImage()throws Exception;\n    public Frame grabSamples()throws Exception;\n    public Frame grabKeyFrame()throws Exception;\n    public Frame grabFrame(boolean doAudio, boolean doVideo, boolean doProcessing, boolean keyFrames)throws Exception;\n    public synchronized Frame grabFrame(boolean doAudio, boolean doVideo, boolean doProcessing, boolean keyFrames, boolean doData)throws Exception;\n    public synchronized AVPacket grabPacket()throws Exception;\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "Frame.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/Frame.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**Care must be taken if this method is to be used in conjunction with movie recordings.\n     *  Cloning a frame containing a full HD picture (alpha channel included) would take 1920 x 1080 * 4 = 8.294.400 Bytes.\n     *  Expect a heap overflow exception when using this method without cleaning up.\n     *\n     * @return A deep copy of this frame.\n     * @see {@link #cloneBufferArray}\n     *\n     * @author Extension proposed by Dragos Dutu\n     * */",
        "source_code": "\n@Override\npublic Frame clone() {\n    Frame newFrame = new Frame();\n\n    // Video part\n    newFrame.imageWidth = imageWidth;\n    newFrame.imageHeight = imageHeight;\n    newFrame.imageDepth = imageDepth;\n    newFrame.imageChannels = imageChannels;\n    newFrame.imageStride = imageStride;\n    newFrame.keyFrame = keyFrame;\n    newFrame.pictType = pictType;\n    newFrame.streamIndex = streamIndex;\n    newFrame.type = type;\n    newFrame.opaque = new Pointer[3];\n    if (image != null) {\n        newFrame.image = new Buffer[image.length];\n        ((Pointer[])newFrame.opaque)[0] = cloneBufferArray(image, newFrame.image);\n    }\n\n    // Audio part\n    newFrame.audioChannels = audioChannels;\n    newFrame.sampleRate = sampleRate;\n    if (samples != null) {\n        newFrame.samples = new Buffer[samples.length];\n        ((Pointer[])newFrame.opaque)[1] = cloneBufferArray(samples, newFrame.samples);\n    }\n\n    // Other data streams\n    if (data != null) {\n        ByteBuffer[] dst = new ByteBuffer[1];\n        ((Pointer[])newFrame.opaque)[2] = cloneBufferArray(new ByteBuffer[]{data}, dst);\n        newFrame.data = dst[0];\n    }\n\n    // Add timestamp\n    newFrame.timestamp = timestamp;\n\n    return newFrame;\n}\n",
        "class_name": "Frame",
        "method_name": "clone",
        "argument_name": [],
        "full_context": "/*\n * Copyright (C) 2015-2021 Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport java.nio.Buffer;\nimport java.nio.ByteBuffer;\nimport java.nio.DoubleBuffer;\nimport java.nio.FloatBuffer;\nimport java.nio.IntBuffer;\nimport java.nio.LongBuffer;\nimport java.nio.ShortBuffer;\nimport java.util.EnumSet;\n\nimport org.bytedeco.javacpp.BytePointer;\nimport org.bytedeco.javacpp.DoublePointer;\nimport org.bytedeco.javacpp.FloatPointer;\nimport org.bytedeco.javacpp.IntPointer;\nimport org.bytedeco.javacpp.LongPointer;\nimport org.bytedeco.javacpp.Pointer;\nimport org.bytedeco.javacpp.ShortPointer;\nimport org.bytedeco.javacpp.indexer.ByteIndexer;\nimport org.bytedeco.javacpp.indexer.DoubleIndexer;\nimport org.bytedeco.javacpp.indexer.FloatIndexer;\nimport org.bytedeco.javacpp.indexer.Indexable;\nimport org.bytedeco.javacpp.indexer.Indexer;\nimport org.bytedeco.javacpp.indexer.IntIndexer;\nimport org.bytedeco.javacpp.indexer.LongIndexer;\nimport org.bytedeco.javacpp.indexer.ShortIndexer;\nimport org.bytedeco.javacpp.indexer.UByteIndexer;\nimport org.bytedeco.javacpp.indexer.UShortIndexer;\n\n/**\n * A class to manage the data of audio and video frames. It it used by\n * {@link CanvasFrame}, {@link FrameGrabber}, {@link FrameRecorder}, and their\n * subclasses. We can also make the link with other APIs, such as Android,\n * Java 2D, FFmpeg, and OpenCV, via a {@link FrameConverter}.\n *\n * @author Samuel Audet\n */\npublic class Frame implements AutoCloseable, Indexable {\n    /** A flag set by a FrameGrabber or a FrameRecorder to indicate a key frame. */\n    public boolean keyFrame;\n\n    /** The type of the image frame ('I', 'P', 'B', etc). */\n    public char pictType;\n\n    /** Constants to be used for {@link #imageDepth}. */\n    public static final int\n            DEPTH_BYTE   =  -8,\n            DEPTH_UBYTE  =   8,\n            DEPTH_SHORT  = -16,\n            DEPTH_USHORT =  16,\n            DEPTH_INT    = -32,\n            DEPTH_LONG   = -64,\n            DEPTH_FLOAT  =  32,\n            DEPTH_DOUBLE =  64;\n\n    /** Constants defining data type in the frame. */\n    public static enum Type {\n        VIDEO,\n        AUDIO,\n        DATA,\n        SUBTITLE,\n        ATTACHMENT\n    }\n\n    /** Information associated with the {@link #image} field. */\n    public int imageWidth, imageHeight, imageDepth, imageChannels, imageStride;\n\n    /**\n     * Buffers to hold image pixels from multiple channels for a video frame.\n     * Most of the software supports packed data only, but an array is provided\n     * to allow users to store images in a planar format as well.\n     */\n    public Buffer[] image;\n\n    /** Information associated with the {@link #samples} field. */\n    public int sampleRate, audioChannels;\n\n    /** Buffers to hold audio samples from multiple channels for an audio frame. */\n    public Buffer[] samples;\n\n    /** Buffer to hold a data stream associated with a frame. */\n    public ByteBuffer data;\n\n    /** Stream number the audio|video|other data is associated with. */\n    public int streamIndex;\n\n    /** The type of the stream. */\n    public Type type;\n\n    /** The underlying data object, for example, Pointer, AVFrame, IplImage, or Mat. */\n    public Object opaque;\n\n    /** Timestamp of the frame creation in microseconds. */\n    public long timestamp;\n\n    /** Returns {@code Math.abs(depth) / 8}. */\n    public static int pixelSize(int depth) {\n        return Math.abs(depth) / 8;\n    }\n\n    /** Empty constructor. */\n    public Frame() { }\n\n    /** Allocates a new packed image frame in native memory where rows are 8-byte aligned. */\n    public Frame(int width, int height, int depth, int channels) {\n        this(width, height, depth, channels, ((width * channels * pixelSize(depth) + 7) & ~7) / pixelSize(depth));\n    }\n    public Frame(int width, int height, int depth, int channels, int imageStride) {\n        this.imageWidth = width;\n        this.imageHeight = height;\n        this.imageDepth = depth;\n        this.imageChannels = channels;\n        this.imageStride = imageStride;\n        this.pictType = '\\0';\n        this.image = new Buffer[1];\n        this.data = null;\n        this.streamIndex = -1;\n        this.type = null;\n\n        Pointer pointer = new BytePointer(imageHeight * imageStride * pixelSize(depth));\n        ByteBuffer buffer = pointer.asByteBuffer();\n        switch (imageDepth) {\n            case DEPTH_BYTE:\n            case DEPTH_UBYTE:  image[0] = buffer;                  break;\n            case DEPTH_SHORT:\n            case DEPTH_USHORT: image[0] = buffer.asShortBuffer();  break;\n            case DEPTH_INT:    image[0] = buffer.asIntBuffer();    break;\n            case DEPTH_LONG:   image[0] = buffer.asLongBuffer();   break;\n            case DEPTH_FLOAT:  image[0] = buffer.asFloatBuffer();  break;\n            case DEPTH_DOUBLE: image[0] = buffer.asDoubleBuffer(); break;\n            default: throw new UnsupportedOperationException(\"Unsupported depth value: \" + imageDepth);\n        }\n        opaque = new Pointer[] {pointer.retainReference()};\n    }\n\n    /** Returns {@code createIndexer(true, 0)}. */\n    public <I extends Indexer> I createIndexer() {\n        return (I)createIndexer(true, 0);\n    }\n    @Override public <I extends Indexer> I createIndexer(boolean direct) {\n        return (I)createIndexer(direct, 0);\n    }\n    /** Returns an {@link Indexer} for the <i>i</i>th image plane. */\n    public <I extends Indexer> I createIndexer(boolean direct, int i) {\n        long[] sizes = {imageHeight, imageWidth, imageChannels};\n        long[] strides = {imageStride, imageChannels, 1};\n        Buffer buffer = image[i];\n        Object array = buffer.hasArray() ? buffer.array() : null;\n        switch (imageDepth) {\n            case DEPTH_UBYTE:\n                return array != null ? (I)UByteIndexer.create((byte[])array, sizes, strides).indexable(this)\n                            : direct ? (I)UByteIndexer.create((ByteBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)UByteIndexer.create(new BytePointer((ByteBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_BYTE:\n                return array != null ? (I)ByteIndexer.create((byte[])array, sizes, strides).indexable(this)\n                            : direct ? (I)ByteIndexer.create((ByteBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)ByteIndexer.create(new BytePointer((ByteBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_USHORT:\n                return array != null ? (I)UShortIndexer.create((short[])array, sizes, strides).indexable(this)\n                            : direct ? (I)UShortIndexer.create((ShortBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)UShortIndexer.create(new ShortPointer((ShortBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_SHORT:\n                return array != null ? (I)ShortIndexer.create((short[])array, sizes, strides).indexable(this)\n                            : direct ? (I)ShortIndexer.create((ShortBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)ShortIndexer.create(new ShortPointer((ShortBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_INT:\n                return array != null ? (I)IntIndexer.create((int[])array, sizes, strides).indexable(this)\n                            : direct ? (I)IntIndexer.create((IntBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)IntIndexer.create(new IntPointer((IntBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_LONG:\n                return array != null ? (I)LongIndexer.create((long[])array, sizes, strides).indexable(this)\n                            : direct ? (I)LongIndexer.create((LongBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)LongIndexer.create(new LongPointer((LongBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_FLOAT:\n                return array != null ? (I)FloatIndexer.create((float[])array, sizes, strides).indexable(this)\n                            : direct ? (I)FloatIndexer.create((FloatBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)FloatIndexer.create(new FloatPointer((FloatBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_DOUBLE:\n                return array != null ? (I)DoubleIndexer.create((double[])array, sizes, strides).indexable(this)\n                            : direct ? (I)DoubleIndexer.create((DoubleBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)DoubleIndexer.create(new DoublePointer((DoubleBuffer)buffer), sizes, strides, false).indexable(this);\n            default: assert false;\n        }\n        return null;\n    }\n\n    /**Care must be taken if this method is to be used in conjunction with movie recordings.\n     *  Cloning a frame containing a full HD picture (alpha channel included) would take 1920 x 1080 * 4 = 8.294.400 Bytes.\n     *  Expect a heap overflow exception when using this method without cleaning up.\n     *\n     * @return A deep copy of this frame.\n     * @see {@link #cloneBufferArray}\n     *\n     * @author Extension proposed by Dragos Dutu\n     * */\n    @Override\n    public Frame clone() {\n        Frame newFrame = new Frame();\n\n        // Video part\n        newFrame.imageWidth = imageWidth;\n        newFrame.imageHeight = imageHeight;\n        newFrame.imageDepth = imageDepth;\n        newFrame.imageChannels = imageChannels;\n        newFrame.imageStride = imageStride;\n        newFrame.keyFrame = keyFrame;\n        newFrame.pictType = pictType;\n        newFrame.streamIndex = streamIndex;\n        newFrame.type = type;\n        newFrame.opaque = new Pointer[3];\n        if (image != null) {\n            newFrame.image = new Buffer[image.length];\n            ((Pointer[])newFrame.opaque)[0] = cloneBufferArray(image, newFrame.image);\n        }\n\n        // Audio part\n        newFrame.audioChannels = audioChannels;\n        newFrame.sampleRate = sampleRate;\n        if (samples != null) {\n            newFrame.samples = new Buffer[samples.length];\n            ((Pointer[])newFrame.opaque)[1] = cloneBufferArray(samples, newFrame.samples);\n        }\n\n        // Other data streams\n        if (data != null) {\n            ByteBuffer[] dst = new ByteBuffer[1];\n            ((Pointer[])newFrame.opaque)[2] = cloneBufferArray(new ByteBuffer[]{data}, dst);\n            newFrame.data = dst[0];\n        }\n\n        // Add timestamp\n        newFrame.timestamp = timestamp;\n\n        return newFrame;\n    }\n\n    /**\n     * This private method takes a buffer array as input and returns a deep copy.\n     * It is assumed that all buffers in the input array are of the same subclass.\n     *\n     * @param srcBuffers - Buffer array to be cloned\n     * @param clonedBuffers - Buffer array to fill with clones\n     * @return Opaque object to store\n     *\n     *  @author Extension proposed by Dragos Dutu\n     */\n    private static Pointer cloneBufferArray(Buffer[] srcBuffers, Buffer[] clonedBuffers) {\n        Pointer opaque = null;\n\n        if (srcBuffers != null && srcBuffers.length > 0) {\n            int totalCapacity = 0;\n            for (int i = 0; i < srcBuffers.length; i++) {\n                srcBuffers[i].rewind();\n                totalCapacity += srcBuffers[i].capacity();\n            }\n\n            /*\n             * In order to optimize the transfer we need a type check.\n             *\n             * Most CPUs support hardware memory transfer for different data\n             * types, so it's faster to copy more bytes at once rather\n             * than one byte per iteration as in case of ByteBuffer.\n             *\n             * For example, Intel CPUs support MOVSB (byte transfer), MOVSW\n             * (word transfer), MOVSD (double word transfer), MOVSS (32 bit\n             * scalar single precision floating point), MOVSQ (quad word\n             * transfer) and so on...\n             *\n             * Type checking may be improved by changing the order in\n             * which a buffer is checked against. If it's likely that the\n             * expected buffer is of type \"ShortBuffer\", then it should be\n             * checked at first place.\n             *\n             */\n\n            if (srcBuffers[0] instanceof ByteBuffer) {\n                BytePointer pointer = new BytePointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((ByteBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof ShortBuffer) {\n                ShortPointer pointer = new ShortPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((ShortBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof IntBuffer) {\n                IntPointer pointer = new IntPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((IntBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof LongBuffer) {\n                LongPointer pointer = new LongPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((LongBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof FloatBuffer) {\n                FloatPointer pointer = new FloatPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((FloatBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof DoubleBuffer) {\n                DoublePointer pointer = new DoublePointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((DoubleBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            }\n\n            for (int i = 0; i < srcBuffers.length; i++) {\n                srcBuffers[i].rewind();\n                clonedBuffers[i].rewind();\n            }\n        }\n\n        if (opaque != null) {\n            opaque.retainReference();\n        }\n        return opaque;\n    }\n\n    /** Returns types of data containing in the frame */\n    public EnumSet<Type> getTypes() {\n        EnumSet<Type> type = EnumSet.noneOf(Type.class);\n        if (image != null) type.add(Type.VIDEO);\n        if (samples != null) type.add(Type.AUDIO);\n        if (data != null) type.add(Type.DATA);\n        return type;\n    }\n\n    @Override public void close() {\n        if (opaque instanceof Pointer[]) {\n            for (Pointer p : (Pointer[])opaque) {\n                if (p != null) {\n                    p.releaseReference();\n                    p = null;\n                }\n            }\n            opaque = null;\n        }\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport java.nio.Buffer;\n\nimport java.nio.ByteBuffer;\n\nimport java.nio.DoubleBuffer;\n\nimport java.nio.FloatBuffer;\n\nimport java.nio.IntBuffer;\n\nimport java.nio.LongBuffer;\n\nimport java.nio.ShortBuffer;\n\nimport java.util.EnumSet;\n\nimport org.bytedeco.javacpp.BytePointer;\n\nimport org.bytedeco.javacpp.DoublePointer;\n\nimport org.bytedeco.javacpp.FloatPointer;\n\nimport org.bytedeco.javacpp.IntPointer;\n\nimport org.bytedeco.javacpp.LongPointer;\n\nimport org.bytedeco.javacpp.Pointer;\n\nimport org.bytedeco.javacpp.ShortPointer;\n\nimport org.bytedeco.javacpp.indexer.ByteIndexer;\n\nimport org.bytedeco.javacpp.indexer.DoubleIndexer;\n\nimport org.bytedeco.javacpp.indexer.FloatIndexer;\n\nimport org.bytedeco.javacpp.indexer.Indexable;\n\nimport org.bytedeco.javacpp.indexer.Indexer;\n\nimport org.bytedeco.javacpp.indexer.IntIndexer;\n\nimport org.bytedeco.javacpp.indexer.LongIndexer;\n\nimport org.bytedeco.javacpp.indexer.ShortIndexer;\n\nimport org.bytedeco.javacpp.indexer.UByteIndexer;\n\nimport org.bytedeco.javacpp.indexer.UShortIndexer;\n\npublic class Frame implements AutoCloseable, Indexable {\n    public boolean keyFrame;\n    public char pictType;\n    static public final int DEPTH_BYTE, DEPTH_UBYTE, DEPTH_SHORT, DEPTH_USHORT, DEPTH_INT, DEPTH_LONG, DEPTH_FLOAT, DEPTH_DOUBLE;\n    static public enum Type{VIDEO, AUDIO, DATA, SUBTITLE, ATTACHMENT}\n    public int imageWidth, imageHeight, imageDepth, imageChannels, imageStride;\n    public Buffer image;\n    public int sampleRate, audioChannels;\n    public Buffer samples;\n    public ByteBuffer data;\n    public int streamIndex;\n    public Type type;\n    public Object opaque;\n    public long timestamp;\n    static public int pixelSize(int depth);\n    public Frame();\n    public Frame(int width, int height, int depth, int channels);\n    public Frame(int width, int height, int depth, int channels, int imageStride);\n    public I createIndexer();\n    public I createIndexer(boolean direct);\n    public I createIndexer(boolean direct, int i);\n    public Frame clone();\n    static private Pointer cloneBufferArray(Buffer srcBuffers, Buffer clonedBuffers);\n    public EnumSet<Type> getTypes();\n    public  close();\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "Frame.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/Frame.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**\n     * This private method takes a buffer array as input and returns a deep copy.\n     * It is assumed that all buffers in the input array are of the same subclass.\n     *\n     * @param srcBuffers - Buffer array to be cloned\n     * @param clonedBuffers - Buffer array to fill with clones\n     * @return Opaque object to store\n     *\n     *  @author Extension proposed by Dragos Dutu\n     */",
        "source_code": "\nprivate static Pointer cloneBufferArray(Buffer[] srcBuffers, Buffer[] clonedBuffers) {\n    Pointer opaque = null;\n\n    if (srcBuffers != null && srcBuffers.length > 0) {\n        int totalCapacity = 0;\n        for (int i = 0; i < srcBuffers.length; i++) {\n            srcBuffers[i].rewind();\n            totalCapacity += srcBuffers[i].capacity();\n        }\n\n        /*\n         * In order to optimize the transfer we need a type check.\n         *\n         * Most CPUs support hardware memory transfer for different data\n         * types, so it's faster to copy more bytes at once rather\n         * than one byte per iteration as in case of ByteBuffer.\n         *\n         * For example, Intel CPUs support MOVSB (byte transfer), MOVSW\n         * (word transfer), MOVSD (double word transfer), MOVSS (32 bit\n         * scalar single precision floating point), MOVSQ (quad word\n         * transfer) and so on...\n         *\n         * Type checking may be improved by changing the order in\n         * which a buffer is checked against. If it's likely that the\n         * expected buffer is of type \"ShortBuffer\", then it should be\n         * checked at first place.\n         *\n         */\n\n        if (srcBuffers[0] instanceof ByteBuffer) {\n            BytePointer pointer = new BytePointer(totalCapacity);\n            for (int i = 0; i < srcBuffers.length; i++) {\n                clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                        .asBuffer().put((ByteBuffer)srcBuffers[i]);\n                pointer.position(pointer.limit());\n            }\n            opaque = pointer;\n        } else if (srcBuffers[0] instanceof ShortBuffer) {\n            ShortPointer pointer = new ShortPointer(totalCapacity);\n            for (int i = 0; i < srcBuffers.length; i++) {\n                clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                        .asBuffer().put((ShortBuffer)srcBuffers[i]);\n                pointer.position(pointer.limit());\n            }\n            opaque = pointer;\n        } else if (srcBuffers[0] instanceof IntBuffer) {\n            IntPointer pointer = new IntPointer(totalCapacity);\n            for (int i = 0; i < srcBuffers.length; i++) {\n                clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                        .asBuffer().put((IntBuffer)srcBuffers[i]);\n                pointer.position(pointer.limit());\n            }\n            opaque = pointer;\n        } else if (srcBuffers[0] instanceof LongBuffer) {\n            LongPointer pointer = new LongPointer(totalCapacity);\n            for (int i = 0; i < srcBuffers.length; i++) {\n                clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                        .asBuffer().put((LongBuffer)srcBuffers[i]);\n                pointer.position(pointer.limit());\n            }\n            opaque = pointer;\n        } else if (srcBuffers[0] instanceof FloatBuffer) {\n            FloatPointer pointer = new FloatPointer(totalCapacity);\n            for (int i = 0; i < srcBuffers.length; i++) {\n                clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                        .asBuffer().put((FloatBuffer)srcBuffers[i]);\n                pointer.position(pointer.limit());\n            }\n            opaque = pointer;\n        } else if (srcBuffers[0] instanceof DoubleBuffer) {\n            DoublePointer pointer = new DoublePointer(totalCapacity);\n            for (int i = 0; i < srcBuffers.length; i++) {\n                clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                        .asBuffer().put((DoubleBuffer)srcBuffers[i]);\n                pointer.position(pointer.limit());\n            }\n            opaque = pointer;\n        }\n\n        for (int i = 0; i < srcBuffers.length; i++) {\n            srcBuffers[i].rewind();\n            clonedBuffers[i].rewind();\n        }\n    }\n\n    if (opaque != null) {\n        opaque.retainReference();\n    }\n    return opaque;\n}\n",
        "class_name": "Frame",
        "method_name": "cloneBufferArray",
        "argument_name": [
            "Buffer srcBuffers",
            "Buffer clonedBuffers"
        ],
        "full_context": "/*\n * Copyright (C) 2015-2021 Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport java.nio.Buffer;\nimport java.nio.ByteBuffer;\nimport java.nio.DoubleBuffer;\nimport java.nio.FloatBuffer;\nimport java.nio.IntBuffer;\nimport java.nio.LongBuffer;\nimport java.nio.ShortBuffer;\nimport java.util.EnumSet;\n\nimport org.bytedeco.javacpp.BytePointer;\nimport org.bytedeco.javacpp.DoublePointer;\nimport org.bytedeco.javacpp.FloatPointer;\nimport org.bytedeco.javacpp.IntPointer;\nimport org.bytedeco.javacpp.LongPointer;\nimport org.bytedeco.javacpp.Pointer;\nimport org.bytedeco.javacpp.ShortPointer;\nimport org.bytedeco.javacpp.indexer.ByteIndexer;\nimport org.bytedeco.javacpp.indexer.DoubleIndexer;\nimport org.bytedeco.javacpp.indexer.FloatIndexer;\nimport org.bytedeco.javacpp.indexer.Indexable;\nimport org.bytedeco.javacpp.indexer.Indexer;\nimport org.bytedeco.javacpp.indexer.IntIndexer;\nimport org.bytedeco.javacpp.indexer.LongIndexer;\nimport org.bytedeco.javacpp.indexer.ShortIndexer;\nimport org.bytedeco.javacpp.indexer.UByteIndexer;\nimport org.bytedeco.javacpp.indexer.UShortIndexer;\n\n/**\n * A class to manage the data of audio and video frames. It it used by\n * {@link CanvasFrame}, {@link FrameGrabber}, {@link FrameRecorder}, and their\n * subclasses. We can also make the link with other APIs, such as Android,\n * Java 2D, FFmpeg, and OpenCV, via a {@link FrameConverter}.\n *\n * @author Samuel Audet\n */\npublic class Frame implements AutoCloseable, Indexable {\n    /** A flag set by a FrameGrabber or a FrameRecorder to indicate a key frame. */\n    public boolean keyFrame;\n\n    /** The type of the image frame ('I', 'P', 'B', etc). */\n    public char pictType;\n\n    /** Constants to be used for {@link #imageDepth}. */\n    public static final int\n            DEPTH_BYTE   =  -8,\n            DEPTH_UBYTE  =   8,\n            DEPTH_SHORT  = -16,\n            DEPTH_USHORT =  16,\n            DEPTH_INT    = -32,\n            DEPTH_LONG   = -64,\n            DEPTH_FLOAT  =  32,\n            DEPTH_DOUBLE =  64;\n\n    /** Constants defining data type in the frame. */\n    public static enum Type {\n        VIDEO,\n        AUDIO,\n        DATA,\n        SUBTITLE,\n        ATTACHMENT\n    }\n\n    /** Information associated with the {@link #image} field. */\n    public int imageWidth, imageHeight, imageDepth, imageChannels, imageStride;\n\n    /**\n     * Buffers to hold image pixels from multiple channels for a video frame.\n     * Most of the software supports packed data only, but an array is provided\n     * to allow users to store images in a planar format as well.\n     */\n    public Buffer[] image;\n\n    /** Information associated with the {@link #samples} field. */\n    public int sampleRate, audioChannels;\n\n    /** Buffers to hold audio samples from multiple channels for an audio frame. */\n    public Buffer[] samples;\n\n    /** Buffer to hold a data stream associated with a frame. */\n    public ByteBuffer data;\n\n    /** Stream number the audio|video|other data is associated with. */\n    public int streamIndex;\n\n    /** The type of the stream. */\n    public Type type;\n\n    /** The underlying data object, for example, Pointer, AVFrame, IplImage, or Mat. */\n    public Object opaque;\n\n    /** Timestamp of the frame creation in microseconds. */\n    public long timestamp;\n\n    /** Returns {@code Math.abs(depth) / 8}. */\n    public static int pixelSize(int depth) {\n        return Math.abs(depth) / 8;\n    }\n\n    /** Empty constructor. */\n    public Frame() { }\n\n    /** Allocates a new packed image frame in native memory where rows are 8-byte aligned. */\n    public Frame(int width, int height, int depth, int channels) {\n        this(width, height, depth, channels, ((width * channels * pixelSize(depth) + 7) & ~7) / pixelSize(depth));\n    }\n    public Frame(int width, int height, int depth, int channels, int imageStride) {\n        this.imageWidth = width;\n        this.imageHeight = height;\n        this.imageDepth = depth;\n        this.imageChannels = channels;\n        this.imageStride = imageStride;\n        this.pictType = '\\0';\n        this.image = new Buffer[1];\n        this.data = null;\n        this.streamIndex = -1;\n        this.type = null;\n\n        Pointer pointer = new BytePointer(imageHeight * imageStride * pixelSize(depth));\n        ByteBuffer buffer = pointer.asByteBuffer();\n        switch (imageDepth) {\n            case DEPTH_BYTE:\n            case DEPTH_UBYTE:  image[0] = buffer;                  break;\n            case DEPTH_SHORT:\n            case DEPTH_USHORT: image[0] = buffer.asShortBuffer();  break;\n            case DEPTH_INT:    image[0] = buffer.asIntBuffer();    break;\n            case DEPTH_LONG:   image[0] = buffer.asLongBuffer();   break;\n            case DEPTH_FLOAT:  image[0] = buffer.asFloatBuffer();  break;\n            case DEPTH_DOUBLE: image[0] = buffer.asDoubleBuffer(); break;\n            default: throw new UnsupportedOperationException(\"Unsupported depth value: \" + imageDepth);\n        }\n        opaque = new Pointer[] {pointer.retainReference()};\n    }\n\n    /** Returns {@code createIndexer(true, 0)}. */\n    public <I extends Indexer> I createIndexer() {\n        return (I)createIndexer(true, 0);\n    }\n    @Override public <I extends Indexer> I createIndexer(boolean direct) {\n        return (I)createIndexer(direct, 0);\n    }\n    /** Returns an {@link Indexer} for the <i>i</i>th image plane. */\n    public <I extends Indexer> I createIndexer(boolean direct, int i) {\n        long[] sizes = {imageHeight, imageWidth, imageChannels};\n        long[] strides = {imageStride, imageChannels, 1};\n        Buffer buffer = image[i];\n        Object array = buffer.hasArray() ? buffer.array() : null;\n        switch (imageDepth) {\n            case DEPTH_UBYTE:\n                return array != null ? (I)UByteIndexer.create((byte[])array, sizes, strides).indexable(this)\n                            : direct ? (I)UByteIndexer.create((ByteBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)UByteIndexer.create(new BytePointer((ByteBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_BYTE:\n                return array != null ? (I)ByteIndexer.create((byte[])array, sizes, strides).indexable(this)\n                            : direct ? (I)ByteIndexer.create((ByteBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)ByteIndexer.create(new BytePointer((ByteBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_USHORT:\n                return array != null ? (I)UShortIndexer.create((short[])array, sizes, strides).indexable(this)\n                            : direct ? (I)UShortIndexer.create((ShortBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)UShortIndexer.create(new ShortPointer((ShortBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_SHORT:\n                return array != null ? (I)ShortIndexer.create((short[])array, sizes, strides).indexable(this)\n                            : direct ? (I)ShortIndexer.create((ShortBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)ShortIndexer.create(new ShortPointer((ShortBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_INT:\n                return array != null ? (I)IntIndexer.create((int[])array, sizes, strides).indexable(this)\n                            : direct ? (I)IntIndexer.create((IntBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)IntIndexer.create(new IntPointer((IntBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_LONG:\n                return array != null ? (I)LongIndexer.create((long[])array, sizes, strides).indexable(this)\n                            : direct ? (I)LongIndexer.create((LongBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)LongIndexer.create(new LongPointer((LongBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_FLOAT:\n                return array != null ? (I)FloatIndexer.create((float[])array, sizes, strides).indexable(this)\n                            : direct ? (I)FloatIndexer.create((FloatBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)FloatIndexer.create(new FloatPointer((FloatBuffer)buffer), sizes, strides, false).indexable(this);\n            case DEPTH_DOUBLE:\n                return array != null ? (I)DoubleIndexer.create((double[])array, sizes, strides).indexable(this)\n                            : direct ? (I)DoubleIndexer.create((DoubleBuffer)buffer, sizes, strides).indexable(this)\n                                     : (I)DoubleIndexer.create(new DoublePointer((DoubleBuffer)buffer), sizes, strides, false).indexable(this);\n            default: assert false;\n        }\n        return null;\n    }\n\n    /**Care must be taken if this method is to be used in conjunction with movie recordings.\n     *  Cloning a frame containing a full HD picture (alpha channel included) would take 1920 x 1080 * 4 = 8.294.400 Bytes.\n     *  Expect a heap overflow exception when using this method without cleaning up.\n     *\n     * @return A deep copy of this frame.\n     * @see {@link #cloneBufferArray}\n     *\n     * @author Extension proposed by Dragos Dutu\n     * */\n    @Override\n    public Frame clone() {\n        Frame newFrame = new Frame();\n\n        // Video part\n        newFrame.imageWidth = imageWidth;\n        newFrame.imageHeight = imageHeight;\n        newFrame.imageDepth = imageDepth;\n        newFrame.imageChannels = imageChannels;\n        newFrame.imageStride = imageStride;\n        newFrame.keyFrame = keyFrame;\n        newFrame.pictType = pictType;\n        newFrame.streamIndex = streamIndex;\n        newFrame.type = type;\n        newFrame.opaque = new Pointer[3];\n        if (image != null) {\n            newFrame.image = new Buffer[image.length];\n            ((Pointer[])newFrame.opaque)[0] = cloneBufferArray(image, newFrame.image);\n        }\n\n        // Audio part\n        newFrame.audioChannels = audioChannels;\n        newFrame.sampleRate = sampleRate;\n        if (samples != null) {\n            newFrame.samples = new Buffer[samples.length];\n            ((Pointer[])newFrame.opaque)[1] = cloneBufferArray(samples, newFrame.samples);\n        }\n\n        // Other data streams\n        if (data != null) {\n            ByteBuffer[] dst = new ByteBuffer[1];\n            ((Pointer[])newFrame.opaque)[2] = cloneBufferArray(new ByteBuffer[]{data}, dst);\n            newFrame.data = dst[0];\n        }\n\n        // Add timestamp\n        newFrame.timestamp = timestamp;\n\n        return newFrame;\n    }\n\n    /**\n     * This private method takes a buffer array as input and returns a deep copy.\n     * It is assumed that all buffers in the input array are of the same subclass.\n     *\n     * @param srcBuffers - Buffer array to be cloned\n     * @param clonedBuffers - Buffer array to fill with clones\n     * @return Opaque object to store\n     *\n     *  @author Extension proposed by Dragos Dutu\n     */\n    private static Pointer cloneBufferArray(Buffer[] srcBuffers, Buffer[] clonedBuffers) {\n        Pointer opaque = null;\n\n        if (srcBuffers != null && srcBuffers.length > 0) {\n            int totalCapacity = 0;\n            for (int i = 0; i < srcBuffers.length; i++) {\n                srcBuffers[i].rewind();\n                totalCapacity += srcBuffers[i].capacity();\n            }\n\n            /*\n             * In order to optimize the transfer we need a type check.\n             *\n             * Most CPUs support hardware memory transfer for different data\n             * types, so it's faster to copy more bytes at once rather\n             * than one byte per iteration as in case of ByteBuffer.\n             *\n             * For example, Intel CPUs support MOVSB (byte transfer), MOVSW\n             * (word transfer), MOVSD (double word transfer), MOVSS (32 bit\n             * scalar single precision floating point), MOVSQ (quad word\n             * transfer) and so on...\n             *\n             * Type checking may be improved by changing the order in\n             * which a buffer is checked against. If it's likely that the\n             * expected buffer is of type \"ShortBuffer\", then it should be\n             * checked at first place.\n             *\n             */\n\n            if (srcBuffers[0] instanceof ByteBuffer) {\n                BytePointer pointer = new BytePointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((ByteBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof ShortBuffer) {\n                ShortPointer pointer = new ShortPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((ShortBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof IntBuffer) {\n                IntPointer pointer = new IntPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((IntBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof LongBuffer) {\n                LongPointer pointer = new LongPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((LongBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof FloatBuffer) {\n                FloatPointer pointer = new FloatPointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((FloatBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            } else if (srcBuffers[0] instanceof DoubleBuffer) {\n                DoublePointer pointer = new DoublePointer(totalCapacity);\n                for (int i = 0; i < srcBuffers.length; i++) {\n                    clonedBuffers[i] = pointer.limit(pointer.position() + srcBuffers[i].limit())\n                            .asBuffer().put((DoubleBuffer)srcBuffers[i]);\n                    pointer.position(pointer.limit());\n                }\n                opaque = pointer;\n            }\n\n            for (int i = 0; i < srcBuffers.length; i++) {\n                srcBuffers[i].rewind();\n                clonedBuffers[i].rewind();\n            }\n        }\n\n        if (opaque != null) {\n            opaque.retainReference();\n        }\n        return opaque;\n    }\n\n    /** Returns types of data containing in the frame */\n    public EnumSet<Type> getTypes() {\n        EnumSet<Type> type = EnumSet.noneOf(Type.class);\n        if (image != null) type.add(Type.VIDEO);\n        if (samples != null) type.add(Type.AUDIO);\n        if (data != null) type.add(Type.DATA);\n        return type;\n    }\n\n    @Override public void close() {\n        if (opaque instanceof Pointer[]) {\n            for (Pointer p : (Pointer[])opaque) {\n                if (p != null) {\n                    p.releaseReference();\n                    p = null;\n                }\n            }\n            opaque = null;\n        }\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport java.nio.Buffer;\n\nimport java.nio.ByteBuffer;\n\nimport java.nio.DoubleBuffer;\n\nimport java.nio.FloatBuffer;\n\nimport java.nio.IntBuffer;\n\nimport java.nio.LongBuffer;\n\nimport java.nio.ShortBuffer;\n\nimport java.util.EnumSet;\n\nimport org.bytedeco.javacpp.BytePointer;\n\nimport org.bytedeco.javacpp.DoublePointer;\n\nimport org.bytedeco.javacpp.FloatPointer;\n\nimport org.bytedeco.javacpp.IntPointer;\n\nimport org.bytedeco.javacpp.LongPointer;\n\nimport org.bytedeco.javacpp.Pointer;\n\nimport org.bytedeco.javacpp.ShortPointer;\n\nimport org.bytedeco.javacpp.indexer.ByteIndexer;\n\nimport org.bytedeco.javacpp.indexer.DoubleIndexer;\n\nimport org.bytedeco.javacpp.indexer.FloatIndexer;\n\nimport org.bytedeco.javacpp.indexer.Indexable;\n\nimport org.bytedeco.javacpp.indexer.Indexer;\n\nimport org.bytedeco.javacpp.indexer.IntIndexer;\n\nimport org.bytedeco.javacpp.indexer.LongIndexer;\n\nimport org.bytedeco.javacpp.indexer.ShortIndexer;\n\nimport org.bytedeco.javacpp.indexer.UByteIndexer;\n\nimport org.bytedeco.javacpp.indexer.UShortIndexer;\n\npublic class Frame implements AutoCloseable, Indexable {\n    public boolean keyFrame;\n    public char pictType;\n    static public final int DEPTH_BYTE, DEPTH_UBYTE, DEPTH_SHORT, DEPTH_USHORT, DEPTH_INT, DEPTH_LONG, DEPTH_FLOAT, DEPTH_DOUBLE;\n    static public enum Type{VIDEO, AUDIO, DATA, SUBTITLE, ATTACHMENT}\n    public int imageWidth, imageHeight, imageDepth, imageChannels, imageStride;\n    public Buffer image;\n    public int sampleRate, audioChannels;\n    public Buffer samples;\n    public ByteBuffer data;\n    public int streamIndex;\n    public Type type;\n    public Object opaque;\n    public long timestamp;\n    static public int pixelSize(int depth);\n    public Frame();\n    public Frame(int width, int height, int depth, int channels);\n    public Frame(int width, int height, int depth, int channels, int imageStride);\n    public I createIndexer();\n    public I createIndexer(boolean direct);\n    public I createIndexer(boolean direct, int i);\n    public Frame clone();\n    static private Pointer cloneBufferArray(Buffer srcBuffers, Buffer clonedBuffers);\n    public EnumSet<Type> getTypes();\n    public  close();\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "IPCameraFrameGrabber.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/IPCameraFrameGrabber.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**\n     * Returns the value of the {@code long} argument;\n     * throwing an exception if the value overflows an {@code int}.\n     *\n     * @param value the long value\n     * @return the argument as an int\n     * @throws ArithmeticException if the {@code argument} overflows an int\n     * @see <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html#toIntExact-long-\">Java 8 Implementation</a>\n     */",
        "source_code": "\nprivate static int toIntExact(long value) {\n    if ((int) value != value) {\n        throw new ArithmeticException(\"integer overflow\");\n    }\n    return (int) value;\n}\n",
        "class_name": "IPCameraFrameGrabber",
        "method_name": "toIntExact",
        "argument_name": [
            "long value"
        ],
        "full_context": "/*\n * Copyright (C) 2013 Greg Perry\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport org.bytedeco.javacpp.BytePointer;\nimport org.bytedeco.javacpp.Loader;\n\nimport javax.imageio.ImageIO;\nimport java.awt.image.BufferedImage;\nimport java.io.ByteArrayInputStream;\nimport java.io.DataInputStream;\nimport java.io.EOFException;\nimport java.io.IOException;\nimport java.net.MalformedURLException;\nimport java.net.URL;\nimport java.net.URLConnection;\nimport java.util.concurrent.TimeUnit;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_imgcodecs.*;\n\npublic class IPCameraFrameGrabber extends FrameGrabber {\n\n    /*\n     * excellent reference - http://www.jpegcameras.com/ foscam url\n     * http://host/videostream.cgi?user=username&pwd=password\n     * http://192.168.0.59:60/videostream.cgi?user=admin&pwd=password android ip\n     * cam http://192.168.0.57:8080/videofeed\n     */\n\n    private static Exception loadingException = null;\n\n    public static void tryLoad() throws Exception {\n        if (loadingException != null) {\n            throw loadingException;\n        } else {\n            try {\n                Loader.load(org.bytedeco.opencv.global.opencv_highgui.class);\n            } catch (Throwable t) {\n                throw loadingException = new Exception(\"Failed to load \" + IPCameraFrameGrabber.class, t);\n            }\n        }\n    }\n\n    private final FrameConverter converter = new OpenCVFrameConverter.ToMat();\n    private final URL url;\n    private final int connectionTimeout;\n    private final int readTimeout;\n    private DataInputStream input;\n    private byte[] pixelBuffer = new byte[1024];\n    private Mat decoded = null;\n\n    /**\n     * @param url          The URL to create the camera connection with.\n     * @param startTimeout How long should this wait on the connection while trying to {@link #start()} before\n     *                     timing out.\n     *                     If this value is less than zero it will be ignored.\n     *                     {@link URLConnection#setConnectTimeout(int)}\n     * @param grabTimeout  How long should grab wait while reading the connection before timing out.\n     *                     If this value is less than zero it will be ignored.\n     *                     {@link URLConnection#setReadTimeout(int)}\n     * @param timeUnit     The time unit to use for the connection and read timeout.\n     *                     If this value is null then the start timeout and grab timeout will be ignored.\n     */\n    public IPCameraFrameGrabber(URL url, int startTimeout, int grabTimeout, TimeUnit timeUnit) {\n        super(); // Always good practice to do this\n        if (url == null) {\n            throw new IllegalArgumentException(\"URL can not be null\");\n        }\n        this.url = url;\n        if (timeUnit != null) {\n            this.connectionTimeout = toIntExact(TimeUnit.MILLISECONDS.convert(startTimeout, timeUnit));\n            this.readTimeout = toIntExact(TimeUnit.MILLISECONDS.convert(grabTimeout, timeUnit));\n        } else {\n            this.connectionTimeout = -1;\n            this.readTimeout = -1;\n        }\n    }\n\n    public IPCameraFrameGrabber(String urlstr, int connectionTimeout, int readTimeout, TimeUnit timeUnit) throws MalformedURLException {\n        this(new URL(urlstr), connectionTimeout, readTimeout, timeUnit);\n    }\n\n    /**\n     * @param urlstr A string to be used to create the URL.\n     * @throws MalformedURLException if the urlstr is a malformed URL\n     * @deprecated By not setting the connection timeout and the read timeout if your network ever crashes\n     * then {@link #start()} or {@link #grab()} can hang for upwards of 45 to 60 seconds before failing.\n     * You should always explicitly set the connectionTimeout and readTimeout so that your application can\n     * respond appropriately to a loss or failure to connect.\n     */\n    @Deprecated\n    public IPCameraFrameGrabber(String urlstr) throws MalformedURLException {\n        this(new URL(urlstr), -1, -1, null);\n    }\n\n    @Override\n    public void start() throws Exception {\n        try {\n            /*\n             * We don't need to keep a reference to the connection\n             * after it is opened in the parent class.\n             * It never uses it outside of start.\n             */\n            final URLConnection connection = url.openConnection();\n            // If the class was initialized with timeout values then configure those\n            if (connectionTimeout >= 0) {\n                connection.setConnectTimeout(connectionTimeout);\n            }\n            if (readTimeout >= 0) {\n                connection.setReadTimeout(readTimeout);\n            }\n            input = new DataInputStream(connection.getInputStream());\n        } catch (IOException e) {\n            throw new Exception(e.getMessage(), e);\n        }\n    }\n\n    @Override\n    public void stop() throws Exception {\n        if (input != null) {\n            try {\n                input.close();\n            } catch (IOException e) {\n                throw new Exception(e.getMessage(), e);\n            } finally {\n                // Close may have failed but there's really nothing we can do about it at this point\n                input = null;\n                // Don't set the url to null, it may be needed to restart this object\n                releaseDecoded();\n            }\n        }\n    }\n\n    @Override\n    public void trigger() throws Exception {\n    }\n\n    @Override\n    public Frame grab() throws Exception {\n        try {\n            final byte[] b = readImage();\n            final Mat mat = new Mat(1, b.length, CV_8UC1, new BytePointer(b));\n            releaseDecoded();\n            return converter.convert(decoded = imdecode(mat, IMREAD_COLOR));\n        } catch (IOException e) {\n            throw new Exception(e.getMessage(), e);\n        }\n    }\n\n    public BufferedImage grabBufferedImage() throws IOException {\n        BufferedImage bi = ImageIO.read(new ByteArrayInputStream(readImage()));\n        return bi;\n    }\n\n    /**\n     * Ensures that if the decoded image is not null that it gets released and set to null.\n     * If the image was not set to null then trying to release a null pointer will cause  a\n     * segfault.\n     */\n    private void releaseDecoded() {\n        if (decoded != null) {\n            decoded.release();\n            decoded = null;\n        }\n    }\n\n    private byte[] readImage() throws IOException {\n        final StringBuffer sb = new StringBuffer();\n        int c;\n        // read http subheader\n        while ((c = input.read()) != -1) {\n            if (c > 0) {\n                sb.append((char) c);\n                if (c == 13) {\n                    sb.append((char) input.read());// '10'+\n                    c = input.read();\n                    sb.append((char) c);\n                    if (c == 13) {\n                        sb.append((char) input.read());// '10'\n                        break; // done with subheader\n                    }\n\n                }\n            }\n        }\n        // find embedded jpeg in stream\n        /*\n         * Some cameras return headers 'content-length' using different casing\n         * Eg. Axis cameras return 'Content-Length:' while TrendNet cameras return 'content-length:'\n         */\n        final String subheader = sb.toString().toLowerCase();\n        //log.debug(subheader);\n\n        // Yay! - server was nice and sent content length\n        int c0 = subheader.indexOf(\"content-length: \");\n        final int c1 = subheader.indexOf('\\r', c0);\n\n        if (c0 < 0) {\n            //log.info(\"no content length returning null\");\n            throw new EOFException(\"The camera stream ended unexpectedly\");\n        }\n\n        c0 += 16;\n        final int contentLength = Integer.parseInt(subheader.substring(c0, c1).trim());\n        //log.debug(\"Content-Length: \" + contentLength);\n\n        // adaptive size - careful - don't want a 2G jpeg\n        ensureBufferCapacity(contentLength);\n\n        input.readFully(pixelBuffer, 0, contentLength);\n        input.read();// \\r\n        input.read();// \\n\n        input.read();// \\r\n        input.read();// \\n\n\n        return pixelBuffer;\n    }\n\n    @Override\n    public void release() throws Exception {\n    }\n\n    /**\n     * Grow the pixel buffer if necessary.  Using this method instead of allocating a new buffer every time a frame\n     * is grabbed improves performance by reducing the frequency of garbage collections.  In a simple test, the\n     * original version of IPCameraFrameGrabber that allocated a 4096 element byte array for every read\n     * caused about 200MB of allocations within 13 seconds.  In this version, almost no additional heap space\n     * is typically allocated per frame.\n     */\n    private void ensureBufferCapacity(int desiredCapacity) {\n        int capacity = pixelBuffer.length;\n\n        while (capacity < desiredCapacity) {\n            capacity *= 2;\n        }\n\n        if (capacity > pixelBuffer.length) {\n            pixelBuffer = new byte[capacity];\n        }\n    }\n\n    /**\n     * Returns the value of the {@code long} argument;\n     * throwing an exception if the value overflows an {@code int}.\n     *\n     * @param value the long value\n     * @return the argument as an int\n     * @throws ArithmeticException if the {@code argument} overflows an int\n     * @see <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html#toIntExact-long-\">Java 8 Implementation</a>\n     */\n    private static int toIntExact(long value) {\n        if ((int) value != value) {\n            throw new ArithmeticException(\"integer overflow\");\n        }\n        return (int) value;\n    }\n\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport org.bytedeco.javacpp.BytePointer;\n\nimport org.bytedeco.javacpp.Loader;\n\nimport javax.imageio.ImageIO;\n\nimport java.awt.image.BufferedImage;\n\nimport java.io.ByteArrayInputStream;\n\nimport java.io.DataInputStream;\n\nimport java.io.EOFException;\n\nimport java.io.IOException;\n\nimport java.net.MalformedURLException;\n\nimport java.net.URL;\n\nimport java.net.URLConnection;\n\nimport java.util.concurrent.TimeUnit;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_imgcodecs;\n\npublic class IPCameraFrameGrabber extends FrameGrabber {\n    static private Exception loadingException;\n    static public  tryLoad()throws Exception;\n    final private FrameConverter converter;\n    final private URL url;\n    final private int connectionTimeout;\n    final private int readTimeout;\n    private DataInputStream input;\n    private byte[] pixelBuffer;\n    private Mat decoded;\n    public IPCameraFrameGrabber(URL url, int startTimeout, int grabTimeout, TimeUnit timeUnit);\n    public IPCameraFrameGrabber(String urlstr, int connectionTimeout, int readTimeout, TimeUnit timeUnit);\n    public IPCameraFrameGrabber(String urlstr);\n    public  start()throws Exception;\n    public  stop()throws Exception;\n    public  trigger()throws Exception;\n    public Frame grab()throws Exception;\n    public BufferedImage grabBufferedImage()throws IOException;\n    private  releaseDecoded();\n    private byte[] readImage()throws IOException;\n    public  release()throws Exception;\n    private  ensureBufferCapacity(int desiredCapacity);\n    static private int toIntExact(long value);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "JavaCV.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/JavaCV.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**\n     * Ported to Java/OpenCV from\n     * Bill Triggs. Autocalibration from Planar Scenes. In 5th European Conference\n     * on Computer Vision (ECCV \u201998), volume I, pages 89\u2013105. Springer-Verlag, 1998.\n     */",
        "source_code": "\npublic static double homogToRt(CvMat H,\n        CvMat R1, CvMat t1, CvMat n1,\n        CvMat R2, CvMat t2, CvMat n2) {\n    CvMat S = S3x3.get(), U = U3x3.get(), V = V3x3.get();\n    cvSVD(H, S, U, V, 0);\n    double zeta = homogToRt(S, U, V, R1, t1, n1, R2, t2, n2);\n    return zeta;\n}\n",
        "class_name": "JavaCV",
        "method_name": "homogToRt",
        "argument_name": [
            "CvMat H",
            "CvMat R1",
            "CvMat t1",
            "CvMat n1",
            "CvMat R2",
            "CvMat t2",
            "CvMat n2"
        ],
        "full_context": "/*\n * Copyright (C) 2009-2016 Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport java.nio.ByteBuffer;\nimport java.nio.DoubleBuffer;\nimport java.nio.FloatBuffer;\nimport java.nio.IntBuffer;\nimport java.nio.ShortBuffer;\nimport java.util.Arrays;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport org.bytedeco.opencv.opencv_imgproc.*;\nimport org.bytedeco.opencv.global.opencv_core;\nimport org.bytedeco.opencv.global.opencv_imgproc;\nimport static org.bytedeco.opencv.global.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\n\n/**\n *\n * @author Samuel Audet\n */\npublic class JavaCV {\n\n    public static final double\n            SQRT2 = 1.41421356237309504880,\n            FLT_EPSILON = 1.19209290e-7F,\n            DBL_EPSILON = 2.2204460492503131e-16;\n\n    /** returns the distance^2 between the line (x1, y1) (x2, y2) and the point (x3, y3) */\n    public static double distanceToLine(double x1, double y1, double x2, double y2, double x3, double y3) {\n        double dx = x2 - x1;\n        double dy = y2 - y1;\n        double d2 = dx*dx + dy*dy;\n        double u = ((x3 - x1)*dx + (y3 - y1)*dy) / d2;\n\n        double x = x1 + u*dx;\n        double y = y1 + u*dy;\n\n        dx = x - x3;\n        dy = y - y3;\n        return dx*dx + dy*dy;\n    }\n\n    /**\n     * returns the largest rectangle of given aspect ratio and angle,\n     * bounded by the contour and sharing the same centroid\n     */\n    private static ThreadLocal<CvMoments> moments = CvMoments.createThreadLocal();\n    public static CvBox2D boundedRect(CvMat contour, CvBox2D box) {\n        int contourLength = contour.length();\n        CvMoments m = moments.get();\n        cvMoments(contour, m, 0);\n        double inv_m00 = 1 / m.m00();\n        double centerX = m.m10() * inv_m00;\n        double centerY = m.m01() * inv_m00;\n\n        float[] pts = new float[8];\n        CvPoint2D32f center = box.center();\n        CvSize2D32f size = box.size();\n        center.put(centerX, centerY);\n        cvBoxPoints(box, pts);\n\n        float scale = Float.POSITIVE_INFINITY;\n        for (int i = 0; i < 4; i++) {\n            double x1 = centerX,  y1 = centerY,\n                   x2 = pts[2*i], y2 = pts[2*i + 1];\n            for (int j = 0; j < contourLength; j++) {\n                int k = (j + 1) % contourLength;\n                double x3 = contour.get(2*j), y3 = contour.get(2*j + 1),\n                       x4 = contour.get(2*k), y4 = contour.get(2*k + 1);\n                double d = (y4 - y3) * (x2 - x1) - (x4 - x3) * (y2 - y1);\n                double ua = ((x4 - x3) * (y1 - y3) - (y4 - y3) * (x1 - x3))/d,\n                       ub = ((x2 - x1) * (y1 - y3) - (y2 - y1) * (x1 - x3))/d;\n                if (ub >= 0 && ub <= 1 && ua >= 0 && ua < scale) {\n                    scale = (float)ua;\n                }\n            }\n        }\n        size.width(scale*size.width()).height(scale*size.height());\n        return box;\n    }\n\n    /**\n     * Similar to cvBoundingRect(), but can also pad the output with some extra\n     * pixels, useful to use as ROI for operations with interpolation. Further\n     * aligns the region to specified boundaries, for easier vectorization and\n     * subsampling, and also uses on input the rect argument as a maximum boundary.\n     */\n    public static CvRect boundingRect(double[] contour, CvRect rect,\n            int padX, int padY, int alignX, int alignY) {\n        double minX = contour[0];\n        double minY = contour[1];\n        double maxX = contour[0];\n        double maxY = contour[1];\n        for (int i = 1; i < contour.length/2; i++) {\n            double x = contour[2*i  ];\n            double y = contour[2*i+1];\n            minX = Math.min(minX, x);\n            minY = Math.min(minY, y);\n            maxX = Math.max(maxX, x);\n            maxY = Math.max(maxY, y);\n        }\n        int x = (int)Math.floor(Math.max(rect.x(), minX-padX)/alignX)*alignX;\n        int y = (int)Math.floor(Math.max(rect.y(), minY-padY)/alignY)*alignY;\n        int width  = (int)Math.ceil(Math.min(rect.width(),  maxX+padX)/alignX)*alignX - x;\n        int height = (int)Math.ceil(Math.min(rect.height(), maxY+padY)/alignY)*alignY - y;\n\n        return rect.x(x).y(y).width(Math.max(0, width)).height(Math.max(0, height));\n    }\n\n    private static ThreadLocal<CvMat>\n            A8x8 = CvMat.createThreadLocal(8, 8),\n            b8x1 = CvMat.createThreadLocal(8, 1),\n            x8x1 = CvMat.createThreadLocal(8, 1);\n    /**\n     * this is basically cvGetPerspectiveTransform() using CV_LU instead of\n     * CV_SVD, because the latter gives inaccurate results...\n     * Consider using {@link opencv_imgproc#getPerspectiveTransform} instead.\n     */\n    public static CvMat getPerspectiveTransform(double[] src, double[] dst, CvMat map_matrix) {\n        // creating and releasing matrices via NIO here in this function\n        // can easily become a bottleneck, so we use ThreadLocal references\n        CvMat A = A8x8.get();\n        CvMat b = b8x1.get();\n        CvMat x = x8x1.get();\n\n        for(int i = 0; i < 4; ++i ) {\n            A.put(i*8+0, src[i*2]);   A.put((i+4)*8+3, src[i*2]);\n            A.put(i*8+1, src[i*2+1]); A.put((i+4)*8+4, src[i*2+1]);\n            A.put(i*8+2, 1);          A.put((i+4)*8+5, 1);\n            A.put(i*8+3, 0);          A.put(i*8+4, 0); A.put(i*8+5, 0);\n            A.put((i+4)*8+0, 0);  A.put((i+4)*8+1, 0); A.put((i+4)*8+2, 0);\n\n            A.put(i*8+6,     -src[i*2]  *dst[i*2]);\n            A.put(i*8+7,     -src[i*2+1]*dst[i*2]);\n            A.put((i+4)*8+6, -src[i*2]  *dst[i*2+1]);\n            A.put((i+4)*8+7, -src[i*2+1]*dst[i*2+1]);\n\n            b.put(i,   dst[i*2]);\n            b.put(i+4, dst[i*2+1]);\n        }\n        cvSolve(A, b, x, CV_LU);\n        map_matrix.put(x.get());\n        map_matrix.put(8, 1);\n\n        return map_matrix;\n    }\n\n    /** Consider using {@link opencv_core#perspectiveTransform} instead. */\n    public static void perspectiveTransform(double[] src, double[] dst, CvMat map_matrix) {\n        double[] mat = map_matrix.get();\n        for (int j = 0; j < src.length; j += 2) {\n            double x = src[j], y = src[j + 1];\n            double w = x*mat[6] + y*mat[7] + mat[8];\n\n            if (Math.abs(w) > FLT_EPSILON) {\n                w = 1.0/w;\n                dst[j] = (x*mat[0] + y*mat[1] + mat[2])*w;\n                dst[j+1] = (x*mat[3] + y*mat[4] + mat[5])*w;\n            } else {\n                dst[j] = dst[j+1] = 0;\n            }\n        }\n    }\n\n    private static ThreadLocal<CvMat>\n            A3x3 = CvMat.createThreadLocal(3, 3), b3x1 = CvMat.createThreadLocal(3, 1);\n    public static CvMat getPlaneParameters(double[] src, double[] dst,\n            CvMat invSrcK, CvMat dstK, CvMat R, CvMat t, CvMat n) {\n        CvMat A = A3x3.get(), b = b3x1.get();\n\n        double[] x = new double[6], y = new double[6];\n        perspectiveTransform(src, x, invSrcK);\n        cvInvert(dstK, A);\n        perspectiveTransform(dst, y, A);\n\n        for (int i = 0; i < 3; i++) {\n            A.put(i, 0, (t.get(2)*y[i*2] - t.get(0))*x[i*2  ]);\n            A.put(i, 1, (t.get(2)*y[i*2] - t.get(0))*x[i*2+1]);\n            A.put(i, 2,  t.get(2)*y[i*2] - t.get(0));\n\n            b.put(i, (R.get(2, 0)*x[i*2] + R.get(2, 1)*x[i*2+1] + R.get(2, 2))*y[i*2] -\n                     (R.get(0, 0)*x[i*2] + R.get(0, 1)*x[i*2+1] + R.get(0, 2)));\n        }\n        cvSolve(A, b, n, CV_LU);\n\n        return n;\n    }\n\n    private static ThreadLocal<CvMat>\n            n3x1 = CvMat.createThreadLocal(3, 1);\n    public static CvMat getPerspectiveTransform(double[] src, double[] dst, \n            CvMat invSrcK, CvMat dstK, CvMat R, CvMat t, CvMat H) {\n        CvMat n = n3x1.get();\n        getPlaneParameters(src, dst, invSrcK, dstK, R, t, n);\n\n        // H = R - t*n^T\n        cvGEMM(t, n, -1,  R, 1,  H, CV_GEMM_B_T);\n        // H = dstK * H * srcK^-1\n        cvMatMul(dstK, H, H);\n        cvMatMul(H, invSrcK, H);\n\n        return H;\n    }\n\n    private static ThreadLocal<CvMat>\n            H3x3 = CvMat.createThreadLocal(3, 3);\n    public static void perspectiveTransform(double[] src, double[] dst,\n            CvMat invSrcK, CvMat dstK, CvMat R, CvMat t, CvMat n, boolean invert) {\n        CvMat H = H3x3.get();\n\n        // H = R - t*n^T\n        cvGEMM(t, n, -1,  R, 1,  H, CV_GEMM_B_T);\n\n        // H = dstK * H * srcK^-1\n        cvMatMul(dstK, H, H);\n        cvMatMul(H, invSrcK, H);\n        if (invert) {\n            cvInvert(H, H);\n        }\n        perspectiveTransform(src, dst, H);\n    }\n\n    private static ThreadLocal<CvMat>\n            M3x2 = CvMat.createThreadLocal(3, 2), S2x2 = CvMat.createThreadLocal(2, 2),\n            U3x2 = CvMat.createThreadLocal(3, 2), V2x2 = CvMat.createThreadLocal(2, 2);\n    /**\n     * Algorithms for Plane-Based Pose Estimation, Peter Sturm\n     * This assumes plane parameters n == z axis.\n     */\n    public static void HtoRt(CvMat H, CvMat R, CvMat t) {\n        CvMat M = M3x2.get(), S = S2x2.get(),\n              U = U3x2.get(), V = V2x2.get();\n        M.put(H.get(0), H.get(1),\n              H.get(3), H.get(4),\n              H.get(6), H.get(7));\n        cvSVD(M, S, U, V, CV_SVD_V_T);\n\n        double lambda = S.get(3);\n        t.put(H.get(2)/lambda, H.get(5)/lambda, H.get(8)/lambda);\n\n        cvMatMul(U, V, M);\n        R.put(M.get(0), M.get(1), M.get(2)*M.get(5) - M.get(3)*M.get(4),\n              M.get(2), M.get(3), M.get(1)*M.get(4) - M.get(0)*M.get(5),\n              M.get(4), M.get(5), M.get(0)*M.get(3) - M.get(1)*M.get(2));\n    }\n\n    private static ThreadLocal<CvMat>\n            R13x3 = CvMat.createThreadLocal(3, 3), R23x3 = CvMat.createThreadLocal(3, 3),\n            t13x1 = CvMat.createThreadLocal(3, 1), t23x1 = CvMat.createThreadLocal(3, 1),\n            n13x1 = CvMat.createThreadLocal(3, 1), n23x1 = CvMat.createThreadLocal(3, 1),\n            H13x3 = CvMat.createThreadLocal(3, 3), H23x3 = CvMat.createThreadLocal(3, 3);\n    public static double HnToRt(CvMat H, CvMat n, CvMat R, CvMat t) {\n        CvMat S = S3x3.get(), U = U3x3.get(), V = V3x3.get();\n        cvSVD(H, S, U, V, 0);\n\n        CvMat R1 = R13x3.get(),  R2 = R23x3.get(),\n              t1 = t13x1.get(),  t2 = t23x1.get(),\n              n1 = n13x1.get(),  n2 = n23x1.get(),\n              H1 = H13x3.get(),  H2 = H23x3.get();\n        double zeta = homogToRt(S, U, V, R1, t1, n1, R2, t2, n2);\n\n        // H = (R^-1 * H)/s2\n        cvGEMM(R1, H, 1/S.get(4),  null, 0,  H1, CV_GEMM_A_T);\n        cvGEMM(R2, H, 1/S.get(4),  null, 0,  H2, CV_GEMM_A_T);\n\n        // H = H - I\n        H1.put(0, H1.get(0)-1); H1.put(4, H1.get(4)-1); H1.put(8, H1.get(8)-1);\n        H2.put(0, H2.get(0)-1); H2.put(4, H2.get(4)-1); H2.put(8, H2.get(8)-1);\n\n        // Now H should ~= -tn^T, so extract \"average\" t\n        double d   =    Math.abs   (n.get(0)) + Math.abs   (n.get(1)) + Math.abs   (n.get(2));\n        double s[] = { -Math.signum(n.get(0)), -Math.signum(n.get(1)), -Math.signum(n.get(2)) };\n        t1.put(0.0, 0.0, 0.0);\n        t2.put(0.0, 0.0, 0.0);\n        for (int i = 0; i < 3; i++) {\n            t1.put(0, t1.get(0) + s[i]*H1.get(i)  /d);\n            t1.put(1, t1.get(1) + s[i]*H1.get(i+3)/d);\n            t1.put(2, t1.get(2) + s[i]*H1.get(i+6)/d);\n\n            t2.put(0, t2.get(0) + s[i]*H2.get(i)  /d);\n            t2.put(1, t2.get(1) + s[i]*H2.get(i+3)/d);\n            t2.put(2, t2.get(2) + s[i]*H2.get(i+6)/d);\n        }\n\n        // H = H + tn^T\n        cvGEMM(t1, n, 1,  H1, 1,  H1, CV_GEMM_B_T);\n        cvGEMM(t2, n, 1,  H2, 1,  H2, CV_GEMM_B_T);\n\n        // take what's left as the error of the model,\n        // this either indicates inaccurate camera matrix K or normal vector n\n        double err1 = cvNorm(H1);\n        double err2 = cvNorm(H2);\n\n        double err;\n        if (err1 < err2) {\n            if (R != null) {\n                R.put(R1);\n            }\n            if (t != null) {\n                t.put(t1);\n            }\n            err = err1;\n        } else {\n            if (R != null) {\n                R.put(R2);\n            }\n            if (t != null) {\n                t.put(t2);\n            }\n            err = err2;\n        }\n\n        return err;\n    }\n\n    private static ThreadLocal<CvMat>\n            S3x3 = CvMat.createThreadLocal(3, 3),\n            U3x3 = CvMat.createThreadLocal(3, 3),\n            V3x3 = CvMat.createThreadLocal(3, 3);\n    /**\n     * Ported to Java/OpenCV from\n     * Bill Triggs. Autocalibration from Planar Scenes. In 5th European Conference\n     * on Computer Vision (ECCV \u201998), volume I, pages 89\u2013105. Springer-Verlag, 1998.\n     */\n    public static double homogToRt(CvMat H,\n            CvMat R1, CvMat t1, CvMat n1,\n            CvMat R2, CvMat t2, CvMat n2) {\n        CvMat S = S3x3.get(), U = U3x3.get(), V = V3x3.get();\n        cvSVD(H, S, U, V, 0);\n        double zeta = homogToRt(S, U, V, R1, t1, n1, R2, t2, n2);\n        return zeta;\n    }\n    public static double homogToRt(CvMat S, CvMat U, CvMat V,\n            CvMat R1, CvMat t1, CvMat n1,\n            CvMat R2, CvMat t2, CvMat n2) {\n        double s1 = S.get(0)/S.get(4);\n        double s3 = S.get(8)/S.get(4);\n        double zeta = s1-s3;\n        double a1 = Math.sqrt(1 - s3*s3);\n        double b1 = Math.sqrt(s1*s1 - 1);\n        double[] ab = unitize(a1, b1);\n        double[] cd = unitize(1+s1*s3, a1*b1);\n        double[] ef = unitize(-ab[1]/s1, -ab[0]/s3);\n\n        R1.put(cd[0],0,cd[1], 0,1,0, -cd[1],0,cd[0]);\n        cvGEMM(U , R1, 1,  null, 0,  R1, 0);\n        cvGEMM(R1, V,  1,  null, 0,  R1, CV_GEMM_B_T);\n\n        R2.put(cd[0],0,-cd[1], 0,1,0, cd[1],0,cd[0]);\n        cvGEMM(U , R2, 1,  null, 0,  R2, 0);\n        cvGEMM(R2, V,  1,  null, 0,  R2, CV_GEMM_B_T);\n\n        double[] v1 = { V.get(0), V.get(3), V.get(6) };\n        double[] v3 = { V.get(2), V.get(5), V.get(8) };\n        double sign1 = 1, sign2 = 1;\n        for (int i = 2; i >= 0; i--) {\n            n1.put(i, sign1*(ab[1]*v1[i] - ab[0]*v3[i]));\n            n2.put(i, sign2*(ab[1]*v1[i] + ab[0]*v3[i]));\n            t1.put(i, sign1*(ef[0]*v1[i] + ef[1]*v3[i]));\n            t2.put(i, sign2*(ef[0]*v1[i] - ef[1]*v3[i]));\n            if (i == 2) {\n                if (n1.get(2) < 0) {\n                    n1.put(2, -n1.get(2));\n                    t1.put(2, -t1.get(2));\n                    sign1 = -1;\n                }\n                if (n2.get(2) < 0) {\n                    n2.put(2, -n2.get(2));\n                    t2.put(2, -t2.get(2));\n                    sign2 = -1;\n                }\n            }\n        }\n\n        return zeta;\n    }\n\n    public static double[] unitize(double a, double b) {\n        double norm = Math.sqrt(a*a + b*b);\n        if (norm > FLT_EPSILON) {\n            a = a / norm;\n            b = b / norm;\n        }\n        return new double[] { a, b };\n    }\n\n    /** more sophisticated than cvAdaptiveThreshold() */\n    public static void adaptiveThreshold(IplImage srcImage, final IplImage sumImage,\n            final IplImage sqSumImage, final IplImage dstImage, final boolean invert,\n            final int windowMax, final int windowMin, final double varMultiplier, final double k) {\n        final int w = srcImage.width();\n        final int h = srcImage.height();\n        final int srcChannels = srcImage.nChannels();\n        final int srcDepth = srcImage.depth();\n        final int dstDepth = dstImage.depth();\n\n        if (srcChannels > 1 && dstDepth == IPL_DEPTH_8U) {\n            cvCvtColor(srcImage, dstImage, srcChannels == 4 ? CV_RGBA2GRAY : CV_BGR2GRAY);\n            srcImage = dstImage;\n        }\n\n        final ByteBuffer srcBuf = srcImage.getByteBuffer();\n        final ByteBuffer dstBuf = dstImage.getByteBuffer();\n        final DoubleBuffer sumBuf = sumImage.getDoubleBuffer();\n        final DoubleBuffer sqSumBuf = sqSumImage.getDoubleBuffer();\n        final int srcStep = srcImage.widthStep();\n        final int dstStep = dstImage.widthStep();\n        final int sumStep = sumImage.widthStep();\n        final int sqSumStep = sqSumImage.widthStep();\n\n        // compute integral images\n        cvIntegral(srcImage, sumImage, sqSumImage, null);\n\n        // try to detect a reasonable maximum and minimum intensity\n        // for thresholds instead of simply 0 and 255...\n        double totalMean = sumBuf.get((h-1)*sumStep/8 + (w-1)) -\n                           sumBuf.get((h-1)*sumStep/8) -\n                           sumBuf.get(w-1) + sumBuf.get(0);\n        totalMean /= w*h;\n        double totalSqMean = sqSumBuf.get((h-1)*sqSumStep/8 + (w-1)) -\n                             sqSumBuf.get((h-1)*sqSumStep/8) -\n                             sqSumBuf.get(w-1) + sqSumBuf.get(0);\n        totalSqMean /= w*h;\n        double totalVar = totalSqMean - totalMean*totalMean;\n//double totalDev = Math.sqrt(totalVar);\n//System.out.println(totalDev);\n        final double targetVar = totalVar*varMultiplier;\n\n        //for (int y = 0; y < h; y++) {\n        Parallel.loop(0, h, new Parallel.Looper() {\n        public void loop(int from, int to, int looperID) {\n            for (int y = from; y < to; y++) {\n                for (int x = 0; x < w; x++) {\n                    double var = 0, mean = 0, sqMean = 0;\n                    int upperLimit = windowMax;\n                    int lowerLimit = windowMin;\n                    int window = upperLimit; // start with windowMax\n                    while (upperLimit - lowerLimit > 2) {\n                        int x1 = Math.max(x-window/2, 0);\n                        int x2 = Math.min(x+window/2+1, w);\n\n                        int y1 = Math.max(y-window/2, 0);\n                        int y2 = Math.min(y+window/2+1, h);\n\n                        mean = sumBuf.get(y2*sumStep/8 + x2) -\n                               sumBuf.get(y2*sumStep/8 + x1) -\n                               sumBuf.get(y1*sumStep/8 + x2) +\n                               sumBuf.get(y1*sumStep/8 + x1);\n                        mean /= window*window;\n                        sqMean = sqSumBuf.get(y2*sqSumStep/8 + x2) -\n                                           sqSumBuf.get(y2*sqSumStep/8 + x1) -\n                                           sqSumBuf.get(y1*sqSumStep/8 + x2) +\n                                           sqSumBuf.get(y1*sqSumStep/8 + x1);\n                        sqMean /= window*window;\n                        var = sqMean - mean*mean;\n\n                        // if we're at maximum window size, but variance is\n                        // too low anyway, let's break out immediately\n                        if (window == upperLimit && var < targetVar) {\n                            break;\n                        }\n\n                        // otherwise, start binary search\n                        if (var > targetVar) {\n                            upperLimit = window;\n                        } else {\n                            lowerLimit = window;\n                        }\n\n                        window = lowerLimit   + (upperLimit-lowerLimit)/2;\n                        window = (window/2)*2 + 1;\n                    }\n\n                    double value = 0;\n                    if (srcDepth == IPL_DEPTH_8U) {\n                        value = srcBuf.get(y*srcStep       +   x) & 0xFF;\n                    } else if (srcDepth == IPL_DEPTH_32F) {\n                        value = srcBuf.getFloat(y*srcStep  + 4*x);\n                    } else if (srcDepth == IPL_DEPTH_64F) {\n                        value = srcBuf.getDouble(y*srcStep + 8*x);\n                    } else {\n                        // cvIntegral() does not support other image types,\n                        // so we should not be able to get here.\n                        assert false;\n                    }\n                    if (invert) {\n                        //double threshold = 255 - (255 - mean) * (1 + 0.1*(Math.sqrt(var)/128 - 1));\n                        double threshold = 255 - (255 - mean) * k;\n                        dstBuf.put(y*dstStep + x, (value < threshold ? (byte)0xFF : (byte)0x00));\n                    } else {\n                        //double threshold = mean * (1 + k*(Math.sqrt(var)/128 - 1));\n                        double threshold = mean * k;\n                        dstBuf.put(y*dstStep + x, (value > threshold ? (byte)0xFF : (byte)0x00));\n                    }\n                }\n            }\n        }});\n    }\n\n    /** similar to hysteresis thresholding as used by the Canny edge detector */\n    public static void hysteresisThreshold(IplImage srcImage, IplImage dstImage,\n            double highThresh, double lowThresh, double maxValue) {\n        int highThreshold = (int)Math.round(highThresh);\n        int lowThreshold  = (int)Math.round(lowThresh);\n        byte lowValue  = 0;\n        byte medValue  = (byte)Math.round(maxValue/2);\n        byte highValue = (byte)Math.round(maxValue);\n\n        int height = srcImage.height();\n        int width  = srcImage.width();\n\n        ByteBuffer srcData = srcImage.getByteBuffer();\n        ByteBuffer dstData = dstImage.getByteBuffer();\n        int srcStep = srcImage.widthStep();\n        int dstStep = dstImage.widthStep();\n        int srcIndex = 0;\n        int dstIndex = 0;\n\n        //\n        // first pass forward\n        //\n\n        // first line\n        int i = 0;\n        int in = srcData.get(srcIndex+i)&0xFF;\n        if (in >= highThreshold) {\n            dstData.put(dstIndex+i, highValue);\n        } else if (in < lowThreshold) {\n            dstData.put(dstIndex+i, lowValue);\n        } else {\n            dstData.put(dstIndex+i, medValue);\n        }\n\n        for (i = 1; i < width-1; i++) {\n            in = srcData.get(srcIndex+i)&0xFF;\n            if (in >= highThreshold) {\n                dstData.put(dstIndex+i, highValue);\n            } else if (in < lowThreshold) {\n                dstData.put(dstIndex+i, lowValue);\n            } else {\n                byte prev = dstData.get(dstIndex+i-1);\n                if (prev == highValue) {\n                    dstData.put(dstIndex+i, highValue);\n                } else {\n                    dstData.put(dstIndex+i, medValue);\n                }\n            }\n        }\n\n        i = width-1;\n        in = srcData.get(srcIndex+i)&0xFF;\n        if (in >= highThreshold) {\n            dstData.put(dstIndex+i, highValue);\n        } else if (in < lowThreshold) {\n            dstData.put(dstIndex+i, lowValue);\n        } else {\n            byte prev = dstData.get(dstIndex+i-1);\n            if (prev == highValue) {\n                dstData.put(dstIndex+i, highValue);\n            } else {\n                dstData.put(dstIndex+i, medValue);\n            }\n        }\n\n        height--;\n\n        // other lines\n        while (height-- > 0) {\n            srcIndex += srcStep;\n            dstIndex += dstStep;\n\n            // first column\n            i = 0;\n            in = srcData.get(srcIndex+i)&0xFF;\n            if (in >= highThreshold) {\n                dstData.put(dstIndex+i, highValue);\n            } else if (in < lowThreshold) {\n                dstData.put(dstIndex+i, lowValue);\n            } else {\n                byte prev1 = dstData.get(dstIndex+i-dstStep);\n                byte prev2 = dstData.get(dstIndex+i-dstStep+1);\n                if (prev1 == highValue || prev2 == highValue) {\n                    dstData.put(dstIndex+i, highValue);\n                } else {\n                    dstData.put(dstIndex+i, medValue);\n                }\n            }\n\n            // other columns\n            for (i = 1; i < width-1; i++) {\n                in = srcData.get(srcIndex+i)&0xFF;\n                if (in >= highThreshold) {\n                    dstData.put(dstIndex+i, highValue);\n                } else if (in < lowThreshold) {\n                    dstData.put(dstIndex+i, lowValue);\n                } else {\n                    byte prev1 = dstData.get(dstIndex+i-1);\n                    byte prev2 = dstData.get(dstIndex+i-dstStep-1);\n                    byte prev3 = dstData.get(dstIndex+i-dstStep);\n                    byte prev4 = dstData.get(dstIndex+i-dstStep+1);\n\n                    if (prev1 == highValue || prev2 == highValue ||\n                        prev3 == highValue || prev4 == highValue) {\n                        dstData.put(dstIndex+i, highValue);\n                    } else {\n                        dstData.put(dstIndex+i, medValue);\n                    }\n                }\n            }\n\n            // last column\n            i = width-1;\n            in = srcData.get(srcIndex+i)&0xFF;\n            if (in >= highThreshold) {\n                dstData.put(dstIndex+i, highValue);\n            } else if (in < lowThreshold) {\n                dstData.put(dstIndex+i, lowValue);\n            } else {\n                byte prev1 = dstData.get(dstIndex+i-1);\n                byte prev2 = dstData.get(dstIndex+i-dstStep-1);\n                byte prev3 = dstData.get(dstIndex+i-dstStep);\n\n                if (prev1 == highValue || prev2 == highValue ||\n                    prev3 == highValue) {\n                    dstData.put(dstIndex+i, highValue);\n                } else {\n                    dstData.put(dstIndex+i, medValue);\n                }\n            }\n        }\n\n        height = srcImage.height();\n        width  = srcImage.width();\n        dstIndex = (height-1)*dstStep;\n\n        //\n        // second pass backward\n        //\n\n        // first (actually last) line\n        i = width-1;\n        if (dstData.get(dstIndex+i) == medValue) {\n            dstData.put(dstIndex+i, lowValue);\n        }\n\n        for (i = width-2; i > 0 ; i--) {\n            if (dstData.get(dstIndex+i) == medValue) {\n                if (dstData.get(dstIndex+i+1) == highValue) {\n                    dstData.put(dstIndex+i, highValue);\n                } else {\n                    dstData.put(dstIndex+i, lowValue);\n                }\n            }\n        }\n\n        i = 0;\n        if (dstData.get(dstIndex+i) == medValue) {\n            if (dstData.get(dstIndex+i+1) == highValue) {\n                dstData.put(dstIndex+i, highValue);\n            } else {\n                dstData.put(dstIndex+i, lowValue);\n            }\n        }\n\n        height--;\n\n        // other lines\n        while (height-- > 0) {\n            dstIndex -= dstStep;\n\n            // first column\n            i = width-1;\n            if (dstData.get(dstIndex+i) == medValue) {\n                if (dstData.get(dstIndex+i+dstStep)   == highValue ||\n                    dstData.get(dstIndex+i+dstStep-1) == highValue) {\n                    dstData.put(dstIndex+i, highValue);\n                } else {\n                    dstData.put(dstIndex+i, lowValue);\n                }\n            }\n\n            // other columns\n            for (i = width-2; i > 0 ; i--) {\n                if (dstData.get(dstIndex+i) == medValue) {\n                    if (dstData.get(dstIndex+i+1)         == highValue ||\n                        dstData.get(dstIndex+i+dstStep+1) == highValue ||\n                        dstData.get(dstIndex+i+dstStep)   == highValue ||\n                        dstData.get(dstIndex+i+dstStep-1) == highValue) {\n                        dstData.put(dstIndex+i, highValue);\n                    } else {\n                        dstData.put(dstIndex+i, lowValue);\n                    }\n                }\n            }\n\n            // last column\n            i = 0;\n            if (dstData.get(dstIndex+i) == medValue) {\n                if (dstData.get(dstIndex+i+1)         == highValue ||\n                    dstData.get(dstIndex+i+dstStep+1) == highValue ||\n                    dstData.get(dstIndex+i+dstStep)   == highValue) {\n                    dstData.put(dstIndex+i, highValue);\n                } else {\n                    dstData.put(dstIndex+i, lowValue);\n                }\n            }\n        }\n    }\n\n    /** Clamps image intensities between min and max. */\n    public static void clamp(IplImage src, IplImage dst, double min, double max) {\n        switch (src.depth()) {\n            case IPL_DEPTH_8U: {\n                ByteBuffer sb = src.getByteBuffer();\n                ByteBuffer db = dst.getByteBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, (byte)Math.max(Math.min(sb.get(i) & 0xFF,max),min));\n                }\n                break;\n            }\n            case IPL_DEPTH_16U: {\n                ShortBuffer sb = src.getShortBuffer();\n                ShortBuffer db = dst.getShortBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, (short)Math.max(Math.min(sb.get(i) & 0xFFFF,max),min));\n                }\n                break;\n            }\n            case IPL_DEPTH_32F: {\n                FloatBuffer sb = src.getFloatBuffer();\n                FloatBuffer db = dst.getFloatBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, (float)Math.max(Math.min(sb.get(i),max),min));\n                }\n                break;\n            }\n            case IPL_DEPTH_8S: {\n                ByteBuffer sb = src.getByteBuffer();\n                ByteBuffer db = dst.getByteBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, (byte)Math.max(Math.min(sb.get(i),max),min));\n                }\n                break;\n            }\n            case IPL_DEPTH_16S: {\n                ShortBuffer sb = src.getShortBuffer();\n                ShortBuffer db = dst.getShortBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, (short)Math.max(Math.min(sb.get(i),max),min));\n                }\n                break;\n            }\n            case IPL_DEPTH_32S: {\n                IntBuffer sb = src.getIntBuffer();\n                IntBuffer db = dst.getIntBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, (int)Math.max(Math.min(sb.get(i),max),min));\n                }\n                break;\n            }\n            case IPL_DEPTH_64F: {\n                DoubleBuffer sb = src.getDoubleBuffer();\n                DoubleBuffer db = dst.getDoubleBuffer();\n                for (int i = 0; i < sb.capacity(); i++) {\n                    db.put(i, Math.max(Math.min(sb.get(i),max),min));\n                }\n                break;\n            }\n            default: assert(false);\n        }\n    }\n\n    /** vector norm 2 */\n    public static double norm(double[] v) {\n        return norm(v, 2.0);\n    }\n    /** vector norm p */\n    public static double norm(double[] v, double p) {\n        double norm = 0;\n        if (p == 1.0) {\n            for (double e : v) {\n                norm += Math.abs(e);\n            }\n        } else if (p == 2.0) {\n            for (double e : v) {\n                norm += e*e;\n            }\n            norm = Math.sqrt(norm);\n        } else if (p == Double.POSITIVE_INFINITY) {\n            for (double e : v) {\n                e = Math.abs(e);\n                if (e > norm) {\n                    norm = e;\n                }\n            }\n        } else if (p == Double.NEGATIVE_INFINITY) {\n            norm = Double.MAX_VALUE;\n            for (double e : v) {\n                e = Math.abs(e);\n                if (e < norm) {\n                    norm = e;\n                }\n            }\n        } else {\n            for (double e : v) {\n                norm += Math.pow(Math.abs(e), p);\n            }\n            norm = Math.pow(norm, 1/p);\n        }\n        return norm;\n    }\n\n    /** induced norm 2 */\n    public static double norm(CvMat A) {\n        return norm(A, 2.0);\n    }\n    /** induced norm p */\n    public static double norm(CvMat A, double p) {\n        return norm(A, p, null);\n    }\n    /** induced norm p */\n    public static double norm(CvMat A, double p, CvMat W) {\n        double norm = -1;\n\n        if (p == 1.0) {\n            int cols = A.cols(), rows = A.rows();\n            for (int j = 0; j < cols; j++) {\n                double n = 0;\n                for (int i = 0; i < rows; i++) {\n                    n += Math.abs(A.get(i, j));\n                }\n                norm = Math.max(n, norm);\n            }\n        } else if (p == 2.0) {\n            int size = Math.min(A.rows(), A.cols());\n            if (W == null || W.rows() != size || W.cols() != 1) {\n                W = CvMat.create(size, 1);\n            }\n            cvSVD(A, W, null, null, 0);\n            norm = W.get(0); // largest singular value\n        } else if (p == Double.POSITIVE_INFINITY) {\n            int rows = A.rows(), cols = A.cols();\n            for (int i = 0; i < rows; i++) {\n                double n = 0;\n                for (int j = 0; j < cols; j++) {\n                    n += Math.abs(A.get(i, j));\n                }\n                norm = Math.max(n, norm);\n            }\n        } else {\n            assert(false);\n        }\n        return norm;\n    }\n\n    public static double cond(CvMat A) {\n        return cond(A, 2.0);\n    }\n    public static double cond(CvMat A, double p) {\n        return cond(A, p, null);\n    }\n    public static double cond(CvMat A, double p, CvMat W) {\n        double cond = -1;\n\n        if (p == 2.0) {\n            int size = Math.min(A.rows(), A.cols());\n            if (W == null || W.rows() != size || W.cols() != 1) {\n                W = CvMat.create(size, 1);\n            }\n            cvSVD(A, W, null, null, 0);\n            cond = W.get(0)/W.get(W.length()-1); // largest/smallest singular value\n        } else {\n            // should put something faster here if we're really serious\n            // about using something other than the 2-norm\n            int rows = A.rows(), cols = A.cols();\n            if (W == null || W.rows() != rows || W.cols() != cols) {\n                W = CvMat.create(rows, cols);\n            }\n            CvMat Ainv = W;\n            cvInvert(A, Ainv);\n            cond = norm(A, p)*norm(Ainv, p);\n        }\n        return cond;\n    }\n\n    public static double median(double[] doubles) {\n        double[] sorted = doubles.clone();\n        Arrays.sort(sorted);\n        if (doubles.length%2 == 0) {\n            return (sorted[doubles.length/2 - 1] + sorted[doubles.length/2])/2;\n        } else {\n            return sorted[doubles.length/2];\n        }\n    }\n    public static <T extends Object> T median(T[] objects) {\n        T[] sorted = objects.clone();\n        Arrays.sort(sorted);\n        return sorted[sorted.length/2];\n    }\n\n    public static void fractalTriangleWave(double[] line, int i, int j, double a) {\n        fractalTriangleWave(line, i, j, a, -1);\n    }\n    public static void fractalTriangleWave(double[] line, int i, int j, double a, int roughness) {\n        int m = (j-i)/2+i;\n        if (i == j || i == m) {\n            return;\n        }\n        line[m] = (line[i]+line[j])/2 + a;\n        if (roughness > 0 && line.length > roughness*(j-i)) {\n            fractalTriangleWave(line, i, m, 0, roughness);\n            fractalTriangleWave(line, m, j, 0, roughness);\n        } else {\n            fractalTriangleWave(line, i, m,  a/SQRT2, roughness);\n            fractalTriangleWave(line, m, j, -a/SQRT2, roughness);\n        }\n    }\n\n    public static void fractalTriangleWave(IplImage image, CvMat H) {\n        fractalTriangleWave(image, H, -1);\n    }\n    public static void fractalTriangleWave(IplImage image, CvMat H, int roughness) {\n        assert (image.depth() == IPL_DEPTH_32F);\n        double[] line = new double[image.width()];\n        fractalTriangleWave(line, 0,             line.length/2,  1, roughness);\n        fractalTriangleWave(line, line.length/2, line.length-1, -1, roughness);\n\n        double[] minMax = { Double.MAX_VALUE, Double.MIN_VALUE };\n        int height   = image.height();\n        int width    = image.width();\n        int channels = image.nChannels();\n        int step     = image.widthStep();\n        int start = 0;\n        if (image.roi() != null) {\n            height = image.roi().height();\n            width  = image.roi().width();\n            start  = image.roi().yOffset()*step/4 + image.roi().xOffset()*channels;\n        }\n        FloatBuffer fb = image.getFloatBuffer(start);\n        double[] h = H == null ? null : H.get();\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                for (int z = 0; z < channels; z++) {\n                    double sum = 0.0;\n                    if (h == null) {\n                        sum += line[x];\n                    } else {\n                        double x2 = (h[0]*x + h[1]*y + h[2])/(h[6]*x + h[7]*y + h[8]);\n                        while (x2 < 0) {\n                            x2 += line.length;\n                        }\n                        int xi2   = (int)x2;\n                        double xn = x2 - xi2;\n                        sum += line[ xi2   %line.length]*(1-xn) +\n                               line[(xi2+1)%line.length]*xn;\n                    }\n                    minMax[0] = Math.min(minMax[0], sum);\n                    minMax[1] = Math.max(minMax[1], sum);\n                    fb.put(y*step/4 + x*channels + z, (float)sum);\n                }\n            }\n        }\n\n        cvConvertScale(image, image, 1/(minMax[1]-minMax[0]),\n                -minMax[0]/(minMax[1]-minMax[0]));\n    }\n\n    public static void main(String[] args) {\n        String version = JavaCV.class.getPackage().getImplementationVersion();\n        if (version == null) {\n            version = \"unknown\";\n        }\n        System.out.println(\n            \"JavaCV version \" + version + \"\\n\" +\n            \"Copyright (C) 2009-2018 Samuel Audet <samuel.audet@gmail.com>\\n\" +\n            \"Project site: https://github.com/bytedeco/javacv\");\n        System.exit(0);\n    }\n\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport java.nio.ByteBuffer;\n\nimport java.nio.DoubleBuffer;\n\nimport java.nio.FloatBuffer;\n\nimport java.nio.IntBuffer;\n\nimport java.nio.ShortBuffer;\n\nimport java.util.Arrays;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport org.bytedeco.opencv.opencv_imgproc;\n\nimport org.bytedeco.opencv.global.opencv_core;\n\nimport org.bytedeco.opencv.global.opencv_imgproc;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_imgproc;\n\npublic class JavaCV {\n    static public final double SQRT2, FLT_EPSILON, DBL_EPSILON;\n    static public double distanceToLine(double x1, double y1, double x2, double y2, double x3, double y3);\n    static private ThreadLocal<CvMoments> moments;\n    static public CvBox2D boundedRect(CvMat contour, CvBox2D box);\n    static public CvRect boundingRect(double[] contour, CvRect rect, int padX, int padY, int alignX, int alignY);\n    static private ThreadLocal<CvMat> A8x8, b8x1, x8x1;\n    static public CvMat getPerspectiveTransform(double[] src, double[] dst, CvMat map_matrix);\n    static public  perspectiveTransform(double[] src, double[] dst, CvMat map_matrix);\n    static private ThreadLocal<CvMat> A3x3, b3x1;\n    static public CvMat getPlaneParameters(double[] src, double[] dst, CvMat invSrcK, CvMat dstK, CvMat R, CvMat t, CvMat n);\n    static private ThreadLocal<CvMat> n3x1;\n    static public CvMat getPerspectiveTransform(double[] src, double[] dst, CvMat invSrcK, CvMat dstK, CvMat R, CvMat t, CvMat H);\n    static private ThreadLocal<CvMat> H3x3;\n    static public  perspectiveTransform(double[] src, double[] dst, CvMat invSrcK, CvMat dstK, CvMat R, CvMat t, CvMat n, boolean invert);\n    static private ThreadLocal<CvMat> M3x2, S2x2, U3x2, V2x2;\n    static public  HtoRt(CvMat H, CvMat R, CvMat t);\n    static private ThreadLocal<CvMat> R13x3, R23x3, t13x1, t23x1, n13x1, n23x1, H13x3, H23x3;\n    static public double HnToRt(CvMat H, CvMat n, CvMat R, CvMat t);\n    static private ThreadLocal<CvMat> S3x3, U3x3, V3x3;\n    static public double homogToRt(CvMat H, CvMat R1, CvMat t1, CvMat n1, CvMat R2, CvMat t2, CvMat n2);\n    static public double homogToRt(CvMat S, CvMat U, CvMat V, CvMat R1, CvMat t1, CvMat n1, CvMat R2, CvMat t2, CvMat n2);\n    static public double[] unitize(double a, double b);\n    static public  adaptiveThreshold(IplImage srcImage, IplImage sumImage, IplImage sqSumImage, IplImage dstImage, boolean invert, int windowMax, int windowMin, double varMultiplier, double k);\n    static public  hysteresisThreshold(IplImage srcImage, IplImage dstImage, double highThresh, double lowThresh, double maxValue);\n    static public  clamp(IplImage src, IplImage dst, double min, double max);\n    static public double norm(double[] v);\n    static public double norm(double[] v, double p);\n    static public double norm(CvMat A);\n    static public double norm(CvMat A, double p);\n    static public double norm(CvMat A, double p, CvMat W);\n    static public double cond(CvMat A);\n    static public double cond(CvMat A, double p);\n    static public double cond(CvMat A, double p, CvMat W);\n    static public double median(double[] doubles);\n    static public T median(T objects);\n    static public  fractalTriangleWave(double[] line, int i, int j, double a);\n    static public  fractalTriangleWave(double[] line, int i, int j, double a, int roughness);\n    static public  fractalTriangleWave(IplImage image, CvMat H);\n    static public  fractalTriangleWave(IplImage image, CvMat H, int roughness);\n    static public  main(String args);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "ObjectFinder.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/ObjectFinder.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/** a rough implementation for object location */",
        "source_code": "\ndouble[] locatePlanarObject(KeyPointVector objectKeypoints, Mat objectDescriptors,\n        KeyPointVector imageKeypoints, Mat imageDescriptors, double[] srcCorners) {\n    ptpairs.clear();\n    if (settings.useFLANN) {\n        flannFindPairs(objectDescriptors, imageDescriptors);\n    } else {\n        findPairs(objectDescriptors, imageDescriptors);\n    }\n    int n = ptpairs.size()/2;\n    logger.info(n + \" matching pairs found\");\n    if (n < settings.matchesMin) {\n        return null;\n    }\n\n    pt1 .resize(n);\n    pt2 .resize(n);\n    mask.resize(n);\n    FloatBuffer pt1Idx = pt1.createBuffer();\n    FloatBuffer pt2Idx = pt2.createBuffer();\n    for (int i = 0; i < n; i++) {\n        Point2f p1 = objectKeypoints.get(ptpairs.get(2*i)).pt();\n        pt1Idx.put(2*i, p1.x()); pt1Idx.put(2*i+1, p1.y());\n        Point2f p2 = imageKeypoints.get(ptpairs.get(2*i+1)).pt();\n        pt2Idx.put(2*i, p2.x()); pt2Idx.put(2*i+1, p2.y());\n    }\n\n    H = findHomography(pt1, pt2, CV_RANSAC, settings.ransacReprojThreshold, mask, 2000, 0.995);\n    if (H.empty() || countNonZero(mask) < settings.matchesMin) {\n        return null;\n    }\n\n    double[] h = (double[])H.createIndexer(false).array();\n    double[] dstCorners = new double[srcCorners.length];\n    for(int i = 0; i < srcCorners.length/2; i++) {\n        double x = srcCorners[2*i], y = srcCorners[2*i + 1];\n        double Z = 1/(h[6]*x + h[7]*y + h[8]);\n        double X = (h[0]*x + h[1]*y + h[2])*Z;\n        double Y = (h[3]*x + h[4]*y + h[5])*Z;\n        dstCorners[2*i    ] = X;\n        dstCorners[2*i + 1] = Y;\n    }\n    return dstCorners;\n}\n",
        "class_name": "ObjectFinder",
        "method_name": "locatePlanarObject",
        "argument_name": [
            "KeyPointVector objectKeypoints",
            "Mat objectDescriptors",
            "KeyPointVector imageKeypoints",
            "Mat imageDescriptors",
            "double[] srcCorners"
        ],
        "full_context": "/*\n * Copyright (C) 2009-2015 Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n *\n * Adapted from the find_obj.cpp sample in the source package of OpenCV 2.3.1:\n *\n * A Demo to OpenCV Implementation of SURF\n * Further Information Refer to \"SURF: Speed-Up Robust Feature\"\n * Author: Liu Liu\n * liuliu.1987+opencv@gmail.com\n */\n\npackage org.bytedeco.javacv;\n\nimport java.nio.ByteBuffer;\nimport java.nio.FloatBuffer;\nimport java.nio.IntBuffer;\nimport java.util.ArrayList;\nimport java.util.logging.Logger;\n\nimport org.bytedeco.opencv.opencv_calib3d.*;\nimport org.bytedeco.opencv.opencv_core.*;\nimport org.bytedeco.opencv.opencv_features2d.*;\nimport org.bytedeco.opencv.opencv_flann.*;\nimport org.bytedeco.opencv.opencv_imgproc.*;\nimport static org.bytedeco.opencv.global.opencv_calib3d.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_features2d.*;\nimport static org.bytedeco.opencv.global.opencv_flann.*;\nimport static org.bytedeco.opencv.global.opencv_imgcodecs.*;\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\n\n/**\n *\n * @author Samuel Audet\n *\n * ObjectFinder does not work out-of-the-box under Android, because it lacks the standard\n * java.beans.beancontext package. We can work around it by doing the following *BEFORE*\n * following the instructions in the README.md file:\n *\n * 1. Remove BaseChildSettings.class and BaseSettings.class from javacv.jar\n * 2. Follow the instructions in the README.md file\n * 3. In your project, define empty classes BaseChildSettings and BaseSettings under the org.bytedeco.javacv package name\n */\npublic class ObjectFinder {\n    public ObjectFinder(IplImage objectImage) {\n        settings = new Settings();\n        settings.objectImage = objectImage;\n        setSettings(settings);\n    }\n    public ObjectFinder(Settings settings) {\n        setSettings(settings);\n    }\n\n    public static class Settings extends BaseChildSettings {\n        IplImage objectImage = null;\n        AKAZE detector = AKAZE.create();\n        double distanceThreshold = 0.75;\n        int matchesMin = 4;\n        double ransacReprojThreshold = 1.0;\n        boolean useFLANN = false;\n\n        public IplImage getObjectImage() {\n            return objectImage;\n        }\n        public void setObjectImage(IplImage objectImage) {\n            this.objectImage = objectImage;\n        }\n\n        public int getDescriptorType() {\n            return detector.getDescriptorType();\n        }\n        public void setDescriptorType(int dtype) {\n            detector.setDescriptorType(dtype);\n        }\n\n        public int getDescriptorSize() {\n            return detector.getDescriptorSize();\n        }\n        public void setDescriptorSize(int dsize) {\n            detector.setDescriptorSize(dsize);\n        }\n\n        public int getDescriptorChannels() {\n            return detector.getDescriptorChannels();\n        }\n        public void setDescriptorChannels(int dch) {\n            detector.setDescriptorChannels(dch);\n        }\n\n        public double getThreshold() {\n            return detector.getThreshold();\n        }\n        public void setThreshold(double threshold) {\n            detector.setThreshold(threshold);\n        }\n\n        public int getNOctaves() {\n            return detector.getNOctaves();\n        }\n        public void setNOctaves(int nOctaves) {\n            detector.setNOctaves(nOctaves);\n        }\n\n        public int getNOctaveLayers() {\n            return detector.getNOctaveLayers();\n        }\n        public void setNOctaveLayers(int nOctaveLayers) {\n            detector.setNOctaveLayers(nOctaveLayers);\n        }\n\n        public double getDistanceThreshold() {\n            return distanceThreshold;\n        }\n        public void setDistanceThreshold(double distanceThreshold) {\n            this.distanceThreshold = distanceThreshold;\n        }\n\n        public int getMatchesMin() {\n            return matchesMin;\n        }\n        public void setMatchesMin(int matchesMin) {\n            this.matchesMin = matchesMin;\n        }\n\n        public double getRansacReprojThreshold() {\n            return ransacReprojThreshold;\n        }\n        public void setRansacReprojThreshold(double ransacReprojThreshold) {\n            this.ransacReprojThreshold = ransacReprojThreshold;\n        }\n\n        public boolean isUseFLANN() {\n            return useFLANN;\n        }\n        public void setUseFLANN(boolean useFLANN) {\n            this.useFLANN = useFLANN;\n        }\n    }\n\n    Settings settings;\n    public Settings getSettings() {\n        return settings;\n    }\n    public void setSettings(Settings settings) {\n        this.settings = settings;\n\n        objectKeypoints = new KeyPointVector();\n        objectDescriptors = new Mat();\n        settings.detector.detectAndCompute(cvarrToMat(settings.objectImage),\n                new Mat(), objectKeypoints, objectDescriptors, false);\n\n        int total = (int)objectKeypoints.size();\n        if (settings.useFLANN) {\n            indicesMat = new Mat(total, 2, CV_32SC1);\n            distsMat   = new Mat(total, 2, CV_32FC1);\n            flannIndex = new Index();\n            indexParams = new LshIndexParams(12, 20, 2); // using LSH Hamming distance\n            searchParams = new SearchParams(64, 0, true); // maximum number of leafs checked\n            searchParams.deallocate(false); // for some reason FLANN seems to do it for us\n        }\n        pt1  = new Mat(total, 1, CV_32FC2);\n        pt2  = new Mat(total, 1, CV_32FC2);\n        mask = new Mat(total, 1, CV_8UC1);\n        H    = new Mat(3, 3, CV_64FC1);\n        ptpairs = new ArrayList<Integer>(2*objectDescriptors.rows());\n        logger.info(total + \" object descriptors\");\n    }\n\n    static final Logger logger = Logger.getLogger(ObjectFinder.class.getName());\n\n    KeyPointVector objectKeypoints = null, imageKeypoints = null;\n    Mat objectDescriptors = null, imageDescriptors = null;\n    Mat indicesMat, distsMat;\n    Index flannIndex = null;\n    IndexParams indexParams = null;\n    SearchParams searchParams = null;\n    Mat pt1 = null, pt2 = null, mask = null, H = null;\n    ArrayList<Integer> ptpairs = null;\n\n    public double[] find(IplImage image) {\n        if (objectDescriptors.rows() < settings.getMatchesMin()) {\n            return null;\n        }\n        imageKeypoints = new KeyPointVector();\n        imageDescriptors = new Mat();\n        settings.detector.detectAndCompute(cvarrToMat(image),\n                new Mat(), imageKeypoints, imageDescriptors, false);\n        if (imageDescriptors.rows() < settings.getMatchesMin()) {\n            return null;\n        }\n\n        int total = (int)imageKeypoints.size();\n        logger.info(total + \" image descriptors\");\n\n        int w = settings.objectImage.width();\n        int h = settings.objectImage.height();\n        double[] srcCorners = {0, 0,  w, 0,  w, h,  0, h};\n        double[] dstCorners = locatePlanarObject(objectKeypoints, objectDescriptors,\n                imageKeypoints, imageDescriptors, srcCorners);\n        return dstCorners;\n    }\n\n    static final int[] bits = new int[256];\n    static {\n        for (int i = 0; i < bits.length; i++) {\n            for (int j = i; j != 0; j >>= 1) {\n                bits[i] += j & 0x1;\n            }\n        }\n    }\n\n    int compareDescriptors(ByteBuffer d1, ByteBuffer d2, int best) {\n        int totalCost = 0;\n        assert d1.limit() - d1.position() == d2.limit() - d2.position();\n        while (d1.position() < d1.limit()) {\n            totalCost += bits[(d1.get() ^ d2.get()) & 0xFF];\n            if (totalCost > best)\n                break;\n        }\n        return totalCost;\n    }\n\n    int naiveNearestNeighbor(ByteBuffer vec, ByteBuffer modelDescriptors) {\n        int neighbor = -1;\n        int d, dist1 = Integer.MAX_VALUE, dist2 = Integer.MAX_VALUE;\n        int size = vec.limit() - vec.position();\n\n        for (int i = 0; i * size < modelDescriptors.capacity(); i++) {\n            ByteBuffer mvec = (ByteBuffer)modelDescriptors.position(i * size).limit((i + 1) * size);\n            d = compareDescriptors((ByteBuffer)vec.reset(), mvec, dist2);\n            if (d < dist1) {\n                dist2 = dist1;\n                dist1 = d;\n                neighbor = i;\n            } else if (d < dist2) {\n                dist2 = d;\n            }\n        }\n        if (dist1 < settings.distanceThreshold*dist2)\n            return neighbor;\n        return -1;\n    }\n\n    void findPairs(Mat objectDescriptors, Mat imageDescriptors) {\n        int size = imageDescriptors.cols();\n        ByteBuffer objectBuf = objectDescriptors.createBuffer();\n        ByteBuffer imageBuf = imageDescriptors.createBuffer();\n\n        for (int i = 0; i * size < objectBuf.capacity(); i++) {\n            ByteBuffer descriptor = (ByteBuffer)objectBuf.position(i * size).limit((i + 1) * size).mark();\n            int nearestNeighbor = naiveNearestNeighbor(descriptor, imageBuf);\n            if (nearestNeighbor >= 0) {\n                ptpairs.add(i);\n                ptpairs.add(nearestNeighbor);\n            }\n        }\n    }\n\n    void flannFindPairs(Mat objectDescriptors, Mat imageDescriptors) {\n        int length = objectDescriptors.rows();\n\n        // find nearest neighbors using FLANN\n        flannIndex.build(imageDescriptors, indexParams, FLANN_DIST_HAMMING);\n        flannIndex.knnSearch(objectDescriptors, indicesMat, distsMat, 2, searchParams);\n\n        IntBuffer indicesBuf = indicesMat.createBuffer();\n        IntBuffer distsBuf = distsMat.createBuffer();\n        for (int i = 0; i < length; i++) {\n            if (distsBuf.get(2*i) < settings.distanceThreshold*distsBuf.get(2*i+1)) {\n                ptpairs.add(i);\n                ptpairs.add(indicesBuf.get(2*i));\n            }\n        }\n    }\n\n    /** a rough implementation for object location */\n    double[] locatePlanarObject(KeyPointVector objectKeypoints, Mat objectDescriptors,\n            KeyPointVector imageKeypoints, Mat imageDescriptors, double[] srcCorners) {\n        ptpairs.clear();\n        if (settings.useFLANN) {\n            flannFindPairs(objectDescriptors, imageDescriptors);\n        } else {\n            findPairs(objectDescriptors, imageDescriptors);\n        }\n        int n = ptpairs.size()/2;\n        logger.info(n + \" matching pairs found\");\n        if (n < settings.matchesMin) {\n            return null;\n        }\n\n        pt1 .resize(n);\n        pt2 .resize(n);\n        mask.resize(n);\n        FloatBuffer pt1Idx = pt1.createBuffer();\n        FloatBuffer pt2Idx = pt2.createBuffer();\n        for (int i = 0; i < n; i++) {\n            Point2f p1 = objectKeypoints.get(ptpairs.get(2*i)).pt();\n            pt1Idx.put(2*i, p1.x()); pt1Idx.put(2*i+1, p1.y());\n            Point2f p2 = imageKeypoints.get(ptpairs.get(2*i+1)).pt();\n            pt2Idx.put(2*i, p2.x()); pt2Idx.put(2*i+1, p2.y());\n        }\n\n        H = findHomography(pt1, pt2, CV_RANSAC, settings.ransacReprojThreshold, mask, 2000, 0.995);\n        if (H.empty() || countNonZero(mask) < settings.matchesMin) {\n            return null;\n        }\n\n        double[] h = (double[])H.createIndexer(false).array();\n        double[] dstCorners = new double[srcCorners.length];\n        for(int i = 0; i < srcCorners.length/2; i++) {\n            double x = srcCorners[2*i], y = srcCorners[2*i + 1];\n            double Z = 1/(h[6]*x + h[7]*y + h[8]);\n            double X = (h[0]*x + h[1]*y + h[2])*Z;\n            double Y = (h[3]*x + h[4]*y + h[5])*Z;\n            dstCorners[2*i    ] = X;\n            dstCorners[2*i + 1] = Y;\n        }\n        return dstCorners;\n    }\n\n    public static void main(String[] args) throws Exception {\n//        Logger.getLogger(\"org.bytedeco.javacv\").setLevel(Level.OFF);\n\n        String objectFilename = args.length == 2 ? args[0] : \"/usr/local/share/OpenCV/samples/c/box.png\";\n        String sceneFilename  = args.length == 2 ? args[1] : \"/usr/local/share/OpenCV/samples/c/box_in_scene.png\";\n\n        IplImage object = cvLoadImage(objectFilename, IMREAD_GRAYSCALE);\n        IplImage image  = cvLoadImage(sceneFilename,  IMREAD_GRAYSCALE);\n        if (object == null || image == null) {\n            System.err.println(\"Can not load \" + objectFilename + \" and/or \" + sceneFilename);\n            System.exit(-1);\n        }\n\n        IplImage objectColor = IplImage.create(object.width(), object.height(), 8, 3);\n        cvCvtColor(object, objectColor, CV_GRAY2BGR);\n\n        IplImage correspond = IplImage.create(image.width(), object.height()+ image.height(), 8, 1);\n        cvSetImageROI(correspond, cvRect(0, 0, object.width(), object.height()));\n        cvCopy(object, correspond);\n        cvSetImageROI(correspond, cvRect(0, object.height(), correspond.width(), correspond.height()));\n        cvCopy(image, correspond);\n        cvResetImageROI(correspond);\n\n        ObjectFinder.Settings settings = new ObjectFinder.Settings();\n        settings.objectImage = object;\n        settings.useFLANN = true;\n        settings.ransacReprojThreshold = 5;\n        ObjectFinder finder = new ObjectFinder(settings);\n\n        long start = System.currentTimeMillis();\n        double[] dst_corners = finder.find(image);\n        System.out.println(\"Finding time = \" + (System.currentTimeMillis() - start) + \" ms\");\n\n        if (dst_corners !=  null) {\n            for (int i = 0; i < 4; i++) {\n                int j = (i+1)%4;\n                int x1 = (int)Math.round(dst_corners[2*i    ]);\n                int y1 = (int)Math.round(dst_corners[2*i + 1]);\n                int x2 = (int)Math.round(dst_corners[2*j    ]);\n                int y2 = (int)Math.round(dst_corners[2*j + 1]);\n                line(cvarrToMat(correspond), new Point(x1, y1 + object.height()),\n                        new Point(x2, y2 + object.height()),\n                        Scalar.WHITE, 1, 8, 0);\n            }\n        }\n\n        for (int i = 0; i < finder.ptpairs.size(); i += 2) {\n            Point2f pt1 = finder.objectKeypoints.get(finder.ptpairs.get(i)).pt();\n            Point2f pt2 = finder.imageKeypoints.get(finder.ptpairs.get(i + 1)).pt();\n            line(cvarrToMat(correspond), new Point(Math.round(pt1.x()), Math.round(pt1.y())),\n                    new Point(Math.round(pt2.x()), Math.round(pt2.y() + object.height())),\n                    Scalar.WHITE, 1, 8, 0);\n        }\n\n        CanvasFrame objectFrame = new CanvasFrame(\"Object\");\n        CanvasFrame correspondFrame = new CanvasFrame(\"Object Correspond\");\n        OpenCVFrameConverter converter = new OpenCVFrameConverter.ToIplImage();\n\n        correspondFrame.showImage(converter.convert(correspond));\n        for (int i = 0; i < finder.objectKeypoints.size(); i++) {\n            KeyPoint r = finder.objectKeypoints.get(i);\n            Point center = new Point(Math.round(r.pt().x()), Math.round(r.pt().y()));\n            int radius = Math.round(r.size() / 2);\n            circle(cvarrToMat(objectColor), center, radius, Scalar.RED, 1, 8, 0);\n        }\n        objectFrame.showImage(converter.convert(objectColor));\n\n        objectFrame.waitKey();\n\n        objectFrame.dispose();\n        correspondFrame.dispose();\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport java.nio.ByteBuffer;\n\nimport java.nio.FloatBuffer;\n\nimport java.nio.IntBuffer;\n\nimport java.util.ArrayList;\n\nimport java.util.logging.Logger;\n\nimport org.bytedeco.opencv.opencv_calib3d;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport org.bytedeco.opencv.opencv_features2d;\n\nimport org.bytedeco.opencv.opencv_flann;\n\nimport org.bytedeco.opencv.opencv_imgproc;\n\nimport static org.bytedeco.opencv.global.opencv_calib3d;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_features2d;\n\nimport static org.bytedeco.opencv.global.opencv_flann;\n\nimport static org.bytedeco.opencv.global.opencv_imgcodecs;\n\nimport static org.bytedeco.opencv.global.opencv_imgproc;\n\npublic class ObjectFinder {\n    public ObjectFinder(IplImage objectImage);\n    public ObjectFinder(Settings settings);\n    static public class Settings extends BaseChildSettings {\n        IplImage objectImage;\n        AKAZE detector;\n        double distanceThreshold;\n        int matchesMin;\n        double ransacReprojThreshold;\n        boolean useFLANN;\n        public IplImage getObjectImage();\n        public  setObjectImage(IplImage objectImage);\n        public int getDescriptorType();\n        public  setDescriptorType(int dtype);\n        public int getDescriptorSize();\n        public  setDescriptorSize(int dsize);\n        public int getDescriptorChannels();\n        public  setDescriptorChannels(int dch);\n        public double getThreshold();\n        public  setThreshold(double threshold);\n        public int getNOctaves();\n        public  setNOctaves(int nOctaves);\n        public int getNOctaveLayers();\n        public  setNOctaveLayers(int nOctaveLayers);\n        public double getDistanceThreshold();\n        public  setDistanceThreshold(double distanceThreshold);\n        public int getMatchesMin();\n        public  setMatchesMin(int matchesMin);\n        public double getRansacReprojThreshold();\n        public  setRansacReprojThreshold(double ransacReprojThreshold);\n        public boolean isUseFLANN();\n        public  setUseFLANN(boolean useFLANN);\n    }\n    Settings settings;\n    public Settings getSettings();\n    public  setSettings(Settings settings);\n    static final Logger logger;\n    KeyPointVector objectKeypoints, imageKeypoints;\n    Mat objectDescriptors, imageDescriptors;\n    Mat indicesMat, distsMat;\n    Index flannIndex;\n    IndexParams indexParams;\n    SearchParams searchParams;\n    Mat pt1, pt2, mask, H;\n    ArrayList<Integer> ptpairs;\n    public double[] find(IplImage image);\n    static final int[] bits;\n    int compareDescriptors(ByteBuffer d1, ByteBuffer d2, int best);\n    int naiveNearestNeighbor(ByteBuffer vec, ByteBuffer modelDescriptors);\n     findPairs(Mat objectDescriptors, Mat imageDescriptors);\n     flannFindPairs(Mat objectDescriptors, Mat imageDescriptors);\n    double[] locatePlanarObject(KeyPointVector objectKeypoints, Mat objectDescriptors, KeyPointVector imageKeypoints, Mat imageDescriptors, double[] srcCorners);\n    static public  main(String args)throws Exception;\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "PS3EyeFrameGrabber.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/PS3EyeFrameGrabber.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/** Ask the driver for all installed PS3 cameras. Resulting array is sorted in order of camera index.\n     *  Its size is defined by CLCamera.cameraCount().\n     * \n     * @return array of camera unique uuids or null if there is no PS3 camera\n     */",
        "source_code": "\npublic static String[] listPS3Cameras() {\n    int no = getCameraCount();\n    String[] uuids;\n    if (no > 0) {\n        uuids = new String[no];\n        for (--no; no >=0; no--) { uuids[no] = CLCamera.cameraUUID(no); }\n        return uuids;\n    }\n    return null;\n}\n",
        "class_name": "PS3EyeFrameGrabber",
        "method_name": "listPS3Cameras",
        "argument_name": [],
        "full_context": "/*\n * Copyright (C) 2011-2012 Jiri Masa, Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport cl.eye.CLCamera;\nimport java.io.File;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport org.bytedeco.opencv.opencv_imgproc.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\n\n/** Minimal Sony PS3 Eye camera grabber implementation.\n * \n *  It allows grabbing of frames at higher speed than OpenCVFrameGrabber or VideoInputFrameGrabber.\n *  Underlying implementation of last two grabbers is limited to 30 FPS. PS3 allows grabbing\n *  at maximum speed of 75 FPS in VGA and 187 FPS in QVGA modes.\n *  \n *  This code was developed and tested with CLEyeMulticam.dll, version 1.2.0.1008. The dll library\n *  is part of Code Laboratories CL-Eye Platform SDK and is distributed as part of CLEyeMulticam\n *  Redistributable Dynamic Link Library. For license, download and installation see http://www.codelaboratories.com.\n *    \n *  The grab() method returns an internal instance of IplImage image with fresh camera frame. This returned image\n *  have to be considered \"read only\" and the caller needs to create it's own copy or clone that image.\n *  Calling of release() method for this image shall be avoided.\n *  Based on used resolution the image is in format either 640x480 or 320x240, IPL_DEPTH_8U, 4 channel (color) or 1 channel (gray).\n *  timestamp is set to actual value of System.nanoTime()/1000 obtained after return from the CL driver.\n *  \n *  Typical use case scenario:\n *     create new instance of PS3MiniGrabber\n *     set camera parameters\n *     start() grabber \n *     wait at least 2 frames\n *     grab() in loop\n *     stop() grabber\n *     release() internal resources\n *     \n * Note:\n * This code depends on the cl.eye.CLCamera class from Code Laboratories CL-Eye\n * Platform SDK. It is suggested to download SDK and edit the sample file\n * ....\\cl\\eye\\CLCamera.java. A few references to processing.core.PApplet class\n * shall be removed and the file recompiled. The tailored file is not included\n * here namely because of unclear licence.\n * \n *  @author jmasa, jmasa@cmail.cz\n *\n */\npublic class PS3EyeFrameGrabber extends FrameGrabber {\n    public static String[] getDeviceDescriptions() throws Exception {\n        tryLoad();\n        String[] descriptions = new String[CLCamera.cameraCount()];\n        for (int i = 0; i < descriptions.length; i++) {\n            descriptions[i] = CLCamera.cameraUUID(i);\n        }\n        return descriptions;\n    }\n\n    public static PS3EyeFrameGrabber createDefault(File deviceFile)   throws Exception { throw new Exception(PS3EyeFrameGrabber.class + \" does not support device files.\"); }\n    public static PS3EyeFrameGrabber createDefault(String devicePath) throws Exception { throw new Exception(PS3EyeFrameGrabber.class + \" does not support device paths.\"); }\n    public static PS3EyeFrameGrabber createDefault(int deviceNumber)  throws Exception { return new PS3EyeFrameGrabber(deviceNumber); }\n\n    private static Exception loadingException = null;\n    public static void tryLoad() throws Exception {\n        if (loadingException != null) {\n            throw loadingException;\n        } else {\n            try {\n                CLCamera.IsLibraryLoaded();\n            } catch (Throwable t) {\n                throw loadingException = new Exception(\"Failed to load \" + PS3EyeFrameGrabber.class, t);\n            }\n        }\n    }\n\n    CLCamera camera;\n    int cameraIndex = 0;\n    int[]  ps3_frame = null;             // buffer for PS3 camera frame data\n    byte[] ipl_frame = null;             // buffer for RGB-3ch, not allocated unless grab_RGB3() is called \n\n    IplImage image_4ch = null;\n    IplImage image_1ch = null;\n    FrameConverter converter = new OpenCVFrameConverter.ToIplImage();\n\n    String stat;                  // status of PS3 camera handling - mostly for debugging\n    String uuid;                  // assigned camera unique key\n\n    // variables for trigger() implementation\n    //\n    protected enum Triggered {NO_TRIGGER, HAS_FRAME, NO_FRAME};\n    protected Triggered triggered = Triggered.NO_TRIGGER;\n\n\n    /** Default grabber, camera idx = 0, color mode, VGA resolution, 60 FPS frame rate.\n     *   \n     */\n    public PS3EyeFrameGrabber() throws Exception {\n        this(0);\n    }\n\n    /** Color mode, VGA resolution, 60 FPS frame rate.\n     *   @param system wide camera index\n     */\n    public PS3EyeFrameGrabber(int cameraIndex) throws Exception {\n        this(cameraIndex, 640, 480, 60);\n    }\n\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate) throws Exception {\n        this(cameraIndex, 640, 480, 60, null);\n    }\n\n    /** Creates grabber, the caller can control basic image and grabbing parameters.\n     * \n     * @param cameraIndex - zero based index of used camera (OS system wide)\n     * @param imageWidth  - width of image \n     * @param imageHeight - height of image\n     * @param framerate   - frame rate - see CLCamera for allowed frame rates based on resolution\n     * @param applet      - PApplet object required by CLCamera\n     * @throws Exception  - if parameters don't follow CLCamera definition or camera is not created\n     */\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate, Object applet) throws Exception {\n        camera = null;\n\n        if (! CLCamera.IsLibraryLoaded()) {\n            throw new Exception(\"CLEye multicam dll not loaded\");\n        }\n\n        this.camera = new CLCamera();\n        this.cameraIndex = cameraIndex;\n\n        stat = \"created\";\n        uuid = CLCamera.cameraUUID(cameraIndex);\n\n        if (((imageWidth == 640) && (imageHeight == 480)) || \n            ((imageWidth == 320) && (imageHeight == 240))) {\n            setImageWidth(imageWidth);\n            setImageHeight(imageHeight);\n        }\n        else throw new Exception(\"Only 640x480 or 320x240 images supported\");\n\n        setImageMode(ImageMode.COLOR);\n        setFrameRate((double) framerate);  \n        setTimeout(1 + 1000/framerate);\n        setBitsPerPixel(8);\n        setTriggerMode(false);\n        setNumBuffers(4);\n    }\n\n    /** \n     * @return system wide number of installed/detected Sony PS3 Eye cameras\n     */\n    public static int getCameraCount() {\n        return CLCamera.cameraCount();\n    }\n\n    /** Ask the driver for all installed PS3 cameras. Resulting array is sorted in order of camera index.\n     *  Its size is defined by CLCamera.cameraCount().\n     * \n     * @return array of camera unique uuids or null if there is no PS3 camera\n     */\n    public static String[] listPS3Cameras() {\n        int no = getCameraCount();\n        String[] uuids;\n        if (no > 0) {\n            uuids = new String[no];\n            for (--no; no >=0; no--) { uuids[no] = CLCamera.cameraUUID(no); }\n            return uuids;\n        }\n        return null;\n    }\n\n\n    /** Make IplImage form raw int[] frame data\n     *  Note: NO array size checks!!\n     * \n     * @param frame int[] image frame data \n     * @return internal IplImage set to frame\n     */\n    public IplImage makeImage(int[] frame) {\n        image_4ch.getIntBuffer().put(ps3_frame);\n        return image_4ch;\n    }\n\n\n    /** Grab one frame and return it as int[] (in the internal camera format RGBA).\n     *  Note: use makeImage() to create RGBA, 4-ch image\n     * @return frame as int[] without any processing or null if frame is not available \n     */\n    public int[] grab_raw() {\n        if (camera.getCameraFrame(ps3_frame, timeout)) {\n            return ps3_frame;\n        }\n        else return null;\n    }\n\n    public void trigger() throws Exception {\n        for (int i = 0; i < numBuffers+1; i++) {\n            grab_raw();\n        }\n\n        if ((ps3_frame = grab_raw()) != null) {\n            triggered = Triggered.HAS_FRAME;\n            timestamp = System.nanoTime()/1000;\n        }\n        else\n            triggered = Triggered.NO_FRAME;\n    }\n\n\n    /** Grab and convert one frame, default timeout is (1 + 1000/framerate) [milliseconds].\n     *  Every successful call returns an internal (preallocated) 640x480 or 320x240, IPL_DEPTH_8U, 4-channel image.\n     *  The caller shall consider it \"read only\" and make a copy/clone of it before further processing.\n     *  \n     *  The call might block for timeout [milliseconds].\n     * @return the image or null if there is no new image\n     */\n     public IplImage grab_RGB4() {\n\n        if (camera.getCameraFrame(ps3_frame, timeout)) {\n            timestamp = System.nanoTime()/1000;\n            image_4ch.getIntBuffer().put(ps3_frame);\n            return image_4ch;\n        }\n        else return null;\n    }\n\n    /** Grab one frame;\n     *  the caller have to make a copy of returned image before processing.\n     *  \n     *  It will throw null pointer exception if not started before grabbing.\n     *  @return \"read-only\" RGB, 4-channel or GRAY/1-channel image, it throws exception if no image is available\n     */\n    @Override\n    public Frame grab() throws Exception {\n        IplImage img = null;\n        switch (triggered) {\n            case NO_TRIGGER:\n                img = grab_RGB4();\n                break;\n            case HAS_FRAME:\n                triggered = Triggered.NO_TRIGGER;\n                img = makeImage(ps3_frame);\n                break;\n            case NO_FRAME:     \n                triggered = Triggered.NO_TRIGGER;\n                return null;\n            default:  // just schizophrenia - for future enhancement\n                throw new Exception(\"Int. error - unknown triggering state\");\n        }\n        if ((img != null) && (imageMode == ImageMode.GRAY)) {\n                cvCvtColor(img, image_1ch, CV_RGB2GRAY);\n                img = image_1ch;\n        }\n        return converter.convert(img);\n    }\n\n\n    /** Start camera first (before grabbing).\n     * \n     * @return success/failure (true/false)\n     */\n    public void start() throws Exception {\n        boolean b;\n\n        if (ps3_frame == null) {\n            ps3_frame = new int[ imageWidth * imageHeight ];\n            image_4ch = IplImage.create(imageWidth, imageHeight, IPL_DEPTH_8U, 4);\n            image_1ch = IplImage.create(imageWidth, imageHeight, IPL_DEPTH_8U, 1);\n        }\n   \n        b = camera.createCamera(\n                 cameraIndex,\n                (imageMode == ImageMode.GRAY) ? CLCamera.CLEYE_MONO_PROCESSED : CLCamera.CLEYE_COLOR_PROCESSED,\n                (imageWidth == 320 && imageHeight == 240) ? CLCamera.CLEYE_QVGA : CLCamera.CLEYE_VGA,\n                (int)frameRate);\n        \n        if (!b) throw new Exception(\"Low level createCamera() failed\");\n        \n        b = camera.startCamera();\n        if (!b) throw new Exception(\"Camera start() failed\");\n        stat = \"started\";\n    }\n\n\n    /** Stop camera. It can be re-started if needed.\n     * \n     * @return success/failure (true/false)\n     */\n    public void stop() throws Exception {\n        boolean b = camera.stopCamera();\n        if (b) stat = \"stopped\";\n        else throw new Exception(\"Camera stop() failed\");\n    }\n\n\n    /** Release resources:\n     *   - CL driver internal resources binded with camera HW\n     *   - internal IplImage\n     *  After calling this function, this mini-grabber object instance can not be used anymore.\n     */\n    public void release() {\n        if (camera != null) {\n            camera.dispose();\n            camera = null;\n        }\n\n        if (image_4ch != null) {\n            image_4ch.release();\n            image_4ch = null;\n        }\n\n        if (image_1ch != null) {\n            image_1ch.release();\n            image_1ch = null;\n        }\n\n        if (ipl_frame != null) ipl_frame = null;\n        if (ps3_frame != null) ps3_frame = null;\n\n        stat = \"released\";\n    }\n\n    /** Release internal resources, the same as calling release()\n     */\n    public void dispose() {\n        release();\n    }\n\n    @Override protected void finalize() throws Throwable {\n        super.finalize();\n        release();\n    }\n\n\n    /** Return internal CLCamera object, mainly to set camera parameters,\n     *  changing camera parameters must be done on stopped camera and before start() is called.\n     *  See CL SDK - setCameraParameter(int param, int val) function.\n     *  \n     * @return internal CLCamera instance\n     */\n    public CLCamera getCamera() { return camera; }\n\n    public String getUUID() { return uuid; }    \n\n    /**\n     * @return status and camera parameters of the grabber\n     */\n    @Override public String toString() {\n        return \"UUID=\"+uuid + \"; status=\" + stat + \"; timeout=\" + timeout\n            + \"; \"\n            + ((camera != null) ? camera.toString() : \"<no camera>\")\n            ;\n    }\n\n\n    /** Just for testing - loads the CL CLEyeMulticam.dll file, invokes driver\n     *  and lists available cameras. \n     *   \n     * @param argv - argv is not used\n     */\n    public static void main(String[] argv) {\n        String[] uuids = listPS3Cameras();\n        for (int i = 0; i < uuids.length; i++)\n            System.out.println(i+\": \"+uuids[i]);\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport cl.eye.CLCamera;\n\nimport java.io.File;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport org.bytedeco.opencv.opencv_imgproc;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_imgproc;\n\npublic class PS3EyeFrameGrabber extends FrameGrabber {\n    static public String getDeviceDescriptions()throws Exception;\n    static public PS3EyeFrameGrabber createDefault(File deviceFile)throws Exception;\n    static public PS3EyeFrameGrabber createDefault(String devicePath)throws Exception;\n    static public PS3EyeFrameGrabber createDefault(int deviceNumber)throws Exception;\n    static private Exception loadingException;\n    static public  tryLoad()throws Exception;\n    CLCamera camera;\n    int cameraIndex;\n    int[] ps3_frame;\n    byte[] ipl_frame;\n    IplImage image_4ch;\n    IplImage image_1ch;\n    FrameConverter converter;\n    String stat;\n    String uuid;\n    protected enum Triggered{NO_TRIGGER, HAS_FRAME, NO_FRAME}\n    protected Triggered triggered;\n    public PS3EyeFrameGrabber();\n    public PS3EyeFrameGrabber(int cameraIndex);\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate);\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate, Object applet);\n    static public int getCameraCount();\n    static public String listPS3Cameras();\n    public IplImage makeImage(int[] frame);\n    public int[] grab_raw();\n    public  trigger()throws Exception;\n    public IplImage grab_RGB4();\n    public Frame grab()throws Exception;\n    public  start()throws Exception;\n    public  stop()throws Exception;\n    public  release();\n    public  dispose();\n    protected  finalize()throws Throwable;\n    public CLCamera getCamera();\n    public String getUUID();\n    public String toString();\n    static public  main(String argv);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "PS3EyeFrameGrabber.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/PS3EyeFrameGrabber.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/** Grab one frame and return it as int[] (in the internal camera format RGBA).\n     *  Note: use makeImage() to create RGBA, 4-ch image\n     * @return frame as int[] without any processing or null if frame is not available \n     */",
        "source_code": "\npublic int[] grab_raw() {\n    if (camera.getCameraFrame(ps3_frame, timeout)) {\n        return ps3_frame;\n    }\n    else return null;\n}\n",
        "class_name": "PS3EyeFrameGrabber",
        "method_name": "grab_raw",
        "argument_name": [],
        "full_context": "/*\n * Copyright (C) 2011-2012 Jiri Masa, Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.bytedeco.javacv;\n\nimport cl.eye.CLCamera;\nimport java.io.File;\n\nimport org.bytedeco.opencv.opencv_core.*;\nimport org.bytedeco.opencv.opencv_imgproc.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\n\n/** Minimal Sony PS3 Eye camera grabber implementation.\n * \n *  It allows grabbing of frames at higher speed than OpenCVFrameGrabber or VideoInputFrameGrabber.\n *  Underlying implementation of last two grabbers is limited to 30 FPS. PS3 allows grabbing\n *  at maximum speed of 75 FPS in VGA and 187 FPS in QVGA modes.\n *  \n *  This code was developed and tested with CLEyeMulticam.dll, version 1.2.0.1008. The dll library\n *  is part of Code Laboratories CL-Eye Platform SDK and is distributed as part of CLEyeMulticam\n *  Redistributable Dynamic Link Library. For license, download and installation see http://www.codelaboratories.com.\n *    \n *  The grab() method returns an internal instance of IplImage image with fresh camera frame. This returned image\n *  have to be considered \"read only\" and the caller needs to create it's own copy or clone that image.\n *  Calling of release() method for this image shall be avoided.\n *  Based on used resolution the image is in format either 640x480 or 320x240, IPL_DEPTH_8U, 4 channel (color) or 1 channel (gray).\n *  timestamp is set to actual value of System.nanoTime()/1000 obtained after return from the CL driver.\n *  \n *  Typical use case scenario:\n *     create new instance of PS3MiniGrabber\n *     set camera parameters\n *     start() grabber \n *     wait at least 2 frames\n *     grab() in loop\n *     stop() grabber\n *     release() internal resources\n *     \n * Note:\n * This code depends on the cl.eye.CLCamera class from Code Laboratories CL-Eye\n * Platform SDK. It is suggested to download SDK and edit the sample file\n * ....\\cl\\eye\\CLCamera.java. A few references to processing.core.PApplet class\n * shall be removed and the file recompiled. The tailored file is not included\n * here namely because of unclear licence.\n * \n *  @author jmasa, jmasa@cmail.cz\n *\n */\npublic class PS3EyeFrameGrabber extends FrameGrabber {\n    public static String[] getDeviceDescriptions() throws Exception {\n        tryLoad();\n        String[] descriptions = new String[CLCamera.cameraCount()];\n        for (int i = 0; i < descriptions.length; i++) {\n            descriptions[i] = CLCamera.cameraUUID(i);\n        }\n        return descriptions;\n    }\n\n    public static PS3EyeFrameGrabber createDefault(File deviceFile)   throws Exception { throw new Exception(PS3EyeFrameGrabber.class + \" does not support device files.\"); }\n    public static PS3EyeFrameGrabber createDefault(String devicePath) throws Exception { throw new Exception(PS3EyeFrameGrabber.class + \" does not support device paths.\"); }\n    public static PS3EyeFrameGrabber createDefault(int deviceNumber)  throws Exception { return new PS3EyeFrameGrabber(deviceNumber); }\n\n    private static Exception loadingException = null;\n    public static void tryLoad() throws Exception {\n        if (loadingException != null) {\n            throw loadingException;\n        } else {\n            try {\n                CLCamera.IsLibraryLoaded();\n            } catch (Throwable t) {\n                throw loadingException = new Exception(\"Failed to load \" + PS3EyeFrameGrabber.class, t);\n            }\n        }\n    }\n\n    CLCamera camera;\n    int cameraIndex = 0;\n    int[]  ps3_frame = null;             // buffer for PS3 camera frame data\n    byte[] ipl_frame = null;             // buffer for RGB-3ch, not allocated unless grab_RGB3() is called \n\n    IplImage image_4ch = null;\n    IplImage image_1ch = null;\n    FrameConverter converter = new OpenCVFrameConverter.ToIplImage();\n\n    String stat;                  // status of PS3 camera handling - mostly for debugging\n    String uuid;                  // assigned camera unique key\n\n    // variables for trigger() implementation\n    //\n    protected enum Triggered {NO_TRIGGER, HAS_FRAME, NO_FRAME};\n    protected Triggered triggered = Triggered.NO_TRIGGER;\n\n\n    /** Default grabber, camera idx = 0, color mode, VGA resolution, 60 FPS frame rate.\n     *   \n     */\n    public PS3EyeFrameGrabber() throws Exception {\n        this(0);\n    }\n\n    /** Color mode, VGA resolution, 60 FPS frame rate.\n     *   @param system wide camera index\n     */\n    public PS3EyeFrameGrabber(int cameraIndex) throws Exception {\n        this(cameraIndex, 640, 480, 60);\n    }\n\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate) throws Exception {\n        this(cameraIndex, 640, 480, 60, null);\n    }\n\n    /** Creates grabber, the caller can control basic image and grabbing parameters.\n     * \n     * @param cameraIndex - zero based index of used camera (OS system wide)\n     * @param imageWidth  - width of image \n     * @param imageHeight - height of image\n     * @param framerate   - frame rate - see CLCamera for allowed frame rates based on resolution\n     * @param applet      - PApplet object required by CLCamera\n     * @throws Exception  - if parameters don't follow CLCamera definition or camera is not created\n     */\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate, Object applet) throws Exception {\n        camera = null;\n\n        if (! CLCamera.IsLibraryLoaded()) {\n            throw new Exception(\"CLEye multicam dll not loaded\");\n        }\n\n        this.camera = new CLCamera();\n        this.cameraIndex = cameraIndex;\n\n        stat = \"created\";\n        uuid = CLCamera.cameraUUID(cameraIndex);\n\n        if (((imageWidth == 640) && (imageHeight == 480)) || \n            ((imageWidth == 320) && (imageHeight == 240))) {\n            setImageWidth(imageWidth);\n            setImageHeight(imageHeight);\n        }\n        else throw new Exception(\"Only 640x480 or 320x240 images supported\");\n\n        setImageMode(ImageMode.COLOR);\n        setFrameRate((double) framerate);  \n        setTimeout(1 + 1000/framerate);\n        setBitsPerPixel(8);\n        setTriggerMode(false);\n        setNumBuffers(4);\n    }\n\n    /** \n     * @return system wide number of installed/detected Sony PS3 Eye cameras\n     */\n    public static int getCameraCount() {\n        return CLCamera.cameraCount();\n    }\n\n    /** Ask the driver for all installed PS3 cameras. Resulting array is sorted in order of camera index.\n     *  Its size is defined by CLCamera.cameraCount().\n     * \n     * @return array of camera unique uuids or null if there is no PS3 camera\n     */\n    public static String[] listPS3Cameras() {\n        int no = getCameraCount();\n        String[] uuids;\n        if (no > 0) {\n            uuids = new String[no];\n            for (--no; no >=0; no--) { uuids[no] = CLCamera.cameraUUID(no); }\n            return uuids;\n        }\n        return null;\n    }\n\n\n    /** Make IplImage form raw int[] frame data\n     *  Note: NO array size checks!!\n     * \n     * @param frame int[] image frame data \n     * @return internal IplImage set to frame\n     */\n    public IplImage makeImage(int[] frame) {\n        image_4ch.getIntBuffer().put(ps3_frame);\n        return image_4ch;\n    }\n\n\n    /** Grab one frame and return it as int[] (in the internal camera format RGBA).\n     *  Note: use makeImage() to create RGBA, 4-ch image\n     * @return frame as int[] without any processing or null if frame is not available \n     */\n    public int[] grab_raw() {\n        if (camera.getCameraFrame(ps3_frame, timeout)) {\n            return ps3_frame;\n        }\n        else return null;\n    }\n\n    public void trigger() throws Exception {\n        for (int i = 0; i < numBuffers+1; i++) {\n            grab_raw();\n        }\n\n        if ((ps3_frame = grab_raw()) != null) {\n            triggered = Triggered.HAS_FRAME;\n            timestamp = System.nanoTime()/1000;\n        }\n        else\n            triggered = Triggered.NO_FRAME;\n    }\n\n\n    /** Grab and convert one frame, default timeout is (1 + 1000/framerate) [milliseconds].\n     *  Every successful call returns an internal (preallocated) 640x480 or 320x240, IPL_DEPTH_8U, 4-channel image.\n     *  The caller shall consider it \"read only\" and make a copy/clone of it before further processing.\n     *  \n     *  The call might block for timeout [milliseconds].\n     * @return the image or null if there is no new image\n     */\n     public IplImage grab_RGB4() {\n\n        if (camera.getCameraFrame(ps3_frame, timeout)) {\n            timestamp = System.nanoTime()/1000;\n            image_4ch.getIntBuffer().put(ps3_frame);\n            return image_4ch;\n        }\n        else return null;\n    }\n\n    /** Grab one frame;\n     *  the caller have to make a copy of returned image before processing.\n     *  \n     *  It will throw null pointer exception if not started before grabbing.\n     *  @return \"read-only\" RGB, 4-channel or GRAY/1-channel image, it throws exception if no image is available\n     */\n    @Override\n    public Frame grab() throws Exception {\n        IplImage img = null;\n        switch (triggered) {\n            case NO_TRIGGER:\n                img = grab_RGB4();\n                break;\n            case HAS_FRAME:\n                triggered = Triggered.NO_TRIGGER;\n                img = makeImage(ps3_frame);\n                break;\n            case NO_FRAME:     \n                triggered = Triggered.NO_TRIGGER;\n                return null;\n            default:  // just schizophrenia - for future enhancement\n                throw new Exception(\"Int. error - unknown triggering state\");\n        }\n        if ((img != null) && (imageMode == ImageMode.GRAY)) {\n                cvCvtColor(img, image_1ch, CV_RGB2GRAY);\n                img = image_1ch;\n        }\n        return converter.convert(img);\n    }\n\n\n    /** Start camera first (before grabbing).\n     * \n     * @return success/failure (true/false)\n     */\n    public void start() throws Exception {\n        boolean b;\n\n        if (ps3_frame == null) {\n            ps3_frame = new int[ imageWidth * imageHeight ];\n            image_4ch = IplImage.create(imageWidth, imageHeight, IPL_DEPTH_8U, 4);\n            image_1ch = IplImage.create(imageWidth, imageHeight, IPL_DEPTH_8U, 1);\n        }\n   \n        b = camera.createCamera(\n                 cameraIndex,\n                (imageMode == ImageMode.GRAY) ? CLCamera.CLEYE_MONO_PROCESSED : CLCamera.CLEYE_COLOR_PROCESSED,\n                (imageWidth == 320 && imageHeight == 240) ? CLCamera.CLEYE_QVGA : CLCamera.CLEYE_VGA,\n                (int)frameRate);\n        \n        if (!b) throw new Exception(\"Low level createCamera() failed\");\n        \n        b = camera.startCamera();\n        if (!b) throw new Exception(\"Camera start() failed\");\n        stat = \"started\";\n    }\n\n\n    /** Stop camera. It can be re-started if needed.\n     * \n     * @return success/failure (true/false)\n     */\n    public void stop() throws Exception {\n        boolean b = camera.stopCamera();\n        if (b) stat = \"stopped\";\n        else throw new Exception(\"Camera stop() failed\");\n    }\n\n\n    /** Release resources:\n     *   - CL driver internal resources binded with camera HW\n     *   - internal IplImage\n     *  After calling this function, this mini-grabber object instance can not be used anymore.\n     */\n    public void release() {\n        if (camera != null) {\n            camera.dispose();\n            camera = null;\n        }\n\n        if (image_4ch != null) {\n            image_4ch.release();\n            image_4ch = null;\n        }\n\n        if (image_1ch != null) {\n            image_1ch.release();\n            image_1ch = null;\n        }\n\n        if (ipl_frame != null) ipl_frame = null;\n        if (ps3_frame != null) ps3_frame = null;\n\n        stat = \"released\";\n    }\n\n    /** Release internal resources, the same as calling release()\n     */\n    public void dispose() {\n        release();\n    }\n\n    @Override protected void finalize() throws Throwable {\n        super.finalize();\n        release();\n    }\n\n\n    /** Return internal CLCamera object, mainly to set camera parameters,\n     *  changing camera parameters must be done on stopped camera and before start() is called.\n     *  See CL SDK - setCameraParameter(int param, int val) function.\n     *  \n     * @return internal CLCamera instance\n     */\n    public CLCamera getCamera() { return camera; }\n\n    public String getUUID() { return uuid; }    \n\n    /**\n     * @return status and camera parameters of the grabber\n     */\n    @Override public String toString() {\n        return \"UUID=\"+uuid + \"; status=\" + stat + \"; timeout=\" + timeout\n            + \"; \"\n            + ((camera != null) ? camera.toString() : \"<no camera>\")\n            ;\n    }\n\n\n    /** Just for testing - loads the CL CLEyeMulticam.dll file, invokes driver\n     *  and lists available cameras. \n     *   \n     * @param argv - argv is not used\n     */\n    public static void main(String[] argv) {\n        String[] uuids = listPS3Cameras();\n        for (int i = 0; i < uuids.length; i++)\n            System.out.println(i+\": \"+uuids[i]);\n    }\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport cl.eye.CLCamera;\n\nimport java.io.File;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport org.bytedeco.opencv.opencv_imgproc;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_imgproc;\n\npublic class PS3EyeFrameGrabber extends FrameGrabber {\n    static public String getDeviceDescriptions()throws Exception;\n    static public PS3EyeFrameGrabber createDefault(File deviceFile)throws Exception;\n    static public PS3EyeFrameGrabber createDefault(String devicePath)throws Exception;\n    static public PS3EyeFrameGrabber createDefault(int deviceNumber)throws Exception;\n    static private Exception loadingException;\n    static public  tryLoad()throws Exception;\n    CLCamera camera;\n    int cameraIndex;\n    int[] ps3_frame;\n    byte[] ipl_frame;\n    IplImage image_4ch;\n    IplImage image_1ch;\n    FrameConverter converter;\n    String stat;\n    String uuid;\n    protected enum Triggered{NO_TRIGGER, HAS_FRAME, NO_FRAME}\n    protected Triggered triggered;\n    public PS3EyeFrameGrabber();\n    public PS3EyeFrameGrabber(int cameraIndex);\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate);\n    public PS3EyeFrameGrabber(int cameraIndex, int imageWidth, int imageHeight, int framerate, Object applet);\n    static public int getCameraCount();\n    static public String listPS3Cameras();\n    public IplImage makeImage(int[] frame);\n    public int[] grab_raw();\n    public  trigger()throws Exception;\n    public IplImage grab_RGB4();\n    public Frame grab()throws Exception;\n    public  start()throws Exception;\n    public  stop()throws Exception;\n    public  release();\n    public  dispose();\n    protected  finalize()throws Throwable;\n    public CLCamera getCamera();\n    public String getUUID();\n    public String toString();\n    static public  main(String argv);\n}\n\n"
    },
    {
        "project_name": "javacv",
        "file_name": "RealSenseFrameGrabber.java",
        "relative_path": "javacv/src/main/java/org/bytedeco/javacv/RealSenseFrameGrabber.java",
        "execute_path": "javacv",
        "package": "org.bytedeco.javacv",
        "docstring": "/**\n     *\n     * @return null grabs all images, get them with grabColor, grabDepth, and\n     * grabIR instead.\n     * @throws org.bytedeco.javacv.FrameGrabber.Exception\n     */",
        "source_code": "\npublic Frame grab() throws Exception {\n    device.wait_for_frames();\n\n      frameNumber++; \n    // For Framegrabber\n    if (colorEnabled && behaveAsColorFrameGrabber) {\n        IplImage image = grabVideo();\n\n        if (returnImage == null) {\n            int deviceWidth = device.get_stream_width(RealSense.color);\n            int deviceHeight = device.get_stream_height(RealSense.color);\n              returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 3);\n            returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 1);\n        }\n        cvCvtColor(image, returnImage, CV_BGR2GRAY);\n        return converter.convert(returnImage);\n    } else {\n        if (IREnabled) {\n            return converter.convert(grabIR());\n        } else {\n            if (depthEnabled) {\n                \n                // Fake colors\n                IplImage image = grabDepth();\n                if (returnImage == null) {\n                    int deviceWidth = device.get_stream_width(RealSense.depth);\n                    int deviceHeight = device.get_stream_height(RealSense.depth);\n              returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 3);\n                    returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 1);\n                } \n                return converter.convert(returnImage);\n            }\n        }\n    }\n\n    return null;\n}\n",
        "class_name": "RealSenseFrameGrabber",
        "method_name": "grab",
        "argument_name": [],
        "full_context": "/*\n * Copyright (C) 2014 Jeremy Laviole, Samuel Audet\n *\n * Licensed either under the Apache License, Version 2.0, or (at your option)\n * under the terms of the GNU General Public License as published by\n * the Free Software Foundation (subject to the \"Classpath\" exception),\n * either version 2, or any later version (collectively, the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *     http://www.gnu.org/licenses/\n *     http://www.gnu.org/software/classpath/license.html\n *\n * or as provided in the LICENSE.txt file that accompanied this code.\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.bytedeco.javacv;\n\nimport java.io.File;\nimport java.nio.ByteBuffer;\nimport java.nio.ByteOrder;\nimport java.nio.ShortBuffer;\nimport org.bytedeco.javacpp.BytePointer;\nimport org.bytedeco.javacpp.Loader;\nimport org.bytedeco.javacpp.Pointer;\n\nimport org.bytedeco.librealsense.*;\nimport org.bytedeco.librealsense.global.RealSense;\nimport org.bytedeco.opencv.opencv_core.*;\nimport org.bytedeco.opencv.opencv_imgproc.*;\nimport static org.bytedeco.opencv.global.opencv_core.*;\nimport static org.bytedeco.opencv.global.opencv_imgproc.*;\n\n/**\n *\n * @author Jeremy Laviole\n */\npublic class RealSenseFrameGrabber extends FrameGrabber {\n\n    public static String[] getDeviceDescriptions() throws FrameGrabber.Exception {\n        tryLoad();\n        String[] desc = new String[context.get_device_count()];\n        for (int i = 0; i < desc.length; i++) {\n            desc[i] = context.get_device(i).get_name().getString();\n        }\n        return desc;\n    }\n\n    public static int DEFAULT_DEPTH_WIDTH = 640;\n    public static int DEFAULT_DEPTH_HEIGHT = 480;\n    public static int DEFAULT_COLOR_WIDTH = 1280;\n    public static int DEFAULT_COLOR_HEIGHT = 720;\n    public static int DEFAULT_COLOR_FRAMERATE = 30;\n\n    private ByteOrder byteOrder = ByteOrder.BIG_ENDIAN;\n    private int depthImageWidth = DEFAULT_DEPTH_WIDTH;\n    private int depthImageHeight = DEFAULT_DEPTH_HEIGHT;\n    private int depthFrameRate = 30;\n\n    private int IRImageWidth = DEFAULT_DEPTH_WIDTH;\n    private int IRImageHeight = DEFAULT_DEPTH_HEIGHT;\n    private int IRFrameRate = 30;\n\n    public ByteOrder getByteOrder() {\n        return byteOrder;\n    }\n\n    public void setByteOrder(ByteOrder byteOrder) {\n        this.byteOrder = byteOrder;\n    }\n\n    public static RealSenseFrameGrabber createDefault(int deviceNumber) throws FrameGrabber.Exception {\n        return new RealSenseFrameGrabber(deviceNumber);\n    }\n\n    public static RealSenseFrameGrabber createDefault(File deviceFile) throws Exception {\n        throw new Exception(RealSenseFrameGrabber.class + \" does not support File devices.\");\n    }\n\n    public static RealSenseFrameGrabber createDefault(String devicePath) throws Exception {\n        throw new Exception(RealSenseFrameGrabber.class + \" does not support path.\");\n    }\n\n    private static FrameGrabber.Exception loadingException = null;\n\n    public static void tryLoad() throws FrameGrabber.Exception {\n        if (loadingException != null) {\n            loadingException.printStackTrace();\n            throw loadingException;\n        } else {\n            try {\n                if (context != null) {\n                    return;\n                }\n                Loader.load(RealSense.class);\n\n                // Context is shared accross cameras. \n                context = new context();\n                System.out.println(\"RealSense devices found: \" + context.get_device_count());\n            } catch (Throwable t) {\n                throw loadingException = new FrameGrabber.Exception(\"Failed to load \" + RealSenseFrameGrabber.class, t);\n            }\n        }\n    }\n\n    private static context context = null;\n    private int deviceNumber = 0;\n    private device device = null;\n    private static device globalDevice = null;\n    private boolean depth = false; // default to \"video\"\n    private boolean colorEnabled = false;\n    private boolean depthEnabled = false;\n    private boolean IREnabled = false;\n    private FrameConverter converter = new OpenCVFrameConverter.ToIplImage();\n\n    public RealSenseFrameGrabber(int deviceNumber) {\n        this.deviceNumber = deviceNumber;\n    }\n\n    public static void main(String[] args) {\n        context context = new context();\n        System.out.println(\"Devices found: \" + context.get_device_count());\n        device device = context.get_device(0);\n        System.out.println(\"Using device 0, an \" + device.get_name());\n        System.out.println(\" Serial number: \" + device.get_serial());\n    }\n\n    public void enableColorStream() {\n        if (!colorEnabled) {\n            if (imageWidth == 0) {\n                imageWidth = DEFAULT_COLOR_WIDTH;\n            }\n            if (imageHeight == 0) {\n                imageHeight = DEFAULT_COLOR_HEIGHT;\n            }\n            if (frameRate == 0) {\n                frameRate = DEFAULT_COLOR_FRAMERATE;\n            }\n            colorEnabled = true;\n        }\n    }\n\n    public void disableColorStream() {\n        if (colorEnabled) {\n            device.disable_stream(RealSense.color);\n            colorEnabled = false;\n        }\n    }\n\n    public void enableDepthStream() {\n        if (!depthEnabled) {\n\n            depthEnabled = true;\n        }\n    }\n\n    public void disableDepthStream() {\n        if (depthEnabled) {\n            device.disable_stream(RealSense.depth);\n            depthEnabled = false;\n        }\n    }\n\n    public void enableIRStream() {\n        if (!IREnabled) {\n\n            IREnabled = true;\n        }\n    }\n\n    public void disableIRStream() {\n        if (IREnabled) {\n            device.disable_stream(RealSense.infrared);\n            IREnabled = false;\n        }\n    }\n\n    public void release() throws FrameGrabber.Exception {\n    }\n\n    @Override\n    protected void finalize() throws Throwable {\n        super.finalize();\n        release();\n    }\n\n    /**\n     * Warning can lead to unsafe situations.\n     *\n     * @return\n     */\n    public device getRealSenseDevice() {\n        return this.device;\n    }\n\n    public float getDepthScale() {\n        return device.get_depth_scale();\n    }\n\n    @Override\n    public double getFrameRate() {  // TODO: check this. \n        return super.getFrameRate();\n    }\n\n    private boolean startedOnce = false;\n    private boolean behaveAsColorFrameGrabber = false;\n\n    public device loadDevice() throws FrameGrabber.Exception {\n        if (context == null) {\n            context = new context();\n        }\n        if (context == null || context.get_device_count() <= deviceNumber) {\n            throw new Exception(\"FATAL error: Realsense camera: \" + deviceNumber + \" not connected/found\");\n        }\n        device = context.get_device(deviceNumber);\n        return device;\n    }\n\n    @Override\n    public void start() throws FrameGrabber.Exception {\n\n        // There is already a device, we reboot everything (for Procamcalib).\n        if (globalDevice != null) {\n            globalDevice.close();\n            context.close();\n            globalDevice = null;\n            context = null;\n        }\n\n        if (context == null) {\n            context = new context();\n        }\n        if (context == null || context.get_device_count() <= deviceNumber) {\n            throw new Exception(\"FATAL error: Realsense camera: \" + deviceNumber + \" not connected/found\");\n        }\n\n        if (device == null) {\n            device = context.get_device(deviceNumber);\n        }\n        globalDevice = device;\n\n        // Choose the camera to enable by format.\n        if (format != null) {\n            switch (format) {\n                case \"rgb\":\n                    this.enableColorStream();\n                    break;\n                case \"ir\":\n                    this.enableIRStream();\n                    break;\n                case \"depth\":\n                    this.enableDepthStream();\n                    break;\n            }\n        }\n\n        if (colorEnabled) {\n            device.enable_stream(RealSense.color, imageWidth, imageHeight, RealSense.rgb8, (int) frameRate);\n        }\n        if (IREnabled) {\n            device.enable_stream(RealSense.infrared, IRImageWidth, IRImageHeight, RealSense.y8, IRFrameRate);\n        }\n        if (depthEnabled) {\n            device.enable_stream(RealSense.depth, depthImageWidth, depthImageHeight, RealSense.z16, depthFrameRate);\n        }\n        // if no stream is select, just get the color.\n        if (!colorEnabled && !IREnabled && !depthEnabled) {\n            enableColorStream();\n            device.enable_stream(RealSense.color, imageWidth, imageHeight, RealSense.rgb8, (int) frameRate);\n            behaveAsColorFrameGrabber = true;\n        }\n        device.start();\n    }\n\n    /**\n     *\n     * @throws Exception\n     */\n    @Override\n    public void stop() throws FrameGrabber.Exception {\n        device.stop();\n//        colorEnabled = false;\n//        IREnabled = false;\n//        depthEnabled = false;\n        frameNumber = 0;\n    }\n    private Pointer rawDepthImageData = new Pointer((Pointer) null),\n            rawVideoImageData = new Pointer((Pointer) null),\n            rawIRImageData = new Pointer((Pointer) null);\n    private IplImage rawDepthImage = null, rawVideoImage = null, rawIRImage = null, returnImage = null;\n\n    public IplImage grabDepth() {\n\n        if (!depthEnabled) {\n            System.out.println(\"Depth stream not enabled, impossible to get the image.\");\n            return null;\n        }\n        rawDepthImageData = device.get_frame_data(RealSense.depth);\n//        ShortBuffer bb = data.position(0).limit(640 * 480 * 2).asByteBuffer().asShortBuffer();\n\n        int iplDepth = IPL_DEPTH_16U, channels = 1;\n        int deviceWidth = device.get_stream_width(RealSense.depth);\n        int deviceHeight = device.get_stream_height(RealSense.depth);\n\n        // AUTOMATIC\n//        int deviceWidth = 0;\n//        int deviceHeight = 0;\n        if (rawDepthImage == null || rawDepthImage.width() != deviceWidth || rawDepthImage.height() != deviceHeight) {\n            rawDepthImage = IplImage.createHeader(deviceWidth, deviceHeight, iplDepth, channels);\n        }\n\n        cvSetData(rawDepthImage, rawDepthImageData, deviceWidth * channels * iplDepth / 8);\n\n//        if (iplDepth > 8 && !ByteOrder.nativeOrder().equals(byteOrder)) {\n//            // ack, the camera's endianness doesn't correspond to our machine ...\n//            // swap bytes of 16-bit images\n//            ByteBuffer bb = rawDepthImage.getByteBuffer();\n//            ShortBuffer in = bb.order(ByteOrder.BIG_ENDIAN).asShortBuffer();\n//            ShortBuffer out = bb.order(ByteOrder.LITTLE_ENDIAN).asShortBuffer();\n//            out.put(in);\n//        }\n        return rawDepthImage;\n    }\n\n    public IplImage grabVideo() {\n\n        if (!colorEnabled) {\n            System.out.println(\"Color stream not enabled, impossible to get the image.\");\n            return null;\n        }\n\n        int iplDepth = IPL_DEPTH_8U, channels = 3;\n\n        rawVideoImageData = device.get_frame_data(RealSense.color);\n        int deviceWidth = device.get_stream_width(RealSense.color);\n        int deviceHeight = device.get_stream_height(RealSense.color);\n\n        if (rawVideoImage == null || rawVideoImage.width() != deviceWidth || rawVideoImage.height() != deviceHeight) {\n            rawVideoImage = IplImage.createHeader(deviceWidth, deviceHeight, iplDepth, channels);\n        }\n\n        cvSetData(rawVideoImage, rawVideoImageData, deviceWidth * channels * iplDepth / 8);\n\n//        if (iplDepth > 8 && !ByteOrder.nativeOrder().equals(byteOrder)) {\n//            // ack, the camera's endianness doesn't correspond to our machine ...\n//            // swap bytes of 16-bit images\n//            ByteBuffer bb = rawVideoImage.getByteBuffer();\n//            ShortBuffer in = bb.order(ByteOrder.BIG_ENDIAN).asShortBuffer();\n//            ShortBuffer out = bb.order(ByteOrder.LITTLE_ENDIAN).asShortBuffer();\n//            out.put(in);\n//        }\n//        if (channels == 3) {\n//            cvCvtColor(rawVideoImage, rawVideoImage, CV_BGR2RGB);\n//        }\n        return rawVideoImage;\n    }\n\n    public IplImage grabIR() {\n\n        if (!IREnabled) {\n            System.out.println(\"IR stream not enabled, impossible to get the image.\");\n            return null;\n        }\n\n        int iplDepth = IPL_DEPTH_8U, channels = 1;\n\n        rawIRImageData = device.get_frame_data(RealSense.infrared);\n\n        int deviceWidth = device.get_stream_width(RealSense.infrared);\n        int deviceHeight = device.get_stream_height(RealSense.infrared);\n\n        if (rawIRImage == null || rawIRImage.width() != deviceWidth || rawIRImage.height() != deviceHeight) {\n            rawIRImage = IplImage.createHeader(deviceWidth, deviceHeight, iplDepth, channels);\n        }\n        cvSetData(rawIRImage, rawIRImageData, deviceWidth * channels * iplDepth / 8);\n\n//        if (iplDepth > 8 && !ByteOrder.nativeOrder().equals(byteOrder)) {\n//            // ack, the camera's endianness doesn't correspond to our machine ...\n//            // swap bytes of 16-bit images\n//            ByteBuffer bb = rawIRImage.getByteBuffer();\n//            ShortBuffer in = bb.order(ByteOrder.BIG_ENDIAN).asShortBuffer();\n//            ShortBuffer out = bb.order(ByteOrder.LITTLE_ENDIAN).asShortBuffer();\n//            out.put(in);\n//        }\n        return rawIRImage;\n    }\n\n    /**\n     *\n     * @return null grabs all images, get them with grabColor, grabDepth, and\n     * grabIR instead.\n     * @throws org.bytedeco.javacv.FrameGrabber.Exception\n     */\n    public Frame grab() throws Exception {\n        device.wait_for_frames();\n\n//        frameNumber++; \n        // For Framegrabber\n        if (colorEnabled && behaveAsColorFrameGrabber) {\n            IplImage image = grabVideo();\n\n            if (returnImage == null) {\n                int deviceWidth = device.get_stream_width(RealSense.color);\n                int deviceHeight = device.get_stream_height(RealSense.color);\n//                returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 3);\n                returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 1);\n            }\n            cvCvtColor(image, returnImage, CV_BGR2GRAY);\n            return converter.convert(returnImage);\n        } else {\n            if (IREnabled) {\n                return converter.convert(grabIR());\n            } else {\n                if (depthEnabled) {\n                    \n                    // Fake colors\n                    IplImage image = grabDepth();\n                    if (returnImage == null) {\n                        int deviceWidth = device.get_stream_width(RealSense.depth);\n                        int deviceHeight = device.get_stream_height(RealSense.depth);\n//                returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 3);\n                        returnImage = IplImage.create(deviceWidth, deviceHeight, IPL_DEPTH_8U, 1);\n                    } \n                    return converter.convert(returnImage);\n                }\n            }\n        }\n\n        return null;\n    }\n\n    @Override\n    public void trigger() throws Exception {\n        device.wait_for_frames();\n    }\n\n    public int getDepthImageWidth() {\n        return depthImageWidth;\n    }\n\n    public void setDepthImageWidth(int depthImageWidth) {\n        this.depthImageWidth = depthImageWidth;\n    }\n\n    public int getDepthImageHeight() {\n        return depthImageHeight;\n    }\n\n    public void setDepthImageHeight(int depthImageHeight) {\n        this.depthImageHeight = depthImageHeight;\n    }\n\n    public int getIRImageWidth() {\n        return IRImageWidth;\n    }\n\n    public void setIRImageWidth(int IRImageWidth) {\n        this.IRImageWidth = IRImageWidth;\n    }\n\n    public int getIRImageHeight() {\n        return IRImageHeight;\n    }\n\n    public void setIRImageHeight(int IRImageHeight) {\n        this.IRImageHeight = IRImageHeight;\n    }\n\n    public int getDepthFrameRate() {\n        return depthFrameRate;\n    }\n\n    public void setDepthFrameRate(int frameRate) {\n        this.depthFrameRate = frameRate;\n    }\n\n    public int getIRFrameRate() {\n        return IRFrameRate;\n    }\n\n    public void setIRFrameRate(int IRFrameRate) {\n        this.IRFrameRate = IRFrameRate;\n    }\n\n    @Override\n    public double getGamma() {\n        // I guess a default gamma of 2.2 is reasonable...\n        if (gamma == 0.0) {\n            return 2.2;\n        } else {\n            return gamma;\n        }\n    }\n\n    // Gamma from the device is not good.\n//    @Override\n//    public double getGamma(){\n//        double gamma = device.get_option(RealSense.RS_OPTION_COLOR_GAMMA);\n//        System.out.println(\"Getting cameraGamma \" + gamma);\n//        return gamma;\n//    }\n// --- Presets --- \n    public void setPreset(int preset) {\n        /* Provide access to several recommend sets of option presets for ivcam */\n        RealSense.apply_ivcam_preset(device, preset);\n    }\n\n    public void setShortRange() {\n        setPreset(RealSense.RS_IVCAM_PRESET_SHORT_RANGE);\n    }\n\n    public void setLongRange() {\n        setPreset(RealSense.RS_IVCAM_PRESET_LONG_RANGE);\n    }\n\n    public void setMidRange() {\n        setPreset(RealSense.RS_IVCAM_PRESET_MID_RANGE);\n    }\n\n    public void setDefaultPreset() {\n        setPreset(RealSense.RS_IVCAM_PRESET_DEFAULT);\n    }\n\n    public void setObjectScanningPreset() {\n        setPreset(RealSense.RS_IVCAM_PRESET_OBJECT_SCANNING);\n    }\n\n    public void setCursorPreset() {\n        setPreset(RealSense.RS_IVCAM_PRESET_GR_CURSOR);\n    }\n\n    public void setGestureRecognitionPreset() {\n        setPreset(RealSense.RS_IVCAM_PRESET_GESTURE_RECOGNITION);\n    }\n\n    public void setBackgroundSegmentationPreset() {\n        setPreset(RealSense.RS_IVCAM_PRESET_BACKGROUND_SEGMENTATION);\n    }\n\n    public void setIROnlyPreset() {\n        setPreset(RealSense.RS_IVCAM_PRESET_IR_ONLY);\n    }\n\n    // ---- Options ---- \n    public void setOption(int option, int value) {\n        device.set_option(option, value);\n    }\n\n    /**\n     * Enable / disable color backlight compensation\n     */\n    public void set(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_BACKLIGHT_COMPENSATION, value);\n    }\n\n    /**\n     * Color image brightness\n     */\n    public void setColorBrightness(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_BRIGHTNESS, value);\n    }\n\n    /**\n     * COLOR image contrast\n     */\n    public void setColorContrast(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_CONTRAST, value);\n    }\n\n    /**\n     * Controls exposure time of color camera. Setting any value will disable\n     * auto exposure\n     */\n    public void setColorExposure(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_EXPOSURE, value);\n    }\n\n    /**\n     * Color image gain\n     */\n    public void setColorGain(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_GAIN, value);\n    }\n\n    /**\n     * Color image gamma setting\n     */\n    public void setColorGamma(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_GAMMA, value);\n    }\n\n    /**\n     * Color image hue\n     */\n    public void setColorHue(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_HUE, value);\n    }\n\n    /**\n     * Color image saturation setting\n     */\n    public void setColorSaturation(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_SATURATION, value);\n    }\n\n    /**\n     * Color image sharpness setting\n     */\n    public void setColorSharpness(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_SHARPNESS, value);\n    }\n\n    /**\n     * Controls white balance of color image. Setting any value will disable\n     * auto white balance\n     */\n    public void setColorWhiteBalance(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_WHITE_BALANCE, value);\n    }\n\n    /**\n     * Enable / disable color image auto-exposure\n     */\n    public void setColorEnableAutoExposure(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_ENABLE_AUTO_EXPOSURE, value);\n    }\n\n    /**\n     * Enable / disable color image auto-white-balance\n     */\n    public void setColorEnableAutoWhiteBalance(int value) {\n        setOption(RealSense.RS_OPTION_COLOR_ENABLE_AUTO_WHITE_BALANCE, value);\n    }\n\n    /**\n     * Power of the F200 / SR300 projector, with 0 meaning projector off\n     */\n    public void setLaserPower(int value) {\n        setOption(RealSense.RS_OPTION_F200_LASER_POWER, value);\n    }\n\n    /**\n     * Set the number of patterns projected per frame. The higher the accuracy\n     * value the more patterns projected. Increasing the number of patterns help\n     * to achieve better accuracy. Note that this control is affecting the Depth\n     * FPS\n     */\n    public void setAccuracy(int value) {\n        setOption(RealSense.RS_OPTION_F200_ACCURACY, value);\n    }\n\n    /**\n     * Motion vs. Range trade-off, with lower values allowing for better motion\n     * sensitivity and higher values allowing for better depth range\n     */\n    public void setMotionRange(int value) {\n        setOption(RealSense.RS_OPTION_F200_MOTION_RANGE, value);\n    }\n\n    /**\n     * Set the filter to apply to each depth frame. Each one of the filter is\n     * optimized per the application requirements\n     */\n    public void setFilterOption(int value) {\n        setOption(RealSense.RS_OPTION_F200_FILTER_OPTION, value);\n    }\n\n    /**\n     * The confidence level threshold used by the Depth algorithm pipe to set\n     * whether a pixel will get a valid range or will be marked with invalid\n     * range\n     */\n    public void setConfidenceThreshold(int value) {\n        setOption(RealSense.RS_OPTION_F200_CONFIDENCE_THRESHOLD, value);\n    }\n\n    /**\n     * (F200-only) Allows to reduce FPS without restarting streaming. Valid\n     * values are {2, 5, 15, 30, 60}\n     */\n    public void setDynamicFPS(int value) {\n        setOption(RealSense.RS_OPTION_F200_DYNAMIC_FPS, value);\n    }\n\n    /**\n     * Enables / disables R200 auto-exposure. This will affect both IR and depth\n     * image.\n     */\n    public void setLR_AutoExposureEnabled(int value) {\n        setOption(RealSense.RS_OPTION_R200_LR_AUTO_EXPOSURE_ENABLED, value);\n    }\n\n    /**\n     * IR image gain\n     */\n    public void setLR_Gain(int value) {\n        setOption(RealSense.RS_OPTION_R200_LR_GAIN, value);\n    }\n\n    /**\n     * This control allows manual adjustment of the exposure time value for the\n     * L/R imagers\n     */\n    public void setLR_Exposure(int value) {\n        setOption(RealSense.RS_OPTION_R200_LR_EXPOSURE, value);\n    }\n\n    /**\n     * Enables / disables R200 emitter\n     */\n    public void setEmitterEnabled(int value) {\n        setOption(RealSense.RS_OPTION_R200_EMITTER_ENABLED, value);\n    }\n\n    /**\n     * Micrometers per increment in integer depth values, 1000 is default (mm\n     * scale). Set before streaming\n     */\n    public void setDepthUnits(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_UNITS, value);\n    }\n\n    /**\n     * Minimum depth in current depth units that will be output. Any values less\n     * than \ufffdMin Depth\ufffd will be mapped to 0 during the conversion between\n     * disparity and depth. Set before streaming\n     */\n    public void setDepthClampMin(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CLAMP_MIN, value);\n    }\n\n    /**\n     * Maximum depth in current depth units that will be output. Any values\n     * greater than \ufffdMax Depth\ufffd will be mapped to 0 during the conversion\n     * between disparity and depth. Set before streaming\n     */\n    public void setDepthClampMax(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CLAMP_MAX, value);\n    }\n\n    /**\n     * The disparity scale factor used when in disparity output mode. Can only\n     * be set before streaming\n     */\n    public void setDisparityMultiplier(int value) {\n        setOption(RealSense.RS_OPTION_R200_DISPARITY_MULTIPLIER, value);\n    }\n\n    /**\n     * {0 - 512}. Can only be set before streaming starts\n     */\n    public void setDisparityShift(int value) {\n        setOption(RealSense.RS_OPTION_R200_DISPARITY_SHIFT, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Mean intensity set point\n     */\n    public void setAutoExposureMeanIntensitySetPoint(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_MEAN_INTENSITY_SET_POINT, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Bright ratio set point\n     */\n    public void setAutoExposureBrightRatioSetPoint(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_BRIGHT_RATIO_SET_POINT, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Kp Gain\n     */\n    public void setAutoExposureKpGain(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_KP_GAIN, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Kp Exposure\n     */\n    public void setAutoExposureKpExposure(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_KP_EXPOSURE, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Kp Dark Threshold\n     */\n    public void setAutoExposureKpDarkThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_KP_DARK_THRESHOLD, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest top edge\n     * (in pixels)\n     */\n    public void setAutoExposureTopEdge(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_TOP_EDGE, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest bottom\n     * edge (in pixels)\n     */\n    public void setAutoExposureBottomEdge(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_BOTTOM_EDGE, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest left edge\n     * (in pixels)\n     */\n    public void setAutoExposureLeftEdge(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_LEFT_EDGE, value);\n    }\n\n    /**\n     * (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest right\n     * edge (in pixels)\n     */\n    public void setAutoExposureRightEdge(int value) {\n        setOption(RealSense.RS_OPTION_R200_AUTO_EXPOSURE_RIGHT_EDGE, value);\n    }\n\n    /**\n     * Value to subtract when estimating the median of the correlation surface\n     */\n    public void setDepthControlEstimateMedianDecrement(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_ESTIMATE_MEDIAN_DECREMENT, value);\n    }\n\n    /**\n     * Value to add when estimating the median of the correlation surface\n     */\n    public void setDepthControlEstimateMedianIncrement(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_ESTIMATE_MEDIAN_INCREMENT, value);\n    }\n\n    /**\n     * A threshold by how much the winning score must beat the median\n     */\n    public void setDepthControlMedianThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_MEDIAN_THRESHOLD, value);\n    }\n\n    /**\n     * The minimum correlation score that is considered acceptable\n     */\n    public void setDepthControlMinimumThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_SCORE_MINIMUM_THRESHOLD, value);\n    }\n\n    /**\n     * The maximum correlation score that is considered acceptable\n     */\n    public void setDepthControlScoreMaximumThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_SCORE_MAXIMUM_THRESHOLD, value);\n    }\n\n    /**\n     * A parameter for determining whether the texture in the region is\n     * sufficient to justify a depth result\n     */\n    public void setDepthControlTextureCountThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_TEXTURE_COUNT_THRESHOLD, value);\n    }\n\n    /**\n     * A parameter for determining whether the texture in the region is\n     * sufficient to justify a depth result\n     */\n    public void setDepthControlTextureDifference(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_TEXTURE_DIFFERENCE_THRESHOLD, value);\n    }\n\n    /**\n     * A threshold on how much the minimum correlation score must differ from\n     * the next best score\n     */\n    public void setDepthControlSecondPeakThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_SECOND_PEAK_THRESHOLD, value);\n    }\n\n    /**\n     * Neighbor threshold value for depth calculation\n     */\n    public void setDepthControlNeighborThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_NEIGHBOR_THRESHOLD, value);\n    }\n\n    /**\n     * Left-Right threshold value for depth calculation\n     */\n    public void setDepthControlLRThreshold(int value) {\n        setOption(RealSense.RS_OPTION_R200_DEPTH_CONTROL_LR_THRESHOLD, value);\n    }\n\n    /**\n     * Fisheye image exposure time in msec\n     */\n    public void setFisheyeExposure(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_EXPOSURE, value);\n    }\n\n    /**\n     * Fisheye image gain\n     */\n    public void setFisheyeGain(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_GAIN, value);\n    }\n\n    /**\n     * Enables / disables fisheye strobe. When enabled this will align\n     * timestamps to common clock-domain with the motion events\n     */\n    public void setFisheyeStobe(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_STROBE, value);\n    }\n\n    /**\n     * Enables / disables fisheye external trigger mode. When enabled fisheye\n     * image will be aquired in-sync with the depth image\n     */\n    public void setFisheyeExternalTrigger(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_EXTERNAL_TRIGGER, value);\n    }\n\n    /**\n     * Enable / disable fisheye auto-exposure\n     */\n    public void setFisheyeEnableAutoExposure(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_ENABLE_AUTO_EXPOSURE, value);\n    }\n\n    /**\n     * 0 - static auto-exposure, 1 - anti-flicker auto-exposure, 2 - hybrid\n     */\n    public void setFisheyeAutoExposureMode(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_AUTO_EXPOSURE_MODE, value);\n    }\n\n    /**\n     * Fisheye auto-exposure anti-flicker rate, can be 50 or 60 Hz\n     */\n    public void setFisheyeAutoExposureAntiflickerRate(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_AUTO_EXPOSURE_ANTIFLICKER_RATE, value);\n    }\n\n    /**\n     * In Fisheye auto-exposure sample frame every given number of pixels\n     */\n    public void setFisheyeAutoExposurePixelSampleRate(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_AUTO_EXPOSURE_PIXEL_SAMPLE_RATE, value);\n    }\n\n    /**\n     * In Fisheye auto-exposure sample every given number of frames\n     */\n    public void setFisheyeAutoExposureSkipFrames(int value) {\n        setOption(RealSense.RS_OPTION_FISHEYE_AUTO_EXPOSURE_SKIP_FRAMES, value);\n    }\n\n    /**\n     * Number of frames the user is allowed to keep per stream. Trying to\n     * hold-on to more frames will cause frame-drops.\n     */\n    public void setFramesQueueSize(int value) {\n        setOption(RealSense.RS_OPTION_FRAMES_QUEUE_SIZE, value);\n    }\n\n    /**\n     * Enable / disable fetching log data from the device\n     */\n    public void setHardwareLoggerEnabled(int value) {\n        setOption(RealSense.RS_OPTION_HARDWARE_LOGGER_ENABLED, value);\n    }\n\n}\n",
        "simple_context": "package org.bytedeco.javacv;\n\nimport java.io.File;\n\nimport java.nio.ByteBuffer;\n\nimport java.nio.ByteOrder;\n\nimport java.nio.ShortBuffer;\n\nimport org.bytedeco.javacpp.BytePointer;\n\nimport org.bytedeco.javacpp.Loader;\n\nimport org.bytedeco.javacpp.Pointer;\n\nimport org.bytedeco.librealsense;\n\nimport org.bytedeco.librealsense.global.RealSense;\n\nimport org.bytedeco.opencv.opencv_core;\n\nimport org.bytedeco.opencv.opencv_imgproc;\n\nimport static org.bytedeco.opencv.global.opencv_core;\n\nimport static org.bytedeco.opencv.global.opencv_imgproc;\n\npublic class RealSenseFrameGrabber extends FrameGrabber {\n    static public String getDeviceDescriptions()throws FrameGrabber.Exception;\n    static public int DEFAULT_DEPTH_WIDTH;\n    static public int DEFAULT_DEPTH_HEIGHT;\n    static public int DEFAULT_COLOR_WIDTH;\n    static public int DEFAULT_COLOR_HEIGHT;\n    static public int DEFAULT_COLOR_FRAMERATE;\n    private ByteOrder byteOrder;\n    private int depthImageWidth;\n    private int depthImageHeight;\n    private int depthFrameRate;\n    private int IRImageWidth;\n    private int IRImageHeight;\n    private int IRFrameRate;\n    public ByteOrder getByteOrder();\n    public  setByteOrder(ByteOrder byteOrder);\n    static public RealSenseFrameGrabber createDefault(int deviceNumber)throws FrameGrabber.Exception;\n    static public RealSenseFrameGrabber createDefault(File deviceFile)throws Exception;\n    static public RealSenseFrameGrabber createDefault(String devicePath)throws Exception;\n    static private FrameGrabber loadingException;\n    static public  tryLoad()throws FrameGrabber.Exception;\n    static private context context;\n    private int deviceNumber;\n    private device device;\n    static private device globalDevice;\n    private boolean depth;\n    private boolean colorEnabled;\n    private boolean depthEnabled;\n    private boolean IREnabled;\n    private FrameConverter converter;\n    public RealSenseFrameGrabber(int deviceNumber);\n    static public  main(String args);\n    public  enableColorStream();\n    public  disableColorStream();\n    public  enableDepthStream();\n    public  disableDepthStream();\n    public  enableIRStream();\n    public  disableIRStream();\n    public  release()throws FrameGrabber.Exception;\n    protected  finalize()throws Throwable;\n    public device getRealSenseDevice();\n    public float getDepthScale();\n    public double getFrameRate();\n    private boolean startedOnce;\n    private boolean behaveAsColorFrameGrabber;\n    public device loadDevice()throws FrameGrabber.Exception;\n    public  start()throws FrameGrabber.Exception;\n    public  stop()throws FrameGrabber.Exception;\n    private Pointer rawDepthImageData, rawVideoImageData, rawIRImageData;\n    private IplImage rawDepthImage, rawVideoImage, rawIRImage, returnImage;\n    public IplImage grabDepth();\n    public IplImage grabVideo();\n    public IplImage grabIR();\n    public Frame grab()throws Exception;\n    public  trigger()throws Exception;\n    public int getDepthImageWidth();\n    public  setDepthImageWidth(int depthImageWidth);\n    public int getDepthImageHeight();\n    public  setDepthImageHeight(int depthImageHeight);\n    public int getIRImageWidth();\n    public  setIRImageWidth(int IRImageWidth);\n    public int getIRImageHeight();\n    public  setIRImageHeight(int IRImageHeight);\n    public int getDepthFrameRate();\n    public  setDepthFrameRate(int frameRate);\n    public int getIRFrameRate();\n    public  setIRFrameRate(int IRFrameRate);\n    public double getGamma();\n    public  setPreset(int preset);\n    public  setShortRange();\n    public  setLongRange();\n    public  setMidRange();\n    public  setDefaultPreset();\n    public  setObjectScanningPreset();\n    public  setCursorPreset();\n    public  setGestureRecognitionPreset();\n    public  setBackgroundSegmentationPreset();\n    public  setIROnlyPreset();\n    public  setOption(int option, int value);\n    public  set(int value);\n    public  setColorBrightness(int value);\n    public  setColorContrast(int value);\n    public  setColorExposure(int value);\n    public  setColorGain(int value);\n    public  setColorGamma(int value);\n    public  setColorHue(int value);\n    public  setColorSaturation(int value);\n    public  setColorSharpness(int value);\n    public  setColorWhiteBalance(int value);\n    public  setColorEnableAutoExposure(int value);\n    public  setColorEnableAutoWhiteBalance(int value);\n    public  setLaserPower(int value);\n    public  setAccuracy(int value);\n    public  setMotionRange(int value);\n    public  setFilterOption(int value);\n    public  setConfidenceThreshold(int value);\n    public  setDynamicFPS(int value);\n    public  setLR_AutoExposureEnabled(int value);\n    public  setLR_Gain(int value);\n    public  setLR_Exposure(int value);\n    public  setEmitterEnabled(int value);\n    public  setDepthUnits(int value);\n    public  setDepthClampMin(int value);\n    public  setDepthClampMax(int value);\n    public  setDisparityMultiplier(int value);\n    public  setDisparityShift(int value);\n    public  setAutoExposureMeanIntensitySetPoint(int value);\n    public  setAutoExposureBrightRatioSetPoint(int value);\n    public  setAutoExposureKpGain(int value);\n    public  setAutoExposureKpExposure(int value);\n    public  setAutoExposureKpDarkThreshold(int value);\n    public  setAutoExposureTopEdge(int value);\n    public  setAutoExposureBottomEdge(int value);\n    public  setAutoExposureLeftEdge(int value);\n    public  setAutoExposureRightEdge(int value);\n    public  setDepthControlEstimateMedianDecrement(int value);\n    public  setDepthControlEstimateMedianIncrement(int value);\n    public  setDepthControlMedianThreshold(int value);\n    public  setDepthControlMinimumThreshold(int value);\n    public  setDepthControlScoreMaximumThreshold(int value);\n    public  setDepthControlTextureCountThreshold(int value);\n    public  setDepthControlTextureDifference(int value);\n    public  setDepthControlSecondPeakThreshold(int value);\n    public  setDepthControlNeighborThreshold(int value);\n    public  setDepthControlLRThreshold(int value);\n    public  setFisheyeExposure(int value);\n    public  setFisheyeGain(int value);\n    public  setFisheyeStobe(int value);\n    public  setFisheyeExternalTrigger(int value);\n    public  setFisheyeEnableAutoExposure(int value);\n    public  setFisheyeAutoExposureMode(int value);\n    public  setFisheyeAutoExposureAntiflickerRate(int value);\n    public  setFisheyeAutoExposurePixelSampleRate(int value);\n    public  setFisheyeAutoExposureSkipFrames(int value);\n    public  setFramesQueueSize(int value);\n    public  setHardwareLoggerEnabled(int value);\n}\n\n"
    }
]